{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/work/04233/sw33286/AIDA-SCRIPTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell, MultiRNNCell, DropoutWrapper\n",
    "\n",
    "from helpers import Indexer, batch\n",
    "from itertools import chain, product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARED_SIZE = 2 # size of noise (or, common vocab for all types).\n",
    "\n",
    "TYPES = ['ANIMAL','VEHICLE','NATURE','FURNITURE','FRUIT']\n",
    "SHARED_VOCAB = ['share'+str(i+1) for i in range(SHARED_SIZE)]\n",
    "TYPE2VOCAB = {'ANIMAL': ['cat','dog','pig','horse','deer']            + SHARED_VOCAB,\n",
    "              'VEHICLE': ['car','bike','motorcycle','train','bus']    + SHARED_VOCAB,\n",
    "              'NATURE': ['hill','mountain','lake','river','valley']   + SHARED_VOCAB,\n",
    "              'FURNITURE': ['stool','table','closet','cabinet','bed'] + SHARED_VOCAB,\n",
    "              'FRUIT': ['apple','pear','strawberry','grape','tomato'] + SHARED_VOCAB}\n",
    "VOCAB = list(chain.from_iterable(TYPE2VOCAB.values()))\n",
    "\n",
    "indexer = Indexer()\n",
    "indexer.get_index('PAD')\n",
    "for word in VOCAB:\n",
    "    indexer.get_index(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_LEN = 5\n",
    "SENT_FROM_LEN = 5\n",
    "SENT_TO_LEN = 15\n",
    "CTX_LEN = ((SENT_TO_LEN-SENT_FROM_LEN)//2)*DOC_LEN\n",
    "\n",
    "def to_sent(code):\n",
    "    return [indexer.get_object(idx) for idx in code]\n",
    "\n",
    "def get_rand_sent_code(sem_type, sent_len):\n",
    "    return [indexer.get_index(np.random.choice(TYPE2VOCAB[sem_type])) for _ in range(sent_len)]\n",
    "\n",
    "def get_mixture(type1, type2):\n",
    "    doc_a = [get_rand_sent_code(type1, np.random.randint(SENT_FROM_LEN, SENT_TO_LEN)) for _ in range(DOC_LEN)]\n",
    "    doc_b = [get_rand_sent_code(type2, np.random.randint(SENT_FROM_LEN, SENT_TO_LEN)) for _ in range(DOC_LEN)]\n",
    "    doc_mix = np.array(doc_a[:] + doc_b[:])\n",
    "    doc_lbs = np.array([0]*DOC_LEN + [1]*DOC_LEN)\n",
    "    indices = list(range(DOC_LEN*2))\n",
    "    random.shuffle(indices)\n",
    "    doc_mix = doc_mix[indices]\n",
    "    doc_lbs = doc_lbs[indices]\n",
    "    return doc_a, doc_b, doc_mix, doc_lbs\n",
    "    \n",
    "def batch_mixture(doc_a, doc_b, doc_mix, k):\n",
    "    batch_x1, batch_x2, batch_ctx, batch_y = [], [], [], []\n",
    "    ys = [1,0,0,1]\n",
    "    doc_mix_flat = list(chain.from_iterable(doc_mix))\n",
    "    doc_mix_len = len(doc_mix_flat)\n",
    "    doc_mix_padded = np.array(doc_mix_flat[:CTX_LEN]) if doc_mix_len>=CTX_LEN else np.array(doc_mix_flat+[0]*(CTX_LEN-doc_mix_len))\n",
    "    for _ in range(k):\n",
    "        for i,(da,db) in enumerate(product([doc_a,doc_b],[doc_a,doc_b])):\n",
    "            batch_x1.append(random.choice(da))\n",
    "            batch_x2.append(random.choice(db))\n",
    "            batch_y.append(ys[i])\n",
    "    return batch(batch_x1), batch(batch_x2), np.array([doc_mix_padded]), np.array(batch_y)\n",
    "\n",
    "def get_batch(n=40):\n",
    "    if n%4!=0:\n",
    "        raise ValueError('The current generation scheme only supports multiples of 4 for batch size!')\n",
    "    type1, type2 = np.random.choice(TYPES, 2, replace=False)\n",
    "    doc_a, doc_b, doc_mix, _ = get_mixture(type1, type2) # document labels isn't germane here.\n",
    "    (batch_x1,batch_x1_len), (batch_x2,batch_x2_len), batch_ctx, batch_y = batch_mixture(doc_a,doc_b,doc_mix,n//4)\n",
    "    return batch_x1,batch_x1_len,batch_x2,batch_x2_len,batch_ctx,batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM classifier with CNN context reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "VOCAB_SIZE = len(indexer)\n",
    "EMB_SIZE = 20\n",
    "HID_SIZE = 10\n",
    "NUM_LAYERS = 2\n",
    "KEEP_PROB = 0.7\n",
    "\n",
    "# hyperparams for cnn context reader.\n",
    "FILTER_SIZES = [3,4,5]\n",
    "NUM_FILTERS = 10\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "input_x1 = tf.placeholder(tf.int32, [None, None], name='input_x1') # <max-time, batch-size>\n",
    "input_x2 = tf.placeholder(tf.int32, [None, None], name='input_x2')\n",
    "input_x1_length = tf.placeholder(tf.int32, [None], name='input_x1_length')\n",
    "input_x2_length = tf.placeholder(tf.int32, [None], name='input_x2_length')\n",
    "input_ctx = tf.placeholder(tf.int32, [1, CTX_LEN], name='input_ctx') # <batch-size, height=max-time>\n",
    "input_y  = tf.placeholder(tf.int32, [None], name='input_y')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "with tf.variable_scope('Embeddings'):\n",
    "    embeddings = tf.get_variable('embeddings', [VOCAB_SIZE, EMB_SIZE], \n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "    input_x1_embedded = tf.nn.embedding_lookup(embeddings, input_x1) # <max-time, batch-size, emb-size>\n",
    "    input_x2_embedded = tf.nn.embedding_lookup(embeddings, input_x2)\n",
    "    input_ctx_embedded = tf.expand_dims(tf.nn.embedding_lookup(embeddings, input_ctx), -1)\n",
    "        # <batch-size, height=max-time, width=EMB_SIZE, num_channels=1>\n",
    "\n",
    "cell = MultiRNNCell([DropoutWrapper(LSTMCell(HID_SIZE),output_keep_prob=keep_prob)]*NUM_LAYERS) \n",
    "\n",
    "def run_lstm(cell, inputs, inputs_length): # lstm-out size *= 2 by bidirectionality.\n",
    "    ((fw_outputs,bw_outputs), # <max-time, batch-size, hid-size>, attention later if needed.\n",
    "     (fw_final_state,bw_final_state)) = ( # <batch-size, hid-size>\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw=cell,\n",
    "                                        cell_bw=cell,\n",
    "                                        inputs=inputs,\n",
    "                                        sequence_length=inputs_length,\n",
    "                                        dtype=tf.float32, time_major=True)\n",
    "    )\n",
    "    return tf.concat([tf.concat([fw_state_tuple.h,bw_state_tuple.h], 1) # lstm-out size *= NUM_LAYERS by stacking.\n",
    "                      for fw_state_tuple,bw_state_tuple in zip(fw_final_state,bw_final_state)], 1)\n",
    "    \n",
    "with tf.variable_scope('Bi-LSTM') as scope:\n",
    "    final_state_x1 = run_lstm(cell, input_x1_embedded, input_x1_length)\n",
    "    scope.reuse_variables() # both sentence inputs share the same weights.\n",
    "    final_state_x2 = run_lstm(cell, input_x2_embedded, input_x2_length)\n",
    "\n",
    "def run_cnn(inputs):\n",
    "    pool_outputs = []\n",
    "    for i,filter_size in enumerate(FILTER_SIZES):\n",
    "        with tf.variable_scope('CNN-ctx-%s' % filter_size):\n",
    "            filter_shape = [filter_size, EMB_SIZE, NUM_CHANNELS, NUM_FILTERS]\n",
    "            W = tf.get_variable('W', filter_shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.get_variable('b', [NUM_FILTERS], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            conv = tf.nn.conv2d(inputs, W, strides=[1,1,1,1], padding='VALID', name='conv')\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
    "            pool = tf.nn.max_pool(h, ksize=[1,CTX_LEN-filter_size+1,1,1], strides=[1,1,1,1], padding='VALID', name='pool')\n",
    "            pool_outputs.append(pool)\n",
    "    num_filters_total = NUM_FILTERS * len(FILTER_SIZES)\n",
    "    h_pool_flat = tf.nn.dropout(tf.reshape(tf.concat(pool_outputs, 3), [-1, num_filters_total]), keep_prob)\n",
    "    return h_pool_flat\n",
    "\n",
    "bc, _ = tf.unstack(tf.shape(final_state_x1))\n",
    "ctx = tf.tile(run_cnn(input_ctx_embedded), [bc, 1]) \n",
    "    # op1: <batch-size=1,total-num-filters>\n",
    "    # op2: create batch-size copies of the context vec.\n",
    "\n",
    "final_vec_size = HID_SIZE*2*2 + NUM_FILTERS*len(FILTER_SIZES)\n",
    "    # op1: bidirection=*2, 2-layer stacked bi-lstm=*2.\n",
    "    # op2: compute the total number of filters.\n",
    "final_vec_x1 = tf.concat([final_state_x1, ctx], 1)\n",
    "final_vec_x2 = tf.concat([final_state_x2, ctx], 1)\n",
    "W_bi = tf.get_variable('W_bi', [final_vec_size, final_vec_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "scores = tf.nn.sigmoid(tf.diag_part(tf.matmul(tf.matmul(final_vec_x1,W_bi),tf.transpose(final_vec_x2))),name='scores')\n",
    "predictions = tf.cast(tf.round(scores), tf.int32, name='predictions') \n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(input_y, tf.float32), logits=scores)\n",
    "    loss = tf.reduce_mean(losses, name='loss')\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct_predictions = tf.equal(predictions, input_y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "    \n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(1e-5)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step, name='train_op')\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 100: <0.7277341485023499, 0.44999998807907104>\n",
      "  batch loss & accuracy at step 200: <0.7354463338851929, 0.5>\n",
      "  batch loss & accuracy at step 300: <0.7261208295822144, 0.4749999940395355>\n",
      "  batch loss & accuracy at step 400: <0.7114263772964478, 0.5>\n",
      "  batch loss & accuracy at step 500: <0.7243891954421997, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 600: <0.7349913716316223, 0.5>\n",
      "  batch loss & accuracy at step 700: <0.693780779838562, 0.5>\n",
      "  batch loss & accuracy at step 800: <0.7239725589752197, 0.5>\n",
      "  batch loss & accuracy at step 900: <0.7070455551147461, 0.5>\n",
      "  batch loss & accuracy at step 1000: <0.7018993496894836, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.7123270034790039, 0.4996749758720398>\n",
      "\n",
      "\n",
      "Epoch  2\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 1100: <0.7114548683166504, 0.5>\n",
      "  batch loss & accuracy at step 1200: <0.6987351179122925, 0.5>\n",
      "  batch loss & accuracy at step 1300: <0.7108908295631409, 0.5>\n",
      "  batch loss & accuracy at step 1400: <0.6965960264205933, 0.5>\n",
      "  batch loss & accuracy at step 1500: <0.6956525444984436, 0.5>\n",
      "  batch loss & accuracy at step 1600: <0.697161078453064, 0.5>\n",
      "  batch loss & accuracy at step 1700: <0.6933136582374573, 0.5>\n",
      "  batch loss & accuracy at step 1800: <0.7186616659164429, 0.5>\n",
      "  batch loss & accuracy at step 1900: <0.6936937570571899, 0.5>\n",
      "  batch loss & accuracy at step 2000: <0.6936997175216675, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.701796293258667, 0.49995002150535583>\n",
      "\n",
      "\n",
      "Epoch  3\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 2100: <0.6981136798858643, 0.5>\n",
      "  batch loss & accuracy at step 2200: <0.6998009085655212, 0.5>\n",
      "  batch loss & accuracy at step 2300: <0.6958154439926147, 0.5>\n",
      "  batch loss & accuracy at step 2400: <0.7064709663391113, 0.5>\n",
      "  batch loss & accuracy at step 2500: <0.6948191523551941, 0.5>\n",
      "  batch loss & accuracy at step 2600: <0.6941794157028198, 0.5>\n",
      "  batch loss & accuracy at step 2700: <0.6944058537483215, 0.5>\n",
      "  batch loss & accuracy at step 2800: <0.6934170722961426, 0.5>\n",
      "  batch loss & accuracy at step 2900: <0.6927738189697266, 0.5>\n",
      "  batch loss & accuracy at step 3000: <0.6994289755821228, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6972068548202515, 0.5002750158309937>\n",
      "\n",
      "\n",
      "Epoch  4\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 3100: <0.6918638944625854, 0.5>\n",
      "  batch loss & accuracy at step 3200: <0.6914670467376709, 0.5>\n",
      "  batch loss & accuracy at step 3300: <0.6921967267990112, 0.5>\n",
      "  batch loss & accuracy at step 3400: <0.6939190030097961, 0.5>\n",
      "  batch loss & accuracy at step 3500: <0.699440598487854, 0.5>\n",
      "  batch loss & accuracy at step 3600: <0.6925362944602966, 0.5>\n",
      "  batch loss & accuracy at step 3700: <0.6950704455375671, 0.5>\n",
      "  batch loss & accuracy at step 3800: <0.6929569840431213, 0.5>\n",
      "  batch loss & accuracy at step 3900: <0.6934075355529785, 0.5>\n",
      "  batch loss & accuracy at step 4000: <0.6934247612953186, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.695182204246521, 0.5>\n",
      "\n",
      "\n",
      "Epoch  5\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 4100: <0.6927978992462158, 0.5>\n",
      "  batch loss & accuracy at step 4200: <0.6934595704078674, 0.5>\n",
      "  batch loss & accuracy at step 4300: <0.6968731880187988, 0.5>\n",
      "  batch loss & accuracy at step 4400: <0.6928914785385132, 0.5>\n",
      "  batch loss & accuracy at step 4500: <0.6958842873573303, 0.5>\n",
      "  batch loss & accuracy at step 4600: <0.693087100982666, 0.5>\n",
      "  batch loss & accuracy at step 4700: <0.6929813027381897, 0.5>\n",
      "  batch loss & accuracy at step 4800: <0.6929374933242798, 0.5>\n",
      "  batch loss & accuracy at step 4900: <0.6940478086471558, 0.5>\n",
      "  batch loss & accuracy at step 5000: <0.6927988529205322, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6941601634025574, 0.4999749958515167>\n",
      "\n",
      "\n",
      "Epoch  6\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 5100: <0.6936811208724976, 0.5>\n",
      "  batch loss & accuracy at step 5200: <0.6924248337745667, 0.5>\n",
      "  batch loss & accuracy at step 5300: <0.6929893493652344, 0.5>\n",
      "  batch loss & accuracy at step 5400: <0.693935215473175, 0.5>\n",
      "  batch loss & accuracy at step 5500: <0.694664478302002, 0.5>\n",
      "  batch loss & accuracy at step 5600: <0.6929780840873718, 0.5>\n",
      "  batch loss & accuracy at step 5700: <0.6931202411651611, 0.5>\n",
      "  batch loss & accuracy at step 5800: <0.6933534741401672, 0.5>\n",
      "  batch loss & accuracy at step 5900: <0.6929678916931152, 0.5>\n",
      "  batch loss & accuracy at step 6000: <0.6935434341430664, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6934171319007874, 0.5>\n",
      "\n",
      "\n",
      "Epoch  7\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 6100: <0.6926357746124268, 0.5>\n",
      "  batch loss & accuracy at step 6200: <0.6926354169845581, 0.5>\n",
      "  batch loss & accuracy at step 6300: <0.6932656168937683, 0.5>\n",
      "  batch loss & accuracy at step 6400: <0.6930258274078369, 0.5>\n",
      "  batch loss & accuracy at step 6500: <0.6925888061523438, 0.5>\n",
      "  batch loss & accuracy at step 6600: <0.6930941343307495, 0.5>\n",
      "  batch loss & accuracy at step 6700: <0.6925830841064453, 0.5>\n",
      "  batch loss & accuracy at step 6800: <0.6939313411712646, 0.5>\n",
      "  batch loss & accuracy at step 6900: <0.6929982900619507, 0.5>\n",
      "  batch loss & accuracy at step 7000: <0.6925992965698242, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6931977272033691, 0.5>\n",
      "\n",
      "\n",
      "Epoch  8\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 7100: <0.6927050352096558, 0.5>\n",
      "  batch loss & accuracy at step 7200: <0.6927750110626221, 0.5>\n",
      "  batch loss & accuracy at step 7300: <0.6918395161628723, 0.5>\n",
      "  batch loss & accuracy at step 7400: <0.6927684545516968, 0.5>\n",
      "  batch loss & accuracy at step 7500: <0.6929899454116821, 0.5>\n",
      "  batch loss & accuracy at step 7600: <0.6922388672828674, 0.5>\n",
      "  batch loss & accuracy at step 7700: <0.6926780939102173, 0.5>\n",
      "  batch loss & accuracy at step 7800: <0.6948739886283875, 0.5>\n",
      "  batch loss & accuracy at step 7900: <0.6927091479301453, 0.5>\n",
      "  batch loss & accuracy at step 8000: <0.6960616111755371, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6928914785385132, 0.5>\n",
      "\n",
      "\n",
      "Epoch  9\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 8100: <0.6929976940155029, 0.5>\n",
      "  batch loss & accuracy at step 8200: <0.6932088136672974, 0.5>\n",
      "  batch loss & accuracy at step 8300: <0.692713737487793, 0.5>\n",
      "  batch loss & accuracy at step 8400: <0.6940287947654724, 0.5>\n",
      "  batch loss & accuracy at step 8500: <0.694217324256897, 0.5>\n",
      "  batch loss & accuracy at step 8600: <0.6894638538360596, 0.5>\n",
      "  batch loss & accuracy at step 8700: <0.6937081813812256, 0.5>\n",
      "  batch loss & accuracy at step 8800: <0.6925563812255859, 0.5>\n",
      "  batch loss & accuracy at step 8900: <0.6929075121879578, 0.5>\n",
      "  batch loss & accuracy at step 9000: <0.6904129385948181, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6925805807113647, 0.5>\n",
      "\n",
      "\n",
      "Epoch  10\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 9100: <0.6912145614624023, 0.5>\n",
      "  batch loss & accuracy at step 9200: <0.6906669735908508, 0.5>\n",
      "  batch loss & accuracy at step 9300: <0.6912291049957275, 0.5>\n",
      "  batch loss & accuracy at step 9400: <0.6928237676620483, 0.5>\n",
      "  batch loss & accuracy at step 9500: <0.6930348873138428, 0.5>\n",
      "  batch loss & accuracy at step 9600: <0.6903035044670105, 0.5>\n",
      "  batch loss & accuracy at step 9700: <0.6988831758499146, 0.5>\n",
      "  batch loss & accuracy at step 9800: <0.6934642195701599, 0.5>\n",
      "  batch loss & accuracy at step 9900: <0.6902101039886475, 0.5>\n",
      "  batch loss & accuracy at step 10000: <0.6881476640701294, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6915640830993652, 0.5000749826431274>\n",
      "\n",
      "\n",
      "Epoch  11\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 10100: <0.6838983297348022, 0.5>\n",
      "  batch loss & accuracy at step 10200: <0.6872352361679077, 0.5250000357627869>\n",
      "  batch loss & accuracy at step 10300: <0.6780771017074585, 0.5>\n",
      "  batch loss & accuracy at step 10400: <0.6912323236465454, 0.5>\n",
      "  batch loss & accuracy at step 10500: <0.6918219327926636, 0.5>\n",
      "  batch loss & accuracy at step 10600: <0.6956772804260254, 0.5>\n",
      "  batch loss & accuracy at step 10700: <0.6864076852798462, 0.5>\n",
      "  batch loss & accuracy at step 10800: <0.69684237241745, 0.5>\n",
      "  batch loss & accuracy at step 10900: <0.6867955923080444, 0.5>\n",
      "  batch loss & accuracy at step 11000: <0.687097430229187, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6887258291244507, 0.5045750141143799>\n",
      "\n",
      "\n",
      "Epoch  12\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 11100: <0.6960737109184265, 0.5250000357627869>\n",
      "  batch loss & accuracy at step 11200: <0.6939861178398132, 0.5>\n",
      "  batch loss & accuracy at step 11300: <0.6920409202575684, 0.5>\n",
      "  batch loss & accuracy at step 11400: <0.6588374972343445, 0.5>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch loss & accuracy at step 11500: <0.6867223978042603, 0.5>\n",
      "  batch loss & accuracy at step 11600: <0.6901494860649109, 0.5250000357627869>\n",
      "  batch loss & accuracy at step 11700: <0.6927722692489624, 0.5>\n",
      "  batch loss & accuracy at step 11800: <0.6691496968269348, 0.5>\n",
      "  batch loss & accuracy at step 11900: <0.6484770774841309, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 12000: <0.6916006207466125, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6842313408851624, 0.5260000228881836>\n",
      "\n",
      "\n",
      "Epoch  13\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 12100: <0.6919918060302734, 0.5>\n",
      "  batch loss & accuracy at step 12200: <0.675395131111145, 0.550000011920929>\n",
      "  batch loss & accuracy at step 12300: <0.67246013879776, 0.5250000357627869>\n",
      "  batch loss & accuracy at step 12400: <0.6676899790763855, 0.5>\n",
      "  batch loss & accuracy at step 12500: <0.6918244957923889, 0.5>\n",
      "  batch loss & accuracy at step 12600: <0.689811110496521, 0.5>\n",
      "  batch loss & accuracy at step 12700: <0.6544325947761536, 0.5250000357627869>\n",
      "  batch loss & accuracy at step 12800: <0.7295620441436768, 0.550000011920929>\n",
      "  batch loss & accuracy at step 12900: <0.6930171847343445, 0.5>\n",
      "  batch loss & accuracy at step 13000: <0.6933615207672119, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.679290235042572, 0.5512999892234802>\n",
      "\n",
      "\n",
      "Epoch  14\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 13100: <0.6679094433784485, 0.550000011920929>\n",
      "  batch loss & accuracy at step 13200: <0.6603662371635437, 0.6749999523162842>\n",
      "  batch loss & accuracy at step 13300: <0.6651568412780762, 0.625>\n",
      "  batch loss & accuracy at step 13400: <0.6661736965179443, 0.574999988079071>\n",
      "  batch loss & accuracy at step 13500: <0.6512466073036194, 0.6000000238418579>\n",
      "  batch loss & accuracy at step 13600: <0.6692085266113281, 0.5>\n",
      "  batch loss & accuracy at step 13700: <0.673632800579071, 0.550000011920929>\n",
      "  batch loss & accuracy at step 13800: <0.6680997610092163, 0.5249999761581421>\n",
      "  batch loss & accuracy at step 13900: <0.6556480526924133, 0.7000000476837158>\n",
      "  batch loss & accuracy at step 14000: <0.7526603937149048, 0.375>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6748914122581482, 0.5657750368118286>\n",
      "\n",
      "\n",
      "Epoch  15\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 14100: <0.6067996621131897, 0.75>\n",
      "  batch loss & accuracy at step 14200: <0.6895999908447266, 0.5>\n",
      "  batch loss & accuracy at step 14300: <0.6928678750991821, 0.5>\n",
      "  batch loss & accuracy at step 14400: <0.6912118792533875, 0.5>\n",
      "  batch loss & accuracy at step 14500: <0.6850218772888184, 0.5>\n",
      "  batch loss & accuracy at step 14600: <0.6709150075912476, 0.5>\n",
      "  batch loss & accuracy at step 14700: <0.6434730291366577, 0.75>\n",
      "  batch loss & accuracy at step 14800: <0.6929874420166016, 0.5>\n",
      "  batch loss & accuracy at step 14900: <0.6107928156852722, 0.75>\n",
      "  batch loss & accuracy at step 15000: <0.6113650798797607, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6707664728164673, 0.5830250382423401>\n",
      "\n",
      "\n",
      "Epoch  16\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 15100: <0.6720200181007385, 0.5250000357627869>\n",
      "  batch loss & accuracy at step 15200: <0.6867352724075317, 0.5>\n",
      "  batch loss & accuracy at step 15300: <0.6468689441680908, 0.699999988079071>\n",
      "  batch loss & accuracy at step 15400: <0.6925601959228516, 0.5>\n",
      "  batch loss & accuracy at step 15500: <0.6576751470565796, 0.574999988079071>\n",
      "  batch loss & accuracy at step 15600: <0.6932017803192139, 0.5>\n",
      "  batch loss & accuracy at step 15700: <0.6267147660255432, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 15800: <0.6144789457321167, 0.75>\n",
      "  batch loss & accuracy at step 15900: <0.6504590511322021, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 16000: <0.693196177482605, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6681620478630066, 0.5863000154495239>\n",
      "\n",
      "\n",
      "Epoch  17\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 16100: <0.6369664072990417, 0.625>\n",
      "  batch loss & accuracy at step 16200: <0.7151008248329163, 0.550000011920929>\n",
      "  batch loss & accuracy at step 16300: <0.665219783782959, 0.625>\n",
      "  batch loss & accuracy at step 16400: <0.6133626699447632, 0.75>\n",
      "  batch loss & accuracy at step 16500: <0.6533347964286804, 0.550000011920929>\n",
      "  batch loss & accuracy at step 16600: <0.6925085783004761, 0.5>\n",
      "  batch loss & accuracy at step 16700: <0.6930462121963501, 0.5>\n",
      "  batch loss & accuracy at step 16800: <0.6903781890869141, 0.5>\n",
      "  batch loss & accuracy at step 16900: <0.6170182824134827, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 17000: <0.6924815773963928, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6637651324272156, 0.60732501745224>\n",
      "\n",
      "\n",
      "Epoch  18\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 17100: <0.6575251221656799, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 17200: <0.6501301527023315, 0.6000000238418579>\n",
      "  batch loss & accuracy at step 17300: <0.6930682063102722, 0.5>\n",
      "  batch loss & accuracy at step 17400: <0.6931710243225098, 0.5>\n",
      "  batch loss & accuracy at step 17500: <0.6922699213027954, 0.5>\n",
      "  batch loss & accuracy at step 17600: <0.6339641213417053, 0.675000011920929>\n",
      "  batch loss & accuracy at step 17700: <0.6146767139434814, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 17800: <0.6931248903274536, 0.5>\n",
      "  batch loss & accuracy at step 17900: <0.6931301951408386, 0.5>\n",
      "  batch loss & accuracy at step 18000: <0.6471479535102844, 0.6749999523162842>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6599015593528748, 0.6162000298500061>\n",
      "\n",
      "\n",
      "Epoch  19\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 18100: <0.6072471737861633, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 18200: <0.6335158944129944, 0.75>\n",
      "  batch loss & accuracy at step 18300: <0.6924747228622437, 0.5>\n",
      "  batch loss & accuracy at step 18400: <0.6328752636909485, 0.75>\n",
      "  batch loss & accuracy at step 18500: <0.746654212474823, 0.4750000238418579>\n",
      "  batch loss & accuracy at step 18600: <0.6047301292419434, 0.75>\n",
      "  batch loss & accuracy at step 18700: <0.6987890005111694, 0.5>\n",
      "  batch loss & accuracy at step 18800: <0.650241494178772, 0.574999988079071>\n",
      "  batch loss & accuracy at step 18900: <0.6204264163970947, 0.75>\n",
      "  batch loss & accuracy at step 19000: <0.7436067461967468, 0.5750000476837158>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6592949628829956, 0.6216999292373657>\n",
      "\n",
      "\n",
      "Epoch  20\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 19100: <0.6022497415542603, 0.75>\n",
      "  batch loss & accuracy at step 19200: <0.6932372450828552, 0.5>\n",
      "  batch loss & accuracy at step 19300: <0.5994097590446472, 0.75>\n",
      "  batch loss & accuracy at step 19400: <0.6136113405227661, 0.75>\n",
      "  batch loss & accuracy at step 19500: <0.6408179998397827, 0.625>\n",
      "  batch loss & accuracy at step 19600: <0.617767333984375, 0.699999988079071>\n",
      "  batch loss & accuracy at step 19700: <0.6666648387908936, 0.550000011920929>\n",
      "  batch loss & accuracy at step 19800: <0.6089723110198975, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 19900: <0.617006778717041, 0.75>\n",
      "  batch loss & accuracy at step 20000: <0.6278717517852783, 0.6499999761581421>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6564350724220276, 0.629425048828125>\n",
      "\n",
      "\n",
      "Epoch  21\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 20100: <0.6043332815170288, 0.75>\n",
      "  batch loss & accuracy at step 20200: <0.6020166277885437, 0.75>\n",
      "  batch loss & accuracy at step 20300: <0.6931647658348083, 0.5>\n",
      "  batch loss & accuracy at step 20400: <0.6931471824645996, 0.5>\n",
      "  batch loss & accuracy at step 20500: <0.615693986415863, 0.75>\n",
      "  batch loss & accuracy at step 20600: <0.6343713998794556, 0.6749999523162842>\n",
      "  batch loss & accuracy at step 20700: <0.69304358959198, 0.5>\n",
      "  batch loss & accuracy at step 20800: <0.6928387880325317, 0.5>\n",
      "  batch loss & accuracy at step 20900: <0.6071773767471313, 0.75>\n",
      "  batch loss & accuracy at step 21000: <0.6113278865814209, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6529920101165771, 0.637024998664856>\n",
      "\n",
      "\n",
      "Epoch  22\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 21100: <0.6397795677185059, 0.7000000476837158>\n",
      "  batch loss & accuracy at step 21200: <0.6161945462226868, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 21300: <0.6060868501663208, 0.75>\n",
      "  batch loss & accuracy at step 21400: <0.6155779361724854, 0.75>\n",
      "  batch loss & accuracy at step 21500: <0.7169625759124756, 0.6499999761581421>\n",
      "  batch loss & accuracy at step 21600: <0.7549139261245728, 0.5>\n",
      "  batch loss & accuracy at step 21700: <0.7722790241241455, 0.5>\n",
      "  batch loss & accuracy at step 21800: <0.6136565208435059, 0.7250000238418579>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch loss & accuracy at step 21900: <0.7533520460128784, 0.550000011920929>\n",
      "  batch loss & accuracy at step 22000: <0.7752465009689331, 0.5250000357627869>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6566755175590515, 0.6336749792098999>\n",
      "\n",
      "\n",
      "Epoch  23\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 22100: <0.7387772798538208, 0.4749999940395355>\n",
      "  batch loss & accuracy at step 22200: <0.6013460755348206, 0.75>\n",
      "  batch loss & accuracy at step 22300: <0.6006646752357483, 0.75>\n",
      "  batch loss & accuracy at step 22400: <0.616828203201294, 0.7000000476837158>\n",
      "  batch loss & accuracy at step 22500: <0.6233983039855957, 0.75>\n",
      "  batch loss & accuracy at step 22600: <0.693121075630188, 0.5>\n",
      "  batch loss & accuracy at step 22700: <0.5968199968338013, 0.75>\n",
      "  batch loss & accuracy at step 22800: <0.6043301820755005, 0.75>\n",
      "  batch loss & accuracy at step 22900: <0.6930908560752869, 0.5>\n",
      "  batch loss & accuracy at step 23000: <0.7504109144210815, 0.5750000476837158>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.651521623134613, 0.6386500000953674>\n",
      "\n",
      "\n",
      "Epoch  24\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 23100: <0.7633593082427979, 0.5>\n",
      "  batch loss & accuracy at step 23200: <0.6050177216529846, 0.75>\n",
      "  batch loss & accuracy at step 23300: <0.6036544442176819, 0.75>\n",
      "  batch loss & accuracy at step 23400: <0.6024042367935181, 0.75>\n",
      "  batch loss & accuracy at step 23500: <0.778647780418396, 0.45000001788139343>\n",
      "  batch loss & accuracy at step 23600: <0.693122148513794, 0.5>\n",
      "  batch loss & accuracy at step 23700: <0.6218739748001099, 0.75>\n",
      "  batch loss & accuracy at step 23800: <0.6028928160667419, 0.75>\n",
      "  batch loss & accuracy at step 23900: <0.6931469440460205, 0.5>\n",
      "  batch loss & accuracy at step 24000: <0.6210081577301025, 0.675000011920929>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6503884196281433, 0.6414000391960144>\n",
      "\n",
      "\n",
      "Epoch  25\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 24100: <0.6047787666320801, 0.75>\n",
      "  batch loss & accuracy at step 24200: <0.6006250977516174, 0.75>\n",
      "  batch loss & accuracy at step 24300: <0.6931467652320862, 0.5>\n",
      "  batch loss & accuracy at step 24400: <0.6025410890579224, 0.75>\n",
      "  batch loss & accuracy at step 24500: <0.5999419689178467, 0.75>\n",
      "  batch loss & accuracy at step 24600: <0.6454230546951294, 0.75>\n",
      "  batch loss & accuracy at step 24700: <0.5846828818321228, 0.75>\n",
      "  batch loss & accuracy at step 24800: <0.6157311797142029, 0.75>\n",
      "  batch loss & accuracy at step 24900: <0.693095326423645, 0.5>\n",
      "  batch loss & accuracy at step 25000: <0.6046330332756042, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6489291787147522, 0.6447749733924866>\n",
      "\n",
      "\n",
      "Epoch  26\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 25100: <0.6007093191146851, 0.75>\n",
      "  batch loss & accuracy at step 25200: <0.6042601466178894, 0.75>\n",
      "  batch loss & accuracy at step 25300: <0.5984712243080139, 0.75>\n",
      "  batch loss & accuracy at step 25400: <0.6927291750907898, 0.5>\n",
      "  batch loss & accuracy at step 25500: <0.617316722869873, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 25600: <0.6184688210487366, 0.7749999761581421>\n",
      "  batch loss & accuracy at step 25700: <0.7693027853965759, 0.5250000357627869>\n",
      "  batch loss & accuracy at step 25800: <0.6044707894325256, 0.75>\n",
      "  batch loss & accuracy at step 25900: <0.6003899574279785, 0.75>\n",
      "  batch loss & accuracy at step 26000: <0.6925859451293945, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6488556861877441, 0.6439000368118286>\n",
      "\n",
      "\n",
      "Epoch  27\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 26100: <0.6079381108283997, 0.75>\n",
      "  batch loss & accuracy at step 26200: <0.5994303822517395, 0.75>\n",
      "  batch loss & accuracy at step 26300: <0.6002720594406128, 0.75>\n",
      "  batch loss & accuracy at step 26400: <0.633502185344696, 0.6499999761581421>\n",
      "  batch loss & accuracy at step 26500: <0.6174809336662292, 0.75>\n",
      "  batch loss & accuracy at step 26600: <0.6043489575386047, 0.75>\n",
      "  batch loss & accuracy at step 26700: <0.6901776790618896, 0.5>\n",
      "  batch loss & accuracy at step 26800: <0.5411717295646667, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 26900: <0.6136261820793152, 0.625>\n",
      "  batch loss & accuracy at step 27000: <0.5895195007324219, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6422968506813049, 0.6600500345230103>\n",
      "\n",
      "\n",
      "Epoch  28\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 27100: <0.6010967493057251, 0.75>\n",
      "  batch loss & accuracy at step 27200: <0.6030250191688538, 0.75>\n",
      "  batch loss & accuracy at step 27300: <0.6030526757240295, 0.75>\n",
      "  batch loss & accuracy at step 27400: <0.595195472240448, 0.75>\n",
      "  batch loss & accuracy at step 27500: <0.6094194054603577, 0.75>\n",
      "  batch loss & accuracy at step 27600: <0.602772057056427, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 27700: <0.6076064109802246, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 27800: <0.6916190981864929, 0.5>\n",
      "  batch loss & accuracy at step 27900: <0.5777235627174377, 0.7749999761581421>\n",
      "  batch loss & accuracy at step 28000: <0.6768233776092529, 0.6500000357627869>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6358606219291687, 0.6797249913215637>\n",
      "\n",
      "\n",
      "Epoch  29\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 28100: <0.6920188665390015, 0.5>\n",
      "  batch loss & accuracy at step 28200: <0.6828972697257996, 0.5249999761581421>\n",
      "  batch loss & accuracy at step 28300: <0.5066903233528137, 1.0>\n",
      "  batch loss & accuracy at step 28400: <0.5095736384391785, 1.0>\n",
      "  batch loss & accuracy at step 28500: <0.62228924036026, 0.7000000476837158>\n",
      "  batch loss & accuracy at step 28600: <0.6041309237480164, 0.75>\n",
      "  batch loss & accuracy at step 28700: <0.7421505451202393, 0.5249999761581421>\n",
      "  batch loss & accuracy at step 28800: <0.6146851778030396, 0.75>\n",
      "  batch loss & accuracy at step 28900: <0.6484252214431763, 0.6500000357627869>\n",
      "  batch loss & accuracy at step 29000: <0.7340459227561951, 0.4750000238418579>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6288651823997498, 0.7059999704360962>\n",
      "\n",
      "\n",
      "Epoch  30\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 29100: <0.5661076903343201, 0.8499999642372131>\n",
      "  batch loss & accuracy at step 29200: <0.7881674766540527, 0.4750000238418579>\n",
      "  batch loss & accuracy at step 29300: <0.7548581957817078, 0.45000001788139343>\n",
      "  batch loss & accuracy at step 29400: <0.6015853881835938, 0.75>\n",
      "  batch loss & accuracy at step 29500: <0.585547149181366, 0.75>\n",
      "  batch loss & accuracy at step 29600: <0.6068652272224426, 0.7749999761581421>\n",
      "  batch loss & accuracy at step 29700: <0.6805365085601807, 0.550000011920929>\n",
      "  batch loss & accuracy at step 29800: <0.6004026532173157, 0.75>\n",
      "  batch loss & accuracy at step 29900: <0.5372151732444763, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 30000: <0.6582100987434387, 0.625>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6224279999732971, 0.7295249700546265>\n",
      "\n",
      "\n",
      "Epoch  31\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 30100: <0.6087880730628967, 0.75>\n",
      "  batch loss & accuracy at step 30200: <0.5268538594245911, 1.0>\n",
      "  batch loss & accuracy at step 30300: <0.6506491899490356, 0.625>\n",
      "  batch loss & accuracy at step 30400: <0.5095250010490417, 1.0>\n",
      "  batch loss & accuracy at step 30500: <0.5709511637687683, 0.800000011920929>\n",
      "  batch loss & accuracy at step 30600: <0.6951753497123718, 0.5>\n",
      "  batch loss & accuracy at step 30700: <0.7389839887619019, 0.5249999761581421>\n",
      "  batch loss & accuracy at step 30800: <0.7072268724441528, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 30900: <0.5689578652381897, 0.824999988079071>\n",
      "  batch loss & accuracy at step 31000: <0.5635724663734436, 0.8750000596046448>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6235045194625854, 0.7340000867843628>\n",
      "\n",
      "\n",
      "Epoch  32\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 31100: <0.5053553581237793, 1.0>\n",
      "  batch loss & accuracy at step 31200: <0.6851038336753845, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 31300: <0.6731963157653809, 0.5250000357627869>\n",
      "  batch loss & accuracy at step 31400: <0.6784666776657104, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 31500: <0.5436974763870239, 0.8750000596046448>\n",
      "  batch loss & accuracy at step 31600: <0.5046799182891846, 1.0>\n",
      "  batch loss & accuracy at step 31700: <0.6158772110939026, 0.75>\n",
      "  batch loss & accuracy at step 31800: <0.5231841206550598, 0.925000011920929>\n",
      "  batch loss & accuracy at step 31900: <0.6841338276863098, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 32000: <0.6020496487617493, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6238086223602295, 0.737500011920929>\n",
      "\n",
      "\n",
      "Epoch  33\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 32100: <0.7253170013427734, 0.5>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch loss & accuracy at step 32200: <0.6174209117889404, 0.7750000357627869>\n",
      "  batch loss & accuracy at step 32300: <0.6076651811599731, 0.7000000476837158>\n",
      "  batch loss & accuracy at step 32400: <0.70317542552948, 0.5249999761581421>\n",
      "  batch loss & accuracy at step 32500: <0.5876457691192627, 0.7749999761581421>\n",
      "  batch loss & accuracy at step 32600: <0.587978184223175, 0.7999999523162842>\n",
      "  batch loss & accuracy at step 32700: <0.5409321784973145, 0.8499999642372131>\n",
      "  batch loss & accuracy at step 32800: <0.675304114818573, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 32900: <0.6100686192512512, 0.7000000476837158>\n",
      "  batch loss & accuracy at step 33000: <0.5550482869148254, 0.9000000357627869>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6156264543533325, 0.7556999921798706>\n",
      "\n",
      "\n",
      "Epoch  34\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 33100: <0.5670778155326843, 0.8250000476837158>\n",
      "  batch loss & accuracy at step 33200: <0.5732020735740662, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 33300: <0.5972273945808411, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 33400: <0.6749783754348755, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 33500: <0.5612127780914307, 1.0>\n",
      "  batch loss & accuracy at step 33600: <0.5285631418228149, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 33700: <0.5597825646400452, 0.9249999523162842>\n",
      "  batch loss & accuracy at step 33800: <0.6052694320678711, 0.75>\n",
      "  batch loss & accuracy at step 33900: <0.619843065738678, 0.6749999523162842>\n",
      "  batch loss & accuracy at step 34000: <0.535681426525116, 0.925000011920929>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6108155250549316, 0.7640750408172607>\n",
      "\n",
      "\n",
      "Epoch  35\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 34100: <0.5249186754226685, 1.0>\n",
      "  batch loss & accuracy at step 34200: <0.5866581797599792, 1.0>\n",
      "  batch loss & accuracy at step 34300: <0.5552505254745483, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 34400: <0.5063056945800781, 1.0>\n",
      "  batch loss & accuracy at step 34500: <0.7267060279846191, 0.5>\n",
      "  batch loss & accuracy at step 34600: <0.7579407691955566, 0.6000000238418579>\n",
      "  batch loss & accuracy at step 34700: <0.512637734413147, 1.0>\n",
      "  batch loss & accuracy at step 34800: <0.6945938467979431, 0.5000000596046448>\n",
      "  batch loss & accuracy at step 34900: <0.5377408862113953, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 35000: <0.5170184373855591, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6165418028831482, 0.7515250444412231>\n",
      "\n",
      "\n",
      "Epoch  36\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 35100: <0.5100315809249878, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 35200: <0.5056513547897339, 1.0>\n",
      "  batch loss & accuracy at step 35300: <0.6373883485794067, 0.75>\n",
      "  batch loss & accuracy at step 35400: <0.7058311700820923, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 35500: <0.5512250661849976, 0.8500000238418579>\n",
      "  batch loss & accuracy at step 35600: <0.5142035484313965, 0.9750000238418579>\n",
      "  batch loss & accuracy at step 35700: <0.570995032787323, 0.9249999523162842>\n",
      "  batch loss & accuracy at step 35800: <0.51369309425354, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 35900: <0.5878611207008362, 0.8250000476837158>\n",
      "  batch loss & accuracy at step 36000: <0.6602112054824829, 0.625>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6100030541419983, 0.7666750550270081>\n",
      "\n",
      "\n",
      "Epoch  37\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 36100: <0.5321624875068665, 0.949999988079071>\n",
      "  batch loss & accuracy at step 36200: <0.6929653882980347, 0.5>\n",
      "  batch loss & accuracy at step 36300: <0.6666900515556335, 0.625>\n",
      "  batch loss & accuracy at step 36400: <0.7221798896789551, 0.4750000238418579>\n",
      "  batch loss & accuracy at step 36500: <0.6313243508338928, 0.6749999523162842>\n",
      "  batch loss & accuracy at step 36600: <0.6337763071060181, 0.7749999761581421>\n",
      "  batch loss & accuracy at step 36700: <0.7527239918708801, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 36800: <0.6104368567466736, 0.75>\n",
      "  batch loss & accuracy at step 36900: <0.5257260799407959, 0.949999988079071>\n",
      "  batch loss & accuracy at step 37000: <0.6236889362335205, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6118154525756836, 0.7578750252723694>\n",
      "\n",
      "\n",
      "Epoch  38\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 37100: <0.5372456312179565, 1.0>\n",
      "  batch loss & accuracy at step 37200: <0.6006996631622314, 0.949999988079071>\n",
      "  batch loss & accuracy at step 37300: <0.6186779737472534, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 37400: <0.5091912150382996, 1.0>\n",
      "  batch loss & accuracy at step 37500: <0.7508671283721924, 0.5000000596046448>\n",
      "  batch loss & accuracy at step 37600: <0.5857040882110596, 0.8250000476837158>\n",
      "  batch loss & accuracy at step 37700: <0.7081332206726074, 0.5250000357627869>\n",
      "  batch loss & accuracy at step 37800: <0.6062126159667969, 0.7749999761581421>\n",
      "  batch loss & accuracy at step 37900: <0.663243293762207, 0.5>\n",
      "  batch loss & accuracy at step 38000: <0.5145031213760376, 0.9749999642372131>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6036157011985779, 0.777275025844574>\n",
      "\n",
      "\n",
      "Epoch  39\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 38100: <0.7520327568054199, 0.42499998211860657>\n",
      "  batch loss & accuracy at step 38200: <0.5551012754440308, 1.0>\n",
      "  batch loss & accuracy at step 38300: <0.6843956708908081, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 38400: <0.5531498193740845, 0.8750000596046448>\n",
      "  batch loss & accuracy at step 38500: <0.65239417552948, 0.5>\n",
      "  batch loss & accuracy at step 38600: <0.7963190674781799, 0.45000001788139343>\n",
      "  batch loss & accuracy at step 38700: <0.575065016746521, 0.8500000238418579>\n",
      "  batch loss & accuracy at step 38800: <0.6890658140182495, 0.6000000238418579>\n",
      "  batch loss & accuracy at step 38900: <0.5885088443756104, 0.75>\n",
      "  batch loss & accuracy at step 39000: <0.6272861361503601, 0.675000011920929>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6036560535430908, 0.7777500152587891>\n",
      "\n",
      "\n",
      "Epoch  40\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 39100: <0.6816296577453613, 0.6000000238418579>\n",
      "  batch loss & accuracy at step 39200: <0.5715689659118652, 0.824999988079071>\n",
      "  batch loss & accuracy at step 39300: <0.5917311906814575, 0.75>\n",
      "  batch loss & accuracy at step 39400: <0.5661845207214355, 0.7999999523162842>\n",
      "  batch loss & accuracy at step 39500: <0.6028646230697632, 0.75>\n",
      "  batch loss & accuracy at step 39600: <0.5235811471939087, 0.949999988079071>\n",
      "  batch loss & accuracy at step 39700: <0.6412526965141296, 0.7000000476837158>\n",
      "  batch loss & accuracy at step 39800: <0.7097969651222229, 0.550000011920929>\n",
      "  batch loss & accuracy at step 39900: <0.6379913687705994, 0.6499999761581421>\n",
      "  batch loss & accuracy at step 40000: <0.685992956161499, 0.625>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5978172421455383, 0.7917999625205994>\n",
      "\n",
      "\n",
      "Epoch  41\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 40100: <0.5422683954238892, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 40200: <0.5994905233383179, 0.75>\n",
      "  batch loss & accuracy at step 40300: <0.5544405579566956, 0.8750000596046448>\n",
      "  batch loss & accuracy at step 40400: <0.5868333578109741, 0.7749999761581421>\n",
      "  batch loss & accuracy at step 40500: <0.5462161302566528, 0.8500000238418579>\n",
      "  batch loss & accuracy at step 40600: <0.6730625629425049, 0.6500000357627869>\n",
      "  batch loss & accuracy at step 40700: <0.6920173168182373, 0.5>\n",
      "  batch loss & accuracy at step 40800: <0.6511564254760742, 0.675000011920929>\n",
      "  batch loss & accuracy at step 40900: <0.6033509969711304, 0.75>\n",
      "  batch loss & accuracy at step 41000: <0.5489835739135742, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5971296429634094, 0.795799970626831>\n",
      "\n",
      "\n",
      "Epoch  42\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 41100: <0.6184303760528564, 0.75>\n",
      "  batch loss & accuracy at step 41200: <0.5132336616516113, 1.0>\n",
      "  batch loss & accuracy at step 41300: <0.5796592235565186, 0.8750000596046448>\n",
      "  batch loss & accuracy at step 41400: <0.619549572467804, 0.675000011920929>\n",
      "  batch loss & accuracy at step 41500: <0.5211167335510254, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 41600: <0.6522972583770752, 0.574999988079071>\n",
      "  batch loss & accuracy at step 41700: <0.5390216708183289, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 41800: <0.7375651597976685, 0.550000011920929>\n",
      "  batch loss & accuracy at step 41900: <0.5056203007698059, 1.0>\n",
      "  batch loss & accuracy at step 42000: <0.5511406660079956, 0.949999988079071>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5927321910858154, 0.8002249598503113>\n",
      "\n",
      "\n",
      "Epoch  43\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch loss & accuracy at step 42100: <0.59852534532547, 0.75>\n",
      "  batch loss & accuracy at step 42200: <0.5899832248687744, 0.75>\n",
      "  batch loss & accuracy at step 42300: <0.5584625601768494, 0.949999988079071>\n",
      "  batch loss & accuracy at step 42400: <0.648621141910553, 0.6500000357627869>\n",
      "  batch loss & accuracy at step 42500: <0.5310324430465698, 0.949999988079071>\n",
      "  batch loss & accuracy at step 42600: <0.6476107835769653, 0.6000000238418579>\n",
      "  batch loss & accuracy at step 42700: <0.5118602514266968, 1.0>\n",
      "  batch loss & accuracy at step 42800: <0.6039524674415588, 0.75>\n",
      "  batch loss & accuracy at step 42900: <0.6004266142845154, 0.75>\n",
      "  batch loss & accuracy at step 43000: <0.5703489184379578, 0.925000011920929>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5916081070899963, 0.801425039768219>\n",
      "\n",
      "\n",
      "Epoch  44\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 43100: <0.504870593547821, 1.0>\n",
      "  batch loss & accuracy at step 43200: <0.5156973600387573, 0.9750000238418579>\n",
      "  batch loss & accuracy at step 43300: <0.5342293977737427, 0.8750000596046448>\n",
      "  batch loss & accuracy at step 43400: <0.685000479221344, 0.6499999761581421>\n",
      "  batch loss & accuracy at step 43500: <0.6033400297164917, 0.75>\n",
      "  batch loss & accuracy at step 43600: <0.5052646398544312, 1.0>\n",
      "  batch loss & accuracy at step 43700: <0.5074546337127686, 1.0>\n",
      "  batch loss & accuracy at step 43800: <0.7373608350753784, 0.5000000596046448>\n",
      "  batch loss & accuracy at step 43900: <0.5455215573310852, 1.0>\n",
      "  batch loss & accuracy at step 44000: <0.6013391017913818, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5918348431587219, 0.8019500374794006>\n",
      "\n",
      "\n",
      "Epoch  45\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 44100: <0.5889846086502075, 0.8500000238418579>\n",
      "  batch loss & accuracy at step 44200: <0.6141061782836914, 0.699999988079071>\n",
      "  batch loss & accuracy at step 44300: <0.6046497225761414, 0.75>\n",
      "  batch loss & accuracy at step 44400: <0.5597245693206787, 0.8499999642372131>\n",
      "  batch loss & accuracy at step 44500: <0.5805517435073853, 0.824999988079071>\n",
      "  batch loss & accuracy at step 44600: <0.5162661075592041, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 44700: <0.6402407288551331, 0.625>\n",
      "  batch loss & accuracy at step 44800: <0.557587742805481, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 44900: <0.604557991027832, 0.75>\n",
      "  batch loss & accuracy at step 45000: <0.6611436605453491, 0.6499999761581421>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.585000216960907, 0.8178499937057495>\n",
      "\n",
      "\n",
      "Epoch  46\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 45100: <0.6658399105072021, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 45200: <0.5950945019721985, 0.75>\n",
      "  batch loss & accuracy at step 45300: <0.5440648794174194, 0.875>\n",
      "  batch loss & accuracy at step 45400: <0.6121259927749634, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 45500: <0.5041943788528442, 1.0>\n",
      "  batch loss & accuracy at step 45600: <0.623981237411499, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 45700: <0.5074480175971985, 1.0>\n",
      "  batch loss & accuracy at step 45800: <0.5062272548675537, 1.0>\n",
      "  batch loss & accuracy at step 45900: <0.5422611832618713, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 46000: <0.5918105840682983, 0.7749999761581421>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5876928567886353, 0.808525025844574>\n",
      "\n",
      "\n",
      "Epoch  47\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 46100: <0.6018696427345276, 0.75>\n",
      "  batch loss & accuracy at step 46200: <0.510711133480072, 1.0>\n",
      "  batch loss & accuracy at step 46300: <0.5984817743301392, 0.75>\n",
      "  batch loss & accuracy at step 46400: <0.5074472427368164, 1.0>\n",
      "  batch loss & accuracy at step 46500: <0.6044559478759766, 0.75>\n",
      "  batch loss & accuracy at step 46600: <0.6489367485046387, 0.675000011920929>\n",
      "  batch loss & accuracy at step 46700: <0.5103779435157776, 1.0>\n",
      "  batch loss & accuracy at step 46800: <0.596138596534729, 0.75>\n",
      "  batch loss & accuracy at step 46900: <0.6919737458229065, 0.6750000715255737>\n",
      "  batch loss & accuracy at step 47000: <0.5468396544456482, 0.9000000357627869>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5845405459403992, 0.8162999749183655>\n",
      "\n",
      "\n",
      "Epoch  48\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 47100: <0.5124711394309998, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 47200: <0.5163125991821289, 1.0>\n",
      "  batch loss & accuracy at step 47300: <0.5397045016288757, 0.9249999523162842>\n",
      "  batch loss & accuracy at step 47400: <0.6149747967720032, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 47500: <0.597452700138092, 0.800000011920929>\n",
      "  batch loss & accuracy at step 47600: <0.5066202878952026, 1.0>\n",
      "  batch loss & accuracy at step 47700: <0.6004356145858765, 0.75>\n",
      "  batch loss & accuracy at step 47800: <0.5141943097114563, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 47900: <0.7074843645095825, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 48000: <0.5943784713745117, 0.8750000596046448>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5822048187255859, 0.824275016784668>\n",
      "\n",
      "\n",
      "Epoch  49\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 48100: <0.612396776676178, 0.75>\n",
      "  batch loss & accuracy at step 48200: <0.603915810585022, 0.75>\n",
      "  batch loss & accuracy at step 48300: <0.6190012693405151, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 48400: <0.6297610402107239, 0.6499999761581421>\n",
      "  batch loss & accuracy at step 48500: <0.5164268612861633, 1.0>\n",
      "  batch loss & accuracy at step 48600: <0.5209059119224548, 0.925000011920929>\n",
      "  batch loss & accuracy at step 48700: <0.5855616927146912, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 48800: <0.5963581800460815, 0.75>\n",
      "  batch loss & accuracy at step 48900: <0.7691460251808167, 0.6000000238418579>\n",
      "  batch loss & accuracy at step 49000: <0.6351636052131653, 0.675000011920929>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5813366770744324, 0.8248499631881714>\n",
      "\n",
      "\n",
      "Epoch  50\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 49100: <0.5962156057357788, 0.75>\n",
      "  batch loss & accuracy at step 49200: <0.6908737421035767, 0.5>\n",
      "  batch loss & accuracy at step 49300: <0.6328006982803345, 0.6499999761581421>\n",
      "  batch loss & accuracy at step 49400: <0.5952069163322449, 0.75>\n",
      "  batch loss & accuracy at step 49500: <0.6195305585861206, 0.7999999523162842>\n",
      "  batch loss & accuracy at step 49600: <0.6028714776039124, 0.75>\n",
      "  batch loss & accuracy at step 49700: <0.6102895140647888, 0.699999988079071>\n",
      "  batch loss & accuracy at step 49800: <0.5037185549736023, 1.0>\n",
      "  batch loss & accuracy at step 49900: <0.5324186086654663, 0.8750000596046448>\n",
      "  batch loss & accuracy at step 50000: <0.507591187953949, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5769319534301758, 0.832474946975708>\n",
      "\n",
      "\n",
      "Epoch  51\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 50100: <0.6109931468963623, 0.75>\n",
      "  batch loss & accuracy at step 50200: <0.5990793108940125, 0.75>\n",
      "  batch loss & accuracy at step 50300: <0.5160230398178101, 0.949999988079071>\n",
      "  batch loss & accuracy at step 50400: <0.645559549331665, 0.7750000357627869>\n",
      "  batch loss & accuracy at step 50500: <0.5128365159034729, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 50600: <0.6004940271377563, 0.75>\n",
      "  batch loss & accuracy at step 50700: <0.6887117624282837, 0.6000000238418579>\n",
      "  batch loss & accuracy at step 50800: <0.5539254546165466, 0.824999988079071>\n",
      "  batch loss & accuracy at step 50900: <0.5072194933891296, 1.0>\n",
      "  batch loss & accuracy at step 51000: <0.510753870010376, 0.9749999642372131>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.571721076965332, 0.8497499823570251>\n",
      "\n",
      "\n",
      "Epoch  52\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 51100: <0.5618572235107422, 0.949999988079071>\n",
      "  batch loss & accuracy at step 51200: <0.5838289856910706, 0.925000011920929>\n",
      "  batch loss & accuracy at step 51300: <0.601198673248291, 0.75>\n",
      "  batch loss & accuracy at step 51400: <0.5055779218673706, 1.0>\n",
      "  batch loss & accuracy at step 51500: <0.5841063261032104, 0.75>\n",
      "  batch loss & accuracy at step 51600: <0.5062087774276733, 1.0>\n",
      "  batch loss & accuracy at step 51700: <0.5929222106933594, 0.8999999761581421>\n",
      "  batch loss & accuracy at step 51800: <0.7081745862960815, 0.5249999761581421>\n",
      "  batch loss & accuracy at step 51900: <0.5714839696884155, 0.875>\n",
      "  batch loss & accuracy at step 52000: <0.5033899545669556, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5724378824234009, 0.8457499742507935>\n",
      "\n",
      "\n",
      "Epoch  53\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 52100: <0.5999389886856079, 0.875>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch loss & accuracy at step 52200: <0.5268557071685791, 1.0>\n",
      "  batch loss & accuracy at step 52300: <0.5398725271224976, 1.0>\n",
      "  batch loss & accuracy at step 52400: <0.5069835186004639, 1.0>\n",
      "  batch loss & accuracy at step 52500: <0.5611804127693176, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 52600: <0.5092684030532837, 1.0>\n",
      "  batch loss & accuracy at step 52700: <0.5228853821754456, 1.0>\n",
      "  batch loss & accuracy at step 52800: <0.5592294335365295, 0.8750000596046448>\n",
      "  batch loss & accuracy at step 52900: <0.5211538672447205, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 53000: <0.5311981439590454, 0.925000011920929>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5666787028312683, 0.8565499782562256>\n",
      "\n",
      "\n",
      "Epoch  54\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 53100: <0.6642520427703857, 0.675000011920929>\n",
      "  batch loss & accuracy at step 53200: <0.5504361391067505, 1.0>\n",
      "  batch loss & accuracy at step 53300: <0.5771095752716064, 0.824999988079071>\n",
      "  batch loss & accuracy at step 53400: <0.5998114347457886, 0.75>\n",
      "  batch loss & accuracy at step 53500: <0.5033085346221924, 1.0>\n",
      "  batch loss & accuracy at step 53600: <0.5085099935531616, 1.0>\n",
      "  batch loss & accuracy at step 53700: <0.503852367401123, 1.0>\n",
      "  batch loss & accuracy at step 53800: <0.510736346244812, 1.0>\n",
      "  batch loss & accuracy at step 53900: <0.6135430932044983, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 54000: <0.6545497179031372, 0.7000000476837158>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5681930780410767, 0.8553749918937683>\n",
      "\n",
      "\n",
      "Epoch  55\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 54100: <0.6014499068260193, 0.75>\n",
      "  batch loss & accuracy at step 54200: <0.6296197772026062, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 54300: <0.5127847194671631, 1.0>\n",
      "  batch loss & accuracy at step 54400: <0.6287054419517517, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 54500: <0.6313945055007935, 0.675000011920929>\n",
      "  batch loss & accuracy at step 54600: <0.6497583389282227, 0.6000000238418579>\n",
      "  batch loss & accuracy at step 54700: <0.5985915660858154, 0.75>\n",
      "  batch loss & accuracy at step 54800: <0.6084699630737305, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 54900: <0.6395187377929688, 0.675000011920929>\n",
      "  batch loss & accuracy at step 55000: <0.5032191276550293, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5642144680023193, 0.8640999794006348>\n",
      "\n",
      "\n",
      "Epoch  56\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 55100: <0.5111799836158752, 1.0>\n",
      "  batch loss & accuracy at step 55200: <0.5930593013763428, 0.75>\n",
      "  batch loss & accuracy at step 55300: <0.5156455636024475, 1.0>\n",
      "  batch loss & accuracy at step 55400: <0.5147917866706848, 1.0>\n",
      "  batch loss & accuracy at step 55500: <0.5850488543510437, 0.925000011920929>\n",
      "  batch loss & accuracy at step 55600: <0.5975901484489441, 0.75>\n",
      "  batch loss & accuracy at step 55700: <0.5253597497940063, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 55800: <0.5473065376281738, 1.0>\n",
      "  batch loss & accuracy at step 55900: <0.5993777513504028, 0.75>\n",
      "  batch loss & accuracy at step 56000: <0.5656898021697998, 0.9749999642372131>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5681408643722534, 0.8530749678611755>\n",
      "\n",
      "\n",
      "Epoch  57\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 56100: <0.6045046448707581, 0.75>\n",
      "  batch loss & accuracy at step 56200: <0.5032695531845093, 1.0>\n",
      "  batch loss & accuracy at step 56300: <0.554890513420105, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 56400: <0.5817210674285889, 0.7749999761581421>\n",
      "  batch loss & accuracy at step 56500: <0.601755678653717, 0.75>\n",
      "  batch loss & accuracy at step 56600: <0.590957522392273, 0.75>\n",
      "  batch loss & accuracy at step 56700: <0.5043643712997437, 1.0>\n",
      "  batch loss & accuracy at step 56800: <0.6215453147888184, 0.7000000476837158>\n",
      "  batch loss & accuracy at step 56900: <0.6569085121154785, 0.625>\n",
      "  batch loss & accuracy at step 57000: <0.6179984211921692, 0.7250000238418579>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5611292719841003, 0.8713250160217285>\n",
      "\n",
      "\n",
      "Epoch  58\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 57100: <0.5181273818016052, 0.949999988079071>\n",
      "  batch loss & accuracy at step 57200: <0.5299411416053772, 1.0>\n",
      "  batch loss & accuracy at step 57300: <0.6169111728668213, 0.6499999761581421>\n",
      "  batch loss & accuracy at step 57400: <0.5038524866104126, 1.0>\n",
      "  batch loss & accuracy at step 57500: <0.5041038990020752, 1.0>\n",
      "  batch loss & accuracy at step 57600: <0.6241840124130249, 0.7000000476837158>\n",
      "  batch loss & accuracy at step 57700: <0.5040361881256104, 1.0>\n",
      "  batch loss & accuracy at step 57800: <0.5097141265869141, 1.0>\n",
      "  batch loss & accuracy at step 57900: <0.5032439231872559, 1.0>\n",
      "  batch loss & accuracy at step 58000: <0.5956464409828186, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5599120855331421, 0.8739250302314758>\n",
      "\n",
      "\n",
      "Epoch  59\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 58100: <0.6043462157249451, 0.75>\n",
      "  batch loss & accuracy at step 58200: <0.5120173692703247, 1.0>\n",
      "  batch loss & accuracy at step 58300: <0.5954483151435852, 0.75>\n",
      "  batch loss & accuracy at step 58400: <0.5782614946365356, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 58500: <0.5095522999763489, 1.0>\n",
      "  batch loss & accuracy at step 58600: <0.5509023666381836, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 58700: <0.5255886912345886, 1.0>\n",
      "  batch loss & accuracy at step 58800: <0.5059594511985779, 1.0>\n",
      "  batch loss & accuracy at step 58900: <0.6127703189849854, 0.699999988079071>\n",
      "  batch loss & accuracy at step 59000: <0.5032652616500854, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5599185824394226, 0.8745750188827515>\n",
      "\n",
      "\n",
      "Epoch  60\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 59100: <0.5716429948806763, 0.800000011920929>\n",
      "  batch loss & accuracy at step 59200: <0.5069147944450378, 1.0>\n",
      "  batch loss & accuracy at step 59300: <0.5043783783912659, 1.0>\n",
      "  batch loss & accuracy at step 59400: <0.636223316192627, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 59500: <0.5213777422904968, 1.0>\n",
      "  batch loss & accuracy at step 59600: <0.6183981895446777, 0.824999988079071>\n",
      "  batch loss & accuracy at step 59700: <0.5037044286727905, 1.0>\n",
      "  batch loss & accuracy at step 59800: <0.5728724598884583, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 59900: <0.5845422744750977, 0.7749999761581421>\n",
      "  batch loss & accuracy at step 60000: <0.5851666927337646, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5584389567375183, 0.8790000081062317>\n",
      "\n",
      "\n",
      "Epoch  61\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 60100: <0.5857239961624146, 0.75>\n",
      "  batch loss & accuracy at step 60200: <0.5398502349853516, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 60300: <0.5581287741661072, 0.8500000238418579>\n",
      "  batch loss & accuracy at step 60400: <0.5063060522079468, 1.0>\n",
      "  batch loss & accuracy at step 60500: <0.5116535425186157, 1.0>\n",
      "  batch loss & accuracy at step 60600: <0.5106660723686218, 1.0>\n",
      "  batch loss & accuracy at step 60700: <0.5205128788948059, 0.949999988079071>\n",
      "  batch loss & accuracy at step 60800: <0.5227264761924744, 1.0>\n",
      "  batch loss & accuracy at step 60900: <0.5572202205657959, 0.8250000476837158>\n",
      "  batch loss & accuracy at step 61000: <0.5036381483078003, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5542821288108826, 0.8878250122070312>\n",
      "\n",
      "\n",
      "Epoch  62\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 61100: <0.535882830619812, 0.925000011920929>\n",
      "  batch loss & accuracy at step 61200: <0.6103949546813965, 0.7749999761581421>\n",
      "  batch loss & accuracy at step 61300: <0.5033555030822754, 1.0>\n",
      "  batch loss & accuracy at step 61400: <0.5040557384490967, 1.0>\n",
      "  batch loss & accuracy at step 61500: <0.5426806807518005, 0.949999988079071>\n",
      "  batch loss & accuracy at step 61600: <0.5111290216445923, 1.0>\n",
      "  batch loss & accuracy at step 61700: <0.7155201435089111, 0.5250000357627869>\n",
      "  batch loss & accuracy at step 61800: <0.5631085634231567, 0.8500000238418579>\n",
      "  batch loss & accuracy at step 61900: <0.5061857104301453, 1.0>\n",
      "  batch loss & accuracy at step 62000: <0.5933414697647095, 0.7749999761581421>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5544715523719788, 0.8885250091552734>\n",
      "\n",
      "\n",
      "Epoch  63\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 62100: <0.53168123960495, 1.0>\n",
      "  batch loss & accuracy at step 62200: <0.5079614520072937, 1.0>\n",
      "  batch loss & accuracy at step 62300: <0.5972520709037781, 0.75>\n",
      "  batch loss & accuracy at step 62400: <0.5904466509819031, 0.699999988079071>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch loss & accuracy at step 62500: <0.6007890701293945, 0.75>\n",
      "  batch loss & accuracy at step 62600: <0.8032793998718262, 0.5>\n",
      "  batch loss & accuracy at step 62700: <0.5408412218093872, 0.925000011920929>\n",
      "  batch loss & accuracy at step 62800: <0.5610224008560181, 0.949999988079071>\n",
      "  batch loss & accuracy at step 62900: <0.5896286964416504, 0.75>\n",
      "  batch loss & accuracy at step 63000: <0.5760179758071899, 0.875>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5557411909103394, 0.884524941444397>\n",
      "\n",
      "\n",
      "Epoch  64\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 63100: <0.5261200666427612, 1.0>\n",
      "  batch loss & accuracy at step 63200: <0.5528150796890259, 0.8750000596046448>\n",
      "  batch loss & accuracy at step 63300: <0.5696096420288086, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 63400: <0.5101366639137268, 1.0>\n",
      "  batch loss & accuracy at step 63500: <0.6002817749977112, 0.875>\n",
      "  batch loss & accuracy at step 63600: <0.5219050049781799, 1.0>\n",
      "  batch loss & accuracy at step 63700: <0.5274341106414795, 1.0>\n",
      "  batch loss & accuracy at step 63800: <0.5097419619560242, 1.0>\n",
      "  batch loss & accuracy at step 63900: <0.5329874753952026, 0.925000011920929>\n",
      "  batch loss & accuracy at step 64000: <0.5648605227470398, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5509657263755798, 0.9004250764846802>\n",
      "\n",
      "\n",
      "Epoch  65\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 64100: <0.6916443705558777, 0.574999988079071>\n",
      "  batch loss & accuracy at step 64200: <0.5427568554878235, 0.8750000596046448>\n",
      "  batch loss & accuracy at step 64300: <0.5207943320274353, 1.0>\n",
      "  batch loss & accuracy at step 64400: <0.5032236576080322, 1.0>\n",
      "  batch loss & accuracy at step 64500: <0.5418902635574341, 1.0>\n",
      "  batch loss & accuracy at step 64600: <0.5046792030334473, 1.0>\n",
      "  batch loss & accuracy at step 64700: <0.5032649636268616, 1.0>\n",
      "  batch loss & accuracy at step 64800: <0.5224149823188782, 0.949999988079071>\n",
      "  batch loss & accuracy at step 64900: <0.5338001847267151, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 65000: <0.5317084193229675, 0.9749999642372131>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5504675507545471, 0.9020499587059021>\n",
      "\n",
      "\n",
      "Epoch  66\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 65100: <0.5285831689834595, 1.0>\n",
      "  batch loss & accuracy at step 65200: <0.5033413171768188, 1.0>\n",
      "  batch loss & accuracy at step 65300: <0.5130061507225037, 1.0>\n",
      "  batch loss & accuracy at step 65400: <0.5071748495101929, 1.0>\n",
      "  batch loss & accuracy at step 65500: <0.773472249507904, 0.45000001788139343>\n",
      "  batch loss & accuracy at step 65600: <0.6141002774238586, 0.75>\n",
      "  batch loss & accuracy at step 65700: <0.5393990874290466, 0.8750000596046448>\n",
      "  batch loss & accuracy at step 65800: <0.5900276303291321, 0.800000011920929>\n",
      "  batch loss & accuracy at step 65900: <0.5205681324005127, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 66000: <0.589087188243866, 0.7749999761581421>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.546812891960144, 0.9121750593185425>\n",
      "\n",
      "\n",
      "Epoch  67\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 66100: <0.5227565169334412, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 66200: <0.5317513942718506, 0.925000011920929>\n",
      "  batch loss & accuracy at step 66300: <0.5280029773712158, 0.949999988079071>\n",
      "  batch loss & accuracy at step 66400: <0.5729198455810547, 0.8999999761581421>\n",
      "  batch loss & accuracy at step 66500: <0.5717393159866333, 0.7750000357627869>\n",
      "  batch loss & accuracy at step 66600: <0.5036561489105225, 1.0>\n",
      "  batch loss & accuracy at step 66700: <0.5269178152084351, 0.925000011920929>\n",
      "  batch loss & accuracy at step 66800: <0.5337691307067871, 0.949999988079071>\n",
      "  batch loss & accuracy at step 66900: <0.5176590085029602, 0.9750000238418579>\n",
      "  batch loss & accuracy at step 67000: <0.5220261216163635, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5476132035255432, 0.9090250134468079>\n",
      "\n",
      "\n",
      "Epoch  68\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 67100: <0.5065318942070007, 1.0>\n",
      "  batch loss & accuracy at step 67200: <0.5189255475997925, 1.0>\n",
      "  batch loss & accuracy at step 67300: <0.5260921120643616, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 67400: <0.558407187461853, 0.925000011920929>\n",
      "  batch loss & accuracy at step 67500: <0.5394554138183594, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 67600: <0.7835144996643066, 0.5000000596046448>\n",
      "  batch loss & accuracy at step 67700: <0.5071623921394348, 1.0>\n",
      "  batch loss & accuracy at step 67800: <0.554455578327179, 0.925000011920929>\n",
      "  batch loss & accuracy at step 67900: <0.5097953081130981, 1.0>\n",
      "  batch loss & accuracy at step 68000: <0.503678560256958, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5425530076026917, 0.9202499985694885>\n",
      "\n",
      "\n",
      "Epoch  69\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 68100: <0.6318042278289795, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 68200: <0.5403624773025513, 0.9249999523162842>\n",
      "  batch loss & accuracy at step 68300: <0.6177103519439697, 0.7749999761581421>\n",
      "  batch loss & accuracy at step 68400: <0.5153497457504272, 1.0>\n",
      "  batch loss & accuracy at step 68500: <0.5358636975288391, 1.0>\n",
      "  batch loss & accuracy at step 68600: <0.5248121619224548, 0.9249999523162842>\n",
      "  batch loss & accuracy at step 68700: <0.5044347047805786, 1.0>\n",
      "  batch loss & accuracy at step 68800: <0.5040936470031738, 1.0>\n",
      "  batch loss & accuracy at step 68900: <0.5066052675247192, 1.0>\n",
      "  batch loss & accuracy at step 69000: <0.5833070874214172, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5470361113548279, 0.9129999876022339>\n",
      "\n",
      "\n",
      "Epoch  70\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 69100: <0.5164308547973633, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 69200: <0.5226815938949585, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 69300: <0.5835423469543457, 0.925000011920929>\n",
      "  batch loss & accuracy at step 69400: <0.5406626462936401, 0.925000011920929>\n",
      "  batch loss & accuracy at step 69500: <0.5448846817016602, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 69600: <0.5034949779510498, 1.0>\n",
      "  batch loss & accuracy at step 69700: <0.5032666921615601, 1.0>\n",
      "  batch loss & accuracy at step 69800: <0.503227949142456, 1.0>\n",
      "  batch loss & accuracy at step 69900: <0.5255653858184814, 1.0>\n",
      "  batch loss & accuracy at step 70000: <0.6316059231758118, 0.7749999761581421>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5406529307365417, 0.9247249960899353>\n",
      "\n",
      "\n",
      "Epoch  71\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 70100: <0.5654703974723816, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 70200: <0.5135776400566101, 1.0>\n",
      "  batch loss & accuracy at step 70300: <0.5033590197563171, 1.0>\n",
      "  batch loss & accuracy at step 70400: <0.5054641366004944, 1.0>\n",
      "  batch loss & accuracy at step 70500: <0.6715347766876221, 0.5>\n",
      "  batch loss & accuracy at step 70600: <0.643329381942749, 0.7749999761581421>\n",
      "  batch loss & accuracy at step 70700: <0.6245917677879333, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 70800: <0.6172354817390442, 0.75>\n",
      "  batch loss & accuracy at step 70900: <0.5193781852722168, 1.0>\n",
      "  batch loss & accuracy at step 71000: <0.6148134469985962, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5442414879798889, 0.9192750453948975>\n",
      "\n",
      "\n",
      "Epoch  72\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 71100: <0.5853134393692017, 0.8500000238418579>\n",
      "  batch loss & accuracy at step 71200: <0.5719081163406372, 0.7749999761581421>\n",
      "  batch loss & accuracy at step 71300: <0.5049416422843933, 1.0>\n",
      "  batch loss & accuracy at step 71400: <0.5251895189285278, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 71500: <0.5032572150230408, 1.0>\n",
      "  batch loss & accuracy at step 71600: <0.5450751781463623, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 71700: <0.7675982117652893, 0.550000011920929>\n",
      "  batch loss & accuracy at step 71800: <0.5118036866188049, 1.0>\n",
      "  batch loss & accuracy at step 71900: <0.793971061706543, 0.5>\n",
      "  batch loss & accuracy at step 72000: <0.5664348006248474, 0.925000011920929>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5409438610076904, 0.9236749410629272>\n",
      "\n",
      "\n",
      "Epoch  73\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 72100: <0.5750921368598938, 0.8750000596046448>\n",
      "  batch loss & accuracy at step 72200: <0.5032497644424438, 1.0>\n",
      "  batch loss & accuracy at step 72300: <0.5387097597122192, 0.949999988079071>\n",
      "  batch loss & accuracy at step 72400: <0.5470209717750549, 1.0>\n",
      "  batch loss & accuracy at step 72500: <0.5099763870239258, 1.0>\n",
      "  batch loss & accuracy at step 72600: <0.509023904800415, 1.0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch loss & accuracy at step 72700: <0.5422528982162476, 1.0>\n",
      "  batch loss & accuracy at step 72800: <0.5301243662834167, 1.0>\n",
      "  batch loss & accuracy at step 72900: <0.5051999688148499, 1.0>\n",
      "  batch loss & accuracy at step 73000: <0.7259360551834106, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5402033925056458, 0.9260751008987427>\n",
      "\n",
      "\n",
      "Epoch  74\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 73100: <0.6414616703987122, 0.6500000357627869>\n",
      "  batch loss & accuracy at step 73200: <0.503881573677063, 1.0>\n",
      "  batch loss & accuracy at step 73300: <0.5111040472984314, 1.0>\n",
      "  batch loss & accuracy at step 73400: <0.5129544734954834, 1.0>\n",
      "  batch loss & accuracy at step 73500: <0.5148286819458008, 1.0>\n",
      "  batch loss & accuracy at step 73600: <0.5982259511947632, 0.824999988079071>\n",
      "  batch loss & accuracy at step 73700: <0.5092711448669434, 1.0>\n",
      "  batch loss & accuracy at step 73800: <0.5250946879386902, 1.0>\n",
      "  batch loss & accuracy at step 73900: <0.5109062790870667, 1.0>\n",
      "  batch loss & accuracy at step 74000: <0.5577054619789124, 0.8750000596046448>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5383669137954712, 0.9317750334739685>\n",
      "\n",
      "\n",
      "Epoch  75\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 74100: <0.503253161907196, 1.0>\n",
      "  batch loss & accuracy at step 74200: <0.7214756608009338, 0.625>\n",
      "  batch loss & accuracy at step 74300: <0.5032689571380615, 1.0>\n",
      "  batch loss & accuracy at step 74400: <0.5108219385147095, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 74500: <0.5035558938980103, 1.0>\n",
      "  batch loss & accuracy at step 74600: <0.503231942653656, 1.0>\n",
      "  batch loss & accuracy at step 74700: <0.5755888819694519, 0.824999988079071>\n",
      "  batch loss & accuracy at step 74800: <0.5093521475791931, 1.0>\n",
      "  batch loss & accuracy at step 74900: <0.7822033166885376, 0.5>\n",
      "  batch loss & accuracy at step 75000: <0.510035514831543, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5371099710464478, 0.9352749586105347>\n",
      "\n",
      "\n",
      "Epoch  76\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 75100: <0.522049605846405, 0.949999988079071>\n",
      "  batch loss & accuracy at step 75200: <0.5056051015853882, 1.0>\n",
      "  batch loss & accuracy at step 75300: <0.52137291431427, 0.925000011920929>\n",
      "  batch loss & accuracy at step 75400: <0.5097766518592834, 1.0>\n",
      "  batch loss & accuracy at step 75500: <0.5610501766204834, 0.925000011920929>\n",
      "  batch loss & accuracy at step 75600: <0.5144228935241699, 1.0>\n",
      "  batch loss & accuracy at step 75700: <0.6221435070037842, 0.7750000357627869>\n",
      "  batch loss & accuracy at step 75800: <0.5032506585121155, 1.0>\n",
      "  batch loss & accuracy at step 75900: <0.5982249975204468, 0.75>\n",
      "  batch loss & accuracy at step 76000: <0.5057687163352966, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5370860695838928, 0.9349249601364136>\n",
      "\n",
      "\n",
      "Epoch  77\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 76100: <0.5068691968917847, 1.0>\n",
      "  batch loss & accuracy at step 76200: <0.6147140860557556, 0.800000011920929>\n",
      "  batch loss & accuracy at step 76300: <0.5699915885925293, 0.824999988079071>\n",
      "  batch loss & accuracy at step 76400: <0.50478196144104, 1.0>\n",
      "  batch loss & accuracy at step 76500: <0.5235956907272339, 1.0>\n",
      "  batch loss & accuracy at step 76600: <0.5032906532287598, 1.0>\n",
      "  batch loss & accuracy at step 76700: <0.538467526435852, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 76800: <0.5134726762771606, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 76900: <0.5219032168388367, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 77000: <0.6243520975112915, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.531493067741394, 0.9464249610900879>\n",
      "\n",
      "\n",
      "Epoch  78\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 77100: <0.5032501220703125, 1.0>\n",
      "  batch loss & accuracy at step 77200: <0.6296029686927795, 0.75>\n",
      "  batch loss & accuracy at step 77300: <0.5033810138702393, 1.0>\n",
      "  batch loss & accuracy at step 77400: <0.5079408288002014, 1.0>\n",
      "  batch loss & accuracy at step 77500: <0.628191351890564, 0.800000011920929>\n",
      "  batch loss & accuracy at step 77600: <0.5763292908668518, 0.7749999761581421>\n",
      "  batch loss & accuracy at step 77700: <0.714279294013977, 0.550000011920929>\n",
      "  batch loss & accuracy at step 77800: <0.5824517607688904, 0.925000011920929>\n",
      "  batch loss & accuracy at step 77900: <0.5409668684005737, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 78000: <0.613445520401001, 0.8500000238418579>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5314827561378479, 0.946899950504303>\n",
      "\n",
      "\n",
      "Epoch  79\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 78100: <0.5033131837844849, 1.0>\n",
      "  batch loss & accuracy at step 78200: <0.5033868551254272, 1.0>\n",
      "  batch loss & accuracy at step 78300: <0.524620532989502, 0.925000011920929>\n",
      "  batch loss & accuracy at step 78400: <0.5713856816291809, 0.7999999523162842>\n",
      "  batch loss & accuracy at step 78500: <0.5270684361457825, 0.949999988079071>\n",
      "  batch loss & accuracy at step 78600: <0.5103169679641724, 1.0>\n",
      "  batch loss & accuracy at step 78700: <0.5035482048988342, 1.0>\n",
      "  batch loss & accuracy at step 78800: <0.5949842929840088, 0.75>\n",
      "  batch loss & accuracy at step 78900: <0.5189118385314941, 1.0>\n",
      "  batch loss & accuracy at step 79000: <0.5105860233306885, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5337470769882202, 0.9431500434875488>\n",
      "\n",
      "\n",
      "Epoch  80\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 79100: <0.5086941719055176, 1.0>\n",
      "  batch loss & accuracy at step 79200: <0.5083300471305847, 1.0>\n",
      "  batch loss & accuracy at step 79300: <0.5032098889350891, 1.0>\n",
      "  batch loss & accuracy at step 79400: <0.5138744711875916, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 79500: <0.5880453586578369, 0.8500000238418579>\n",
      "  batch loss & accuracy at step 79600: <0.5033563375473022, 1.0>\n",
      "  batch loss & accuracy at step 79700: <0.545962393283844, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 79800: <0.5339006185531616, 0.949999988079071>\n",
      "  batch loss & accuracy at step 79900: <0.5152880549430847, 1.0>\n",
      "  batch loss & accuracy at step 80000: <0.5911440849304199, 0.875>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5298864841461182, 0.9526000618934631>\n",
      "\n",
      "\n",
      "Epoch  81\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 80100: <0.5032583475112915, 1.0>\n",
      "  batch loss & accuracy at step 80200: <0.5883262157440186, 0.75>\n",
      "  batch loss & accuracy at step 80300: <0.5045090913772583, 1.0>\n",
      "  batch loss & accuracy at step 80400: <0.5612196922302246, 0.8750000596046448>\n",
      "  batch loss & accuracy at step 80500: <0.6486504077911377, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 80600: <0.5033355951309204, 1.0>\n",
      "  batch loss & accuracy at step 80700: <0.5393912196159363, 0.8750000596046448>\n",
      "  batch loss & accuracy at step 80800: <0.5033270716667175, 1.0>\n",
      "  batch loss & accuracy at step 80900: <0.5499703288078308, 0.8999999761581421>\n",
      "  batch loss & accuracy at step 81000: <0.503248929977417, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.528244137763977, 0.9554000496864319>\n",
      "\n",
      "\n",
      "Epoch  82\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 81100: <0.6289032697677612, 0.8250000476837158>\n",
      "  batch loss & accuracy at step 81200: <0.5037796497344971, 1.0>\n",
      "  batch loss & accuracy at step 81300: <0.5161988139152527, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 81400: <0.5032224655151367, 1.0>\n",
      "  batch loss & accuracy at step 81500: <0.5070553421974182, 1.0>\n",
      "  batch loss & accuracy at step 81600: <0.5035279989242554, 1.0>\n",
      "  batch loss & accuracy at step 81700: <0.5225049257278442, 1.0>\n",
      "  batch loss & accuracy at step 81800: <0.5121431946754456, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 81900: <0.5052838325500488, 1.0>\n",
      "  batch loss & accuracy at step 82000: <0.5645449757575989, 0.925000011920929>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5277431607246399, 0.9555250406265259>\n",
      "\n",
      "\n",
      "Epoch  83\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 82100: <0.5299234986305237, 0.925000011920929>\n",
      "  batch loss & accuracy at step 82200: <0.5036435723304749, 1.0>\n",
      "  batch loss & accuracy at step 82300: <0.5036256313323975, 1.0>\n",
      "  batch loss & accuracy at step 82400: <0.5032633543014526, 1.0>\n",
      "  batch loss & accuracy at step 82500: <0.5288898348808289, 1.0>\n",
      "  batch loss & accuracy at step 82600: <0.5091509819030762, 1.0>\n",
      "  batch loss & accuracy at step 82700: <0.50323885679245, 1.0>\n",
      "  batch loss & accuracy at step 82800: <0.5287312269210815, 0.9249999523162842>\n",
      "  batch loss & accuracy at step 82900: <0.5032429695129395, 1.0>\n",
      "  batch loss & accuracy at step 83000: <0.5032675862312317, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5274891257286072, 0.9589499831199646>\n",
      "\n",
      "\n",
      "Epoch  84\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch loss & accuracy at step 83100: <0.6916308403015137, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 83200: <0.5034245848655701, 1.0>\n",
      "  batch loss & accuracy at step 83300: <0.504574716091156, 1.0>\n",
      "  batch loss & accuracy at step 83400: <0.5057514309883118, 1.0>\n",
      "  batch loss & accuracy at step 83500: <0.5316415429115295, 0.925000011920929>\n",
      "  batch loss & accuracy at step 83600: <0.5538402795791626, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 83700: <0.5340368747711182, 0.949999988079071>\n",
      "  batch loss & accuracy at step 83800: <0.5225213766098022, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 83900: <0.507955014705658, 1.0>\n",
      "  batch loss & accuracy at step 84000: <0.5225496292114258, 0.9749999642372131>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.527116596698761, 0.9585999846458435>\n",
      "\n",
      "\n",
      "Epoch  85\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 84100: <0.5054488182067871, 1.0>\n",
      "  batch loss & accuracy at step 84200: <0.504226803779602, 1.0>\n",
      "  batch loss & accuracy at step 84300: <0.503237783908844, 1.0>\n",
      "  batch loss & accuracy at step 84400: <0.5522885322570801, 0.8500000238418579>\n",
      "  batch loss & accuracy at step 84500: <0.5053519606590271, 1.0>\n",
      "  batch loss & accuracy at step 84600: <0.5207886099815369, 0.949999988079071>\n",
      "  batch loss & accuracy at step 84700: <0.5585077404975891, 0.949999988079071>\n",
      "  batch loss & accuracy at step 84800: <0.7613210678100586, 0.5>\n",
      "  batch loss & accuracy at step 84900: <0.5324180126190186, 0.925000011920929>\n",
      "  batch loss & accuracy at step 85000: <0.5081359148025513, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5252344012260437, 0.9628249406814575>\n",
      "\n",
      "\n",
      "Epoch  86\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 85100: <0.5078849792480469, 1.0>\n",
      "  batch loss & accuracy at step 85200: <0.5073133111000061, 1.0>\n",
      "  batch loss & accuracy at step 85300: <0.5056793689727783, 1.0>\n",
      "  batch loss & accuracy at step 85400: <0.5090507864952087, 1.0>\n",
      "  batch loss & accuracy at step 85500: <0.5156891942024231, 1.0>\n",
      "  batch loss & accuracy at step 85600: <0.5032582879066467, 1.0>\n",
      "  batch loss & accuracy at step 85700: <0.5239012241363525, 0.9750000238418579>\n",
      "  batch loss & accuracy at step 85800: <0.5033554434776306, 1.0>\n",
      "  batch loss & accuracy at step 85900: <0.5116823315620422, 1.0>\n",
      "  batch loss & accuracy at step 86000: <0.5335003137588501, 0.9749999642372131>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5233397483825684, 0.9674500226974487>\n",
      "\n",
      "\n",
      "Epoch  87\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 86100: <0.5689420700073242, 0.925000011920929>\n",
      "  batch loss & accuracy at step 86200: <0.6584295630455017, 0.6500000357627869>\n",
      "  batch loss & accuracy at step 86300: <0.5033356547355652, 1.0>\n",
      "  batch loss & accuracy at step 86400: <0.5115945339202881, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 86500: <0.721896231174469, 0.5249999761581421>\n",
      "  batch loss & accuracy at step 86600: <0.5051090717315674, 1.0>\n",
      "  batch loss & accuracy at step 86700: <0.5162614583969116, 1.0>\n",
      "  batch loss & accuracy at step 86800: <0.5075482726097107, 1.0>\n",
      "  batch loss & accuracy at step 86900: <0.5035074949264526, 1.0>\n",
      "  batch loss & accuracy at step 87000: <0.5036727786064148, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5241291522979736, 0.9650750756263733>\n",
      "\n",
      "\n",
      "Epoch  88\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 87100: <0.5078200697898865, 1.0>\n",
      "  batch loss & accuracy at step 87200: <0.5151731967926025, 1.0>\n",
      "  batch loss & accuracy at step 87300: <0.5083597898483276, 1.0>\n",
      "  batch loss & accuracy at step 87400: <0.5037332773208618, 1.0>\n",
      "  batch loss & accuracy at step 87500: <0.533716082572937, 0.949999988079071>\n",
      "  batch loss & accuracy at step 87600: <0.504957377910614, 1.0>\n",
      "  batch loss & accuracy at step 87700: <0.5092081427574158, 1.0>\n",
      "  batch loss & accuracy at step 87800: <0.5216889381408691, 1.0>\n",
      "  batch loss & accuracy at step 87900: <0.5039643049240112, 1.0>\n",
      "  batch loss & accuracy at step 88000: <0.5520952939987183, 0.875>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5226292014122009, 0.9678750038146973>\n",
      "\n",
      "\n",
      "Epoch  89\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 88100: <0.504490315914154, 1.0>\n",
      "  batch loss & accuracy at step 88200: <0.5032203197479248, 1.0>\n",
      "  batch loss & accuracy at step 88300: <0.5127320885658264, 1.0>\n",
      "  batch loss & accuracy at step 88400: <0.5734807252883911, 0.824999988079071>\n",
      "  batch loss & accuracy at step 88500: <0.5133874416351318, 0.949999988079071>\n",
      "  batch loss & accuracy at step 88600: <0.520878791809082, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 88700: <0.5043433904647827, 1.0>\n",
      "  batch loss & accuracy at step 88800: <0.5032241344451904, 1.0>\n",
      "  batch loss & accuracy at step 88900: <0.5055577754974365, 1.0>\n",
      "  batch loss & accuracy at step 89000: <0.5053309202194214, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5208935737609863, 0.9713499546051025>\n",
      "\n",
      "\n",
      "Epoch  90\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 89100: <0.5733025074005127, 0.800000011920929>\n",
      "  batch loss & accuracy at step 89200: <0.5076329708099365, 1.0>\n",
      "  batch loss & accuracy at step 89300: <0.5246939659118652, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 89400: <0.5135550498962402, 1.0>\n",
      "  batch loss & accuracy at step 89500: <0.5045739412307739, 1.0>\n",
      "  batch loss & accuracy at step 89600: <0.5036996006965637, 1.0>\n",
      "  batch loss & accuracy at step 89700: <0.5043343901634216, 1.0>\n",
      "  batch loss & accuracy at step 89800: <0.5639042854309082, 0.875>\n",
      "  batch loss & accuracy at step 89900: <0.5262715816497803, 0.949999988079071>\n",
      "  batch loss & accuracy at step 90000: <0.5058454871177673, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5208125114440918, 0.9730250239372253>\n",
      "\n",
      "\n",
      "Epoch  91\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 90100: <0.5033321380615234, 1.0>\n",
      "  batch loss & accuracy at step 90200: <0.531563401222229, 0.925000011920929>\n",
      "  batch loss & accuracy at step 90300: <0.5262660384178162, 0.949999988079071>\n",
      "  batch loss & accuracy at step 90400: <0.5107641220092773, 1.0>\n",
      "  batch loss & accuracy at step 90500: <0.5081887245178223, 1.0>\n",
      "  batch loss & accuracy at step 90600: <0.5047646760940552, 1.0>\n",
      "  batch loss & accuracy at step 90700: <0.5436946153640747, 0.949999988079071>\n",
      "  batch loss & accuracy at step 90800: <0.5373913049697876, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 90900: <0.5114680528640747, 1.0>\n",
      "  batch loss & accuracy at step 91000: <0.5125668048858643, 0.9749999642372131>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5221526622772217, 0.9695000052452087>\n",
      "\n",
      "\n",
      "Epoch  92\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 91100: <0.5134130716323853, 1.0>\n",
      "  batch loss & accuracy at step 91200: <0.5094910264015198, 1.0>\n",
      "  batch loss & accuracy at step 91300: <0.5032091736793518, 1.0>\n",
      "  batch loss & accuracy at step 91400: <0.768132746219635, 0.5>\n",
      "  batch loss & accuracy at step 91500: <0.5037458539009094, 1.0>\n",
      "  batch loss & accuracy at step 91600: <0.5101596117019653, 1.0>\n",
      "  batch loss & accuracy at step 91700: <0.5087881684303284, 1.0>\n",
      "  batch loss & accuracy at step 91800: <0.5044484734535217, 1.0>\n",
      "  batch loss & accuracy at step 91900: <0.5676660537719727, 0.949999988079071>\n",
      "  batch loss & accuracy at step 92000: <0.5253780484199524, 0.9749999642372131>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5202061533927917, 0.9742500185966492>\n",
      "\n",
      "\n",
      "Epoch  93\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 92100: <0.5497365593910217, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 92200: <0.5117517709732056, 1.0>\n",
      "  batch loss & accuracy at step 92300: <0.5123763680458069, 1.0>\n",
      "  batch loss & accuracy at step 92400: <0.5032131671905518, 1.0>\n",
      "  batch loss & accuracy at step 92500: <0.5522615909576416, 0.8750000596046448>\n",
      "  batch loss & accuracy at step 92600: <0.506100594997406, 1.0>\n",
      "  batch loss & accuracy at step 92700: <0.5356177091598511, 1.0>\n",
      "  batch loss & accuracy at step 92800: <0.5047680139541626, 1.0>\n",
      "  batch loss & accuracy at step 92900: <0.5073038339614868, 1.0>\n",
      "  batch loss & accuracy at step 93000: <0.5202507972717285, 0.9750000238418579>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5202246308326721, 0.9745749235153198>\n",
      "\n",
      "\n",
      "Epoch  94\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 93100: <0.6277188062667847, 0.7999999523162842>\n",
      "  batch loss & accuracy at step 93200: <0.5032647252082825, 1.0>\n",
      "  batch loss & accuracy at step 93300: <0.5063040256500244, 1.0>\n",
      "  batch loss & accuracy at step 93400: <0.5045539140701294, 1.0>\n",
      "  batch loss & accuracy at step 93500: <0.5936509966850281, 0.675000011920929>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch loss & accuracy at step 93600: <0.5032482743263245, 1.0>\n",
      "  batch loss & accuracy at step 93700: <0.5032118558883667, 1.0>\n",
      "  batch loss & accuracy at step 93800: <0.5182570219039917, 1.0>\n",
      "  batch loss & accuracy at step 93900: <0.5050915479660034, 1.0>\n",
      "  batch loss & accuracy at step 94000: <0.5043675899505615, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5174872279167175, 0.9795500040054321>\n",
      "\n",
      "\n",
      "Epoch  95\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 94100: <0.5032058954238892, 1.0>\n",
      "  batch loss & accuracy at step 94200: <0.5076834559440613, 1.0>\n",
      "  batch loss & accuracy at step 94300: <0.5032087564468384, 1.0>\n",
      "  batch loss & accuracy at step 94400: <0.5124678611755371, 1.0>\n",
      "  batch loss & accuracy at step 94500: <0.5039421916007996, 1.0>\n",
      "  batch loss & accuracy at step 94600: <0.5050219297409058, 1.0>\n",
      "  batch loss & accuracy at step 94700: <0.5071864128112793, 1.0>\n",
      "  batch loss & accuracy at step 94800: <0.5279756784439087, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 94900: <0.5047625303268433, 1.0>\n",
      "  batch loss & accuracy at step 95000: <0.503270149230957, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5170391798019409, 0.9799749851226807>\n",
      "\n",
      "\n",
      "Epoch  96\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 95100: <0.5077969431877136, 1.0>\n",
      "  batch loss & accuracy at step 95200: <0.5035731196403503, 1.0>\n",
      "  batch loss & accuracy at step 95300: <0.5240383148193359, 0.949999988079071>\n",
      "  batch loss & accuracy at step 95400: <0.5122348070144653, 1.0>\n",
      "  batch loss & accuracy at step 95500: <0.5032537579536438, 1.0>\n",
      "  batch loss & accuracy at step 95600: <0.5063214898109436, 1.0>\n",
      "  batch loss & accuracy at step 95700: <0.5034064054489136, 1.0>\n",
      "  batch loss & accuracy at step 95800: <0.5070772171020508, 1.0>\n",
      "  batch loss & accuracy at step 95900: <0.5216956734657288, 1.0>\n",
      "  batch loss & accuracy at step 96000: <0.508651614189148, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5157595872879028, 0.9816250205039978>\n",
      "\n",
      "\n",
      "Epoch  97\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 96100: <0.503872275352478, 1.0>\n",
      "  batch loss & accuracy at step 96200: <0.5066697001457214, 1.0>\n",
      "  batch loss & accuracy at step 96300: <0.5033017992973328, 1.0>\n",
      "  batch loss & accuracy at step 96400: <0.5052556395530701, 1.0>\n",
      "  batch loss & accuracy at step 96500: <0.5032444000244141, 1.0>\n",
      "  batch loss & accuracy at step 96600: <0.504589319229126, 1.0>\n",
      "  batch loss & accuracy at step 96700: <0.5247496366500854, 1.0>\n",
      "  batch loss & accuracy at step 96800: <0.506341814994812, 1.0>\n",
      "  batch loss & accuracy at step 96900: <0.5273632407188416, 1.0>\n",
      "  batch loss & accuracy at step 97000: <0.5033510327339172, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5177646279335022, 0.9782750010490417>\n",
      "\n",
      "\n",
      "Epoch  98\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 97100: <0.5032926797866821, 1.0>\n",
      "  batch loss & accuracy at step 97200: <0.5329692363739014, 0.949999988079071>\n",
      "  batch loss & accuracy at step 97300: <0.5098599791526794, 1.0>\n",
      "  batch loss & accuracy at step 97400: <0.505282461643219, 1.0>\n",
      "  batch loss & accuracy at step 97500: <0.5148085355758667, 1.0>\n",
      "  batch loss & accuracy at step 97600: <0.5033618807792664, 1.0>\n",
      "  batch loss & accuracy at step 97700: <0.5056228637695312, 1.0>\n",
      "  batch loss & accuracy at step 97800: <0.5037875175476074, 1.0>\n",
      "  batch loss & accuracy at step 97900: <0.532086193561554, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 98000: <0.5150945782661438, 0.9749999642372131>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5150686502456665, 0.9840500354766846>\n",
      "\n",
      "\n",
      "Epoch  99\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 98100: <0.5032426118850708, 1.0>\n",
      "  batch loss & accuracy at step 98200: <0.5060175061225891, 1.0>\n",
      "  batch loss & accuracy at step 98300: <0.5032088160514832, 1.0>\n",
      "  batch loss & accuracy at step 98400: <0.503425657749176, 1.0>\n",
      "  batch loss & accuracy at step 98500: <0.5083136558532715, 1.0>\n",
      "  batch loss & accuracy at step 98600: <0.520224928855896, 0.925000011920929>\n",
      "  batch loss & accuracy at step 98700: <0.5085598230361938, 1.0>\n",
      "  batch loss & accuracy at step 98800: <0.5302535891532898, 0.925000011920929>\n",
      "  batch loss & accuracy at step 98900: <0.5038236379623413, 1.0>\n",
      "  batch loss & accuracy at step 99000: <0.51396644115448, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5158108472824097, 0.9831750392913818>\n",
      "\n",
      "\n",
      "Epoch  100\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 99100: <0.5032345652580261, 1.0>\n",
      "  batch loss & accuracy at step 99200: <0.5135005712509155, 1.0>\n",
      "  batch loss & accuracy at step 99300: <0.517145574092865, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 99400: <0.5453006029129028, 0.8500000238418579>\n",
      "  batch loss & accuracy at step 99500: <0.5070406198501587, 1.0>\n",
      "  batch loss & accuracy at step 99600: <0.503354012966156, 1.0>\n",
      "  batch loss & accuracy at step 99700: <0.5298166871070862, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 99800: <0.503578782081604, 1.0>\n",
      "  batch loss & accuracy at step 99900: <0.5043134689331055, 1.0>\n",
      "  batch loss & accuracy at step 100000: <0.5033413767814636, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5163101553916931, 0.9815499782562256>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "NUM_BATCHES = 1000\n",
    "VERBOSE = 100\n",
    "\n",
    "loss_track, accuracy_track = [], []\n",
    "for e in range(NUM_EPOCHS):\n",
    "    print('Epoch ', e+1)\n",
    "    print('\\n')\n",
    "    curr_loss_track, curr_accuracy_track = [], []\n",
    "    for _ in range(NUM_BATCHES):\n",
    "        batch_x1, batch_x1_length, batch_x2, batch_x2_length, batch_ctx, batch_y = get_batch()\n",
    "        fd = {input_x1:batch_x1, input_x1_length:batch_x1_length,\n",
    "              input_x2:batch_x2, input_x2_length:batch_x2_length,\n",
    "              input_ctx:batch_ctx,\n",
    "              input_y:batch_y,\n",
    "              keep_prob:KEEP_PROB}\n",
    "        _, step, loss_, accuracy_ = sess.run([train_op, global_step, loss, accuracy], feed_dict=fd)\n",
    "        curr_loss_track.append(loss_)\n",
    "        curr_accuracy_track.append(accuracy_)\n",
    "        if step%VERBOSE==0:\n",
    "            print('  batch loss & accuracy at step {}: <{}, {}>'.format(step, loss_, accuracy_))\n",
    "    print('\\n')\n",
    "    print('  epoch mean loss & accuracy: <{}, {}>'.format(np.mean(curr_loss_track),np.mean(curr_accuracy_track)))\n",
    "    print('\\n')    \n",
    "    loss_track += curr_loss_track\n",
    "    accuracy_track += curr_accuracy_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/04233/sw33286/AIDA-MODEL-SAVE/MOCK/stacked-bilstm-cnn-context-mock-01'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Save model\n",
    "\n",
    "# saver = tf.train.Saver()\n",
    "# save_dir = \"/work/04233/sw33286/AIDA-MODEL-SAVE/MOCK/\"\n",
    "# save_path = save_dir + \"stacked-bilstm-cnn-context-mock-01\"\n",
    "# saver.save(sess, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM + HAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import colorama\n",
    "from colorama import Style\n",
    "\n",
    "class ClfHAC:\n",
    "    \n",
    "    def __init__(self, clf_dir, clf_filename):\n",
    "        self.sess = tf.Session()\n",
    "        saver = tf.train.import_meta_graph(clf_dir + clf_filename)\n",
    "        saver.restore(self.sess, tf.train.latest_checkpoint(clf_dir))\n",
    "        self.graph = tf.get_default_graph()\n",
    "        self.input_x1 = self.graph.get_tensor_by_name('input_x1:0')\n",
    "        self.input_x2 = self.graph.get_tensor_by_name('input_x2:0')\n",
    "        self.input_x1_length = self.graph.get_tensor_by_name('input_x1_length:0')\n",
    "        self.input_x2_length = self.graph.get_tensor_by_name('input_x2_length:0')\n",
    "        self.input_ctx = self.graph.get_tensor_by_name('input_ctx:0')\n",
    "        self.input_y = self.graph.get_tensor_by_name('input_y:0')\n",
    "        self.keep_prob = self.graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "        self.scores = self.graph.get_tensor_by_name('scores:0')\n",
    "        self.predictions = self.graph.get_tensor_by_name('predictions:0')\n",
    "        self.loss = self.graph.get_tensor_by_name('Loss/loss:0')\n",
    "        self.accuracy = self.graph.get_tensor_by_name('Accuracy/accuracy:0')\n",
    "        self.global_step = self.graph.get_tensor_by_name('global_step:0')\n",
    "        self.train_op = self.graph.get_tensor_by_name('train_op:0')\n",
    "     \n",
    "    def dist(self, x1, x2):\n",
    "        x1, x1_len = batch([x1])\n",
    "        x2, x2_len = batch([x2])\n",
    "        fd = {self.input_x1:x1, self.input_x1_length:x1_len,\n",
    "              self.input_x2:x2, self.input_x2_length:x2_len,\n",
    "              self.input_ctx:self.ctx,\n",
    "              self.keep_prob:1.0}\n",
    "        conf = self.sess.run(self.scores, feed_dict=fd)\n",
    "        return 1-conf[0]\n",
    "    \n",
    "    def evaluate(self, doc_mix, doc_lbs, ctx, method='average', plot=True):\n",
    "        self.ctx = ctx\n",
    "        doc_mix_sq, _ = batch(doc_mix)\n",
    "        doc_mix_sq = doc_mix_sq.T\n",
    "        doc_mix_clust = linkage(doc_mix_sq, method=method, metric=self.dist)\n",
    "        # evaluate by class-based prec/rec/f1\n",
    "        doc_prd = fcluster(doc_mix_clust, 2, criterion='maxclust') - 1 # predicted assignments (label adjusted)\n",
    "        acc = clust_accuracy(doc_lbs, doc_prd)\n",
    "        if plot:\n",
    "            print('Clustering accuracy = {}'.format(acc))\n",
    "            print('\\n')\n",
    "            plt.figure(figsize=(25, 10))\n",
    "            plt.title('Hierarchical Clustering Dendrogram')\n",
    "            plt.xlabel('sample index')\n",
    "            plt.ylabel('distance')\n",
    "            dendrogram(\n",
    "                doc_mix_clust,\n",
    "                leaf_rotation=90.,  # rotates the x axis labels\n",
    "                leaf_font_size=15.,  # font size for the x axis labels\n",
    "            )\n",
    "            plt.show() \n",
    "            print('True | Pred | Sentence')\n",
    "            for label,pred_label,code in zip(doc_lbs,doc_prd,doc_mix):\n",
    "                if label==0:\n",
    "                    print('\\033[1;37;40m {}    | {}    | {}'.format(label,pred_label,to_sent(code)))\n",
    "                else:\n",
    "                    print('\\033[1;30;47m {}    | {}    | {}'.format(label,pred_label,to_sent(code)))\n",
    "            print('\\n' + Style.RESET_ALL) \n",
    "        else:\n",
    "            return doc_mix_clust, acc\n",
    "\n",
    "def get_mixture(type1, type2):\n",
    "    doc_a = [get_rand_sent_code(type1, np.random.randint(SENT_FROM_LEN, SENT_TO_LEN)) for _ in range(DOC_LEN)]\n",
    "    doc_b = [get_rand_sent_code(type2, np.random.randint(SENT_FROM_LEN, SENT_TO_LEN)) for _ in range(DOC_LEN)]\n",
    "    doc_mix = np.array(doc_a[:] + doc_b[:])\n",
    "    doc_lbs = np.array([0]*DOC_LEN + [1]*DOC_LEN)\n",
    "    indices = list(range(DOC_LEN*2))\n",
    "    random.shuffle(indices)\n",
    "    doc_mix = doc_mix[indices]\n",
    "    doc_lbs = doc_lbs[indices]\n",
    "    return doc_a, doc_b, doc_mix, doc_lbs\n",
    "\n",
    "def get_rand_mixture():\n",
    "    type1, type2 = np.random.choice(TYPES, 2, replace=False)\n",
    "    _,_, doc_mix, doc_lbs = get_mixture(type1, type2)\n",
    "    doc_mix_flat = list(chain.from_iterable(doc_mix))\n",
    "    doc_mix_len = len(doc_mix_flat)\n",
    "    ctx = np.array([doc_mix_flat[:CTX_LEN]]) if doc_mix_len>=CTX_LEN else np.array([doc_mix_flat+[0]*(CTX_LEN-doc_mix_len)])\n",
    "    return doc_mix, doc_lbs, ctx\n",
    "\n",
    "def flip_clust(clust):\n",
    "    return np.array([0 if i==1 else 1 for i in clust])\n",
    "\n",
    "def clust_accuracy(true, pred):\n",
    "    return max(accuracy_score(true, pred),\n",
    "               accuracy_score(true, flip_clust(pred)))\n",
    "\n",
    "def to_sent(code):\n",
    "    return [indexer.get_object(idx) for idx in code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_dir = \"/work/04233/sw33286/AIDA-MODEL-SAVE/MOCK/\"\n",
    "restore_filename = 'stacked-bilstm-cnn-context-mock-01.meta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_hac = ClfHAC(restore_dir, restore_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering accuracy = 1.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB9YAAANVCAYAAAAtFBVGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZKkAUysS5IkSZIkSZIkSZI0wP8CFAdsCaxmAEsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b2822dbed30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True | Pred | Sentence\n",
      "\u001b[1;37;40m 0    | 1    | ['tomato', 'share1', 'pear', 'tomato', 'apple']\n",
      "\u001b[1;37;40m 0    | 1    | ['pear', 'tomato', 'tomato', 'pear', 'strawberry', 'tomato', 'grape', 'apple', 'strawberry', 'pear', 'apple', 'strawberry']\n",
      "\u001b[1;30;47m 1    | 0    | ['share1', 'river', 'share1', 'lake', 'share1', 'share2', 'river', 'lake', 'lake', 'lake', 'hill', 'valley']\n",
      "\u001b[1;30;47m 1    | 0    | ['share2', 'mountain', 'lake', 'mountain', 'share1', 'hill', 'valley', 'valley', 'valley', 'valley', 'river', 'valley', 'mountain']\n",
      "\u001b[1;37;40m 0    | 1    | ['apple', 'tomato', 'tomato', 'share2', 'share2', 'apple', 'strawberry', 'tomato', 'pear']\n",
      "\u001b[1;30;47m 1    | 0    | ['river', 'hill', 'share2', 'share2', 'lake', 'share1', 'river', 'valley', 'hill', 'share1', 'lake', 'share1', 'valley']\n",
      "\u001b[1;37;40m 0    | 1    | ['share1', 'strawberry', 'grape', 'tomato', 'share2', 'share1', 'tomato', 'grape', 'share1', 'share1', 'share2']\n",
      "\u001b[1;30;47m 1    | 0    | ['lake', 'lake', 'share1', 'river', 'lake', 'share1', 'valley', 'lake', 'share1', 'share1', 'river']\n",
      "\u001b[1;30;47m 1    | 0    | ['mountain', 'mountain', 'river', 'share1', 'valley', 'share1']\n",
      "\u001b[1;37;40m 0    | 1    | ['apple', 'grape', 'share1', 'share2', 'grape', 'pear']\n",
      "\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "clf_hac.evaluate(*get_rand_mixture())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_evaluation(k=1000):\n",
    "    accuracies = []\n",
    "    for _ in range(k):\n",
    "        _, acc = clf_hac.evaluate(*get_rand_mixture(), plot=False)\n",
    "        accuracies.append(acc)\n",
    "    print('Average clustering accuracy over {} samples = {}'.format(k, np.mean(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average clustering accuracy over 1000 samples = 0.9987\n",
      "CPU times: user 54min 26s, sys: 6min 37s, total: 1h 1min 3s\n",
      "Wall time: 22min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rand_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
