{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/work/04233/sw33286/AIDA-SCRIPTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from helpers import Indexer\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES = ['ANIMAL','VEHICLE','NATURE','FURNITURE','FRUIT']\n",
    "TYPE2VOCAB = {'ANIMAL': ['cat','dog','pig','horse','deer'],\n",
    "              'VEHICLE': ['car','bike','motorcycle','train','bus'],\n",
    "              'NATURE': ['hill','mountain','lake','river','valley'],\n",
    "              'FURNITURE': ['stool','table','closet','cabinet','bed'],\n",
    "              'FRUIT': ['apple','pear','strawberry','grape','tomato']}\n",
    "VOCAB = list(chain.from_iterable(TYPE2VOCAB.values()))\n",
    "\n",
    "indexer = Indexer()\n",
    "indexer.get_index('PAD')\n",
    "for word in VOCAB:\n",
    "    indexer.get_index(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM_LEN = 5\n",
    "TO_LEN = 15\n",
    "\n",
    "def generate_datum(from_len=FROM_LEN, to_len=TO_LEN):\n",
    "    y = 0 if np.random.rand() < 0.5 else 1\n",
    "    x1_length = np.random.randint(from_len, to_len)\n",
    "    x2_length = np.random.randint(from_len, to_len)\n",
    "    x1_type = np.random.choice(TYPES)\n",
    "    x2_type = np.random.choice([t for t in TYPES if t!=x1_type]) if y==0 else x1_type\n",
    "    \n",
    "    x1_code = [indexer.get_index(np.random.choice(TYPE2VOCAB[x1_type])) for _ in range(x1_length)]\n",
    "    x2_code = [indexer.get_index(np.random.choice(TYPE2VOCAB[x2_type])) for _ in range(x2_length)]\n",
    "    if x1_length < to_len:\n",
    "        x1_code += [indexer.get_index('PAD')] * (to_len-x1_length)\n",
    "    if x2_length < to_len:\n",
    "        x2_code += [indexer.get_index('PAD')] * (to_len-x2_length)\n",
    "    return x1_code, x2_code, y\n",
    "\n",
    "def to_sent(code):\n",
    "    return [indexer.get_object(idx) for idx in code]\n",
    "\n",
    "def get_batch(n, from_len=FROM_LEN, to_len=TO_LEN):\n",
    "    x1_batch, x2_batch, y_batch = [], [], []\n",
    "    for _ in range(n):\n",
    "        x1_code, x2_code, y = generate_datum(from_len, to_len)\n",
    "        x1_batch.append(x1_code)\n",
    "        x2_batch.append(x2_code)\n",
    "        y_batch.append(y)\n",
    "    return np.array(x1_batch), np.array(x2_batch), np.array(y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "MAX_LEN = TO_LEN\n",
    "VOCAB_SIZE = len(indexer)\n",
    "EMBED_SIZE = 20\n",
    "FILTER_SIZES = [3,4,5]\n",
    "NUM_FILTERS = 10\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "input_x1 = tf.placeholder(tf.int32, [None, MAX_LEN], name='input_x1')\n",
    "input_x2 = tf.placeholder(tf.int32, [None, MAX_LEN], name='input_x2')\n",
    "input_y  = tf.placeholder(tf.int32, [None], name='input_y')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "with tf.device('/cpu:0'), tf.variable_scope('embeddings'): \n",
    "        # name_scope works only with tf.Variable\n",
    "        # variable_scope works with tf.get_variable\n",
    "    E = tf.get_variable('E', [VOCAB_SIZE, EMBED_SIZE], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    embed_x1 = tf.expand_dims(tf.nn.embedding_lookup(E, input_x1), -1)\n",
    "    embed_x2 = tf.expand_dims(tf.nn.embedding_lookup(E, input_x2), -1)\n",
    "        # embed_x*: [batch_size, height=MAX_LEN, width=EMBED_SIZE, num_channels=1]\n",
    "        \n",
    "pool1_outputs, pool2_outputs = [], []\n",
    "for i, filter_size in enumerate(FILTER_SIZES):\n",
    "    with tf.variable_scope('conv-max-pool-%s' % filter_size): \n",
    "        filter_shape = [filter_size, EMBED_SIZE, NUM_CHANNELS, NUM_FILTERS]\n",
    "            # Filter dims: [filter_size, emb_size, num_channels, num_filters]\n",
    "        W1 = tf.get_variable('W1', filter_shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        W2 = tf.get_variable('W2', filter_shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b1 = tf.get_variable('b1', [NUM_FILTERS], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b2 = tf.get_variable('b2', [NUM_FILTERS], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        conv1 = tf.nn.conv2d(embed_x1, W1, strides=[1,1,1,1], padding='VALID', name='conv1')\n",
    "        conv2 = tf.nn.conv2d(embed_x2, W2, strides=[1,1,1,1], padding='VALID', name='conv2')\n",
    "            # Conv dims: [batch_size, height, width, num_channels]\n",
    "        h1 = tf.nn.relu(tf.nn.bias_add(conv1, b1), name='relu1')\n",
    "        h2 = tf.nn.relu(tf.nn.bias_add(conv2, b2), name='relu2')\n",
    "        pool1 = tf.nn.max_pool(h1, ksize=[1,MAX_LEN-filter_size+1,1,1], strides=[1,1,1,1], padding='VALID', name='pool1')\n",
    "        pool2 = tf.nn.max_pool(h2, ksize=[1,MAX_LEN-filter_size+1,1,1], strides=[1,1,1,1], padding='VALID', name='pool2')\n",
    "            # kernel size (ksize): [batch_size, height, width, num_channels]\n",
    "        pool1_outputs.append(pool1)\n",
    "        pool2_outputs.append(pool2)\n",
    "\n",
    "num_filters_total = NUM_FILTERS * len(FILTER_SIZES)\n",
    "h_pool1_flat = tf.nn.dropout(tf.reshape(tf.concat(pool1_outputs, 3), [-1, num_filters_total]), keep_prob)\n",
    "h_pool2_flat = tf.nn.dropout(tf.reshape(tf.concat(pool2_outputs, 3), [-1, num_filters_total]), keep_prob)\n",
    "    # flat shape: [batch_size, num_filters_total].\n",
    "W_bi = tf.get_variable('W_bi', [num_filters_total, num_filters_total],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "scores = tf.nn.sigmoid(tf.diag_part(tf.matmul(tf.matmul(h_pool1_flat, W_bi), tf.transpose(h_pool2_flat))))\n",
    "\n",
    "predictions = tf.cast(tf.round(scores), tf.int32, name='predictions')\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(input_y, tf.float32), logits=scores)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_predictions = tf.equal(predictions, input_y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "    \n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 1500: <0.6947721242904663, 0.5625>\n",
      "  batch loss & accuracy at step 2000: <0.6969879865646362, 0.59375>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6937199831008911, 0.5033749938011169>\n",
      "\n",
      "\n",
      "Epoch  2\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 2500: <0.7057040333747864, 0.625>\n",
      "  batch loss & accuracy at step 3000: <0.6936823725700378, 0.40625>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6931237578392029, 0.4974375069141388>\n",
      "\n",
      "\n",
      "Epoch  3\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 3500: <0.6921899318695068, 0.53125>\n",
      "  batch loss & accuracy at step 4000: <0.6854833364486694, 0.65625>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6873135566711426, 0.5146562457084656>\n",
      "\n",
      "\n",
      "Epoch  4\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 4500: <0.6426358222961426, 0.59375>\n",
      "  batch loss & accuracy at step 5000: <0.5703009963035583, 0.8125>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6281496286392212, 0.7140937447547913>\n",
      "\n",
      "\n",
      "Epoch  5\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 5500: <0.602730929851532, 0.8125>\n",
      "  batch loss & accuracy at step 6000: <0.6447353363037109, 0.71875>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5826100707054138, 0.8671562671661377>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCH = 5\n",
    "NUM_BATCH = 1000\n",
    "BATCH_SIZE = 32\n",
    "VERBOSE = 500\n",
    "\n",
    "for e in range(NUM_EPOCH):\n",
    "    print('Epoch ', e+1)\n",
    "    print('\\n')\n",
    "    loss_track, accuracy_track = [], []\n",
    "    for _ in range(NUM_BATCH):\n",
    "        batch_x1, batch_x2, batch_y = get_batch(BATCH_SIZE)\n",
    "        fd = {input_x1:batch_x1, input_x2:batch_x2, input_y:batch_y, keep_prob:0.7}\n",
    "        _, step, loss_, accuracy_ = sess.run([train_op, global_step, loss, accuracy], feed_dict=fd)\n",
    "        loss_track.append(loss_)\n",
    "        accuracy_track.append(accuracy_)\n",
    "        if step%VERBOSE==0:\n",
    "            print('  batch loss & accuracy at step {}: <{}, {}>'.format(step, loss_, accuracy_))\n",
    "    print('\\n')\n",
    "    print('  epoch mean loss & accuracy: <{}, {}>'.format(np.mean(loss_track),np.mean(accuracy_track)))\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
