{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/work/04233/sw33286/AIDA-SCRIPTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.contrib.layers import safe_embedding_lookup_sparse as embedding_lookup_unique\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple, GRUCell\n",
    "\n",
    "from helpers import Indexer, batch\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = ['one','two','three','four','five',\n",
    "         'six','seven','eight','nine','ten']\n",
    "\n",
    "# VOCAB = ['cat','dog','pig','horse','deer',\n",
    "#          'car','bike','motorcycle','train','bus',\n",
    "#          'hill','mountain','lake','river','valley',\n",
    "#          'stool','table','closet','cabinet','bed',\n",
    "#          'apple','pear','strawberry','grape','tomato']\n",
    "\n",
    "indexer = Indexer()\n",
    "indexer.get_index('PAD')\n",
    "indexer.get_index('EOS')\n",
    "for word in VOCAB:\n",
    "    indexer.get_index(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM_LEN = 5\n",
    "TO_LEN = 15\n",
    "MAX_LEN = TO_LEN\n",
    "\n",
    "def generate_datum(from_len=FROM_LEN, to_len=TO_LEN, pad=False):\n",
    "    length = np.random.randint(from_len, to_len)\n",
    "    code = [indexer.get_index(np.random.choice(VOCAB)) for _ in range(length)]\n",
    "    if pad and length < MAX_LEN:\n",
    "        code += [indexer.get_index('PAD')] * (MAX_LEN-length)\n",
    "    return code\n",
    "\n",
    "def to_sent(code):\n",
    "    return [indexer.get_object(idx) for idx in code]\n",
    "\n",
    "def get_batch(n, from_len=FROM_LEN, to_len=TO_LEN):\n",
    "    return [generate_datum(pad=False) for _ in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "batch_size = 10\n",
    "vocab_size = len(indexer)\n",
    "embed_size = 20\n",
    "encoder_hidden_size = 10\n",
    "decoder_hidden_size = encoder_hidden_size*2\n",
    "latent_size = 20\n",
    "\n",
    "encoder_cell = LSTMCell(encoder_hidden_size)\n",
    "decoder_cell = LSTMCell(decoder_hidden_size)\n",
    "\n",
    "encoder_inputs = tf.placeholder(tf.int32, [None, None], name='encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(tf.int32, [None,], name='encoder_inputs_length')\n",
    "decoder_targets = tf.placeholder(tf.int32, [None, None], name='decoder_targets')\n",
    "decoder_targets_length = tf.placeholder(tf.int32, [None,], name='decoder_targets_length')\n",
    "\n",
    "with tf.name_scope('DecoderTrainFeeds'):\n",
    "    sequence_size, batch_size_ = tf.unstack(tf.shape(decoder_targets))\n",
    "    EOS_SLICE = tf.ones([1, batch_size_], dtype=tf.int32) * indexer.get_index('EOS')\n",
    "    PAD_SLICE = tf.ones([1, batch_size_], dtype=tf.int32) * indexer.get_index('PAD')\n",
    "    decoder_train_inputs = tf.concat([EOS_SLICE, decoder_targets], axis=0) # [max-time+1, batch_size]\n",
    "    decoder_train_length = decoder_targets_length + 1\n",
    "    decoder_train_targets = tf.concat([decoder_targets, PAD_SLICE], axis=0)\n",
    "    decoder_train_targets_seq_len,_ = tf.unstack(tf.shape(decoder_train_targets))\n",
    "    decoder_train_targets_eos_mask = tf.one_hot(decoder_train_length-1,\n",
    "                                                decoder_train_targets_seq_len,\n",
    "                                                on_value=indexer.get_index('EOS'), \n",
    "                                                off_value=indexer.get_index('PAD'),\n",
    "                                                dtype=tf.int32)\n",
    "    decoder_train_targets_eos_mask = tf.transpose(decoder_train_targets_eos_mask, [1,0])\n",
    "    decoder_train_targets = tf.add(decoder_train_targets,\n",
    "                                   decoder_train_targets_eos_mask)\n",
    "\n",
    "    loss_weights = tf.ones([\n",
    "        batch_size,\n",
    "        tf.reduce_max(decoder_train_length)\n",
    "    ], dtype=tf.float32, name='loss_weights')\n",
    "        # weights on predictions, usually set as uniform, unless otherwise is needed.\n",
    "\n",
    "with tf.variable_scope('embedding') as scope:\n",
    "    embedding_matrix = tf.get_variable('embedding_matrix', [vocab_size, embed_size], \n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "        # to use GloVe, do the following:\n",
    "        # glove_feed = tf.placeholder(tf.float32, glove_embs.shape)\n",
    "        # glove_init = embedding_matrix.assign(glove_feed)\n",
    "    encoder_inputs_embedded = tf.nn.embedding_lookup(embedding_matrix, encoder_inputs)\n",
    "    decoder_train_inputs_embedded = tf.nn.embedding_lookup(embedding_matrix, decoder_train_inputs)\n",
    "    \n",
    "with tf.variable_scope('BidirectionalEncoder') as scope:\n",
    "    encoder_cell = LSTMCell(encoder_hidden_size)\n",
    "    ((encoder_fw_outputs,encoder_bw_outputs),\n",
    "     (encoder_fw_state,encoder_bw_state)) = (\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                        cell_bw=encoder_cell,\n",
    "                                        inputs=encoder_inputs_embedded,\n",
    "                                        sequence_length=encoder_inputs_length,\n",
    "                                        dtype=tf.float32, time_major=True)\n",
    "    )\n",
    "    encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "    if isinstance(encoder_fw_state, LSTMStateTuple):\n",
    "        encoder_state_c = tf.concat((encoder_fw_state.c, encoder_bw_state.c), 1, name='bidirectional_concat_c')\n",
    "        \n",
    "        ## INSERTION 1. Variational reparameterization ##        \n",
    "        \n",
    "        encoder_state_h = tf.concat((encoder_fw_state.h, encoder_bw_state.h), 1, name='bidirectional_concat_h')\n",
    "            # [batch_size, encoder_hidden_size*2]\n",
    "        out_mean_w = tf.get_variable('out_mean_w', [encoder_hidden_size*2, latent_size], \n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "        out_mean_b = tf.get_variable('out_mean_b', [latent_size], \n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "        out_log_sigma_w = tf.get_variable('out_log_sigma_w', [encoder_hidden_size*2, latent_size], \n",
    "                                          initializer=tf.contrib.layers.xavier_initializer())\n",
    "        out_log_sigma_b = tf.get_variable('out_log_sigma_b', [latent_size], \n",
    "                                          initializer=tf.contrib.layers.xavier_initializer())  \n",
    "        z_mean = tf.add(tf.matmul(encoder_state_h, out_mean_w), out_mean_b)\n",
    "        z_log_sigma_sq = tf.add(tf.matmul(encoder_state_h, out_log_sigma_w), out_log_sigma_b)\n",
    "            # both are of [batch_size, latent_size]\n",
    "        eps = tf.random_normal((batch_size, latent_size), 0, 1, dtype=tf.float32)\n",
    "        z = tf.add(z_mean, tf.multiply(tf.sqrt(tf.exp(z_log_sigma_sq)), eps))\n",
    "        \n",
    "        encoder_state = LSTMStateTuple(c=encoder_state_c, h=z)\n",
    "        \n",
    "        #################################################\n",
    "        \n",
    "#         encoder_state = LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "        \n",
    "    elif isinstance(encoder_fw_state, tf.Tensor):\n",
    "        encoder_state = tf.concat((encoder_fw_state, encoder_bw_state), 1, name='bidirectional_concat')\n",
    "        \n",
    "with tf.variable_scope('Decoder') as scope:\n",
    "    def output_fn(outputs):\n",
    "        return tf.contrib.layers.linear(outputs, vocab_size, scope=scope)\n",
    "    attention_states = tf.transpose(encoder_outputs, [1,0,2]) # [batch_size,max-time,hidden_size]\n",
    "    (attention_keys,\n",
    "     attention_values,\n",
    "     attention_score_fn,\n",
    "     attention_construct_fn) = seq2seq.prepare_attention(\n",
    "        attention_states=attention_states,\n",
    "        attention_option='bahdanau',\n",
    "        num_units=decoder_hidden_size\n",
    "    )\n",
    "    decoder_fn_train = seq2seq.attention_decoder_fn_train(\n",
    "        encoder_state=encoder_state,\n",
    "        attention_keys=attention_keys,\n",
    "        attention_values=attention_values,\n",
    "        attention_score_fn=attention_score_fn,\n",
    "        attention_construct_fn=attention_construct_fn,\n",
    "        name='attention_decoder'\n",
    "    )\n",
    "    decoder_fn_inference = seq2seq.attention_decoder_fn_inference(\n",
    "        output_fn=output_fn,\n",
    "        encoder_state=encoder_state,\n",
    "        attention_keys=attention_keys,\n",
    "        attention_values=attention_values,\n",
    "        attention_score_fn=attention_score_fn,\n",
    "        attention_construct_fn=attention_construct_fn,\n",
    "        embeddings=embedding_matrix,\n",
    "        start_of_sequence_id=indexer.get_index('EOS'),\n",
    "        end_of_sequence_id=indexer.get_index('EOS'),\n",
    "        maximum_length=tf.reduce_max(encoder_inputs_length) + 3,\n",
    "        num_decoder_symbols=vocab_size\n",
    "    )\n",
    "    (decoder_outputs_train,\n",
    "     decoder_state_train,\n",
    "     decoder_context_state_train) = (\n",
    "        seq2seq.dynamic_rnn_decoder(\n",
    "            cell=decoder_cell,\n",
    "            decoder_fn=decoder_fn_train,\n",
    "            inputs=decoder_train_inputs_embedded,\n",
    "            sequence_length=decoder_train_length,\n",
    "            time_major=True,\n",
    "            scope=scope\n",
    "        )\n",
    "    )\n",
    "    decoder_logits_train = output_fn(decoder_outputs_train)\n",
    "    decoder_prediction_train = tf.argmax(decoder_logits_train, axis=-1, name='decoder_prediction_train')\n",
    "    scope.reuse_variables()\n",
    "    (decoder_logits_inference,\n",
    "     decoder_state_inference,\n",
    "     decoder_context_state_inference) = (\n",
    "        seq2seq.dynamic_rnn_decoder(\n",
    "            cell=decoder_cell,\n",
    "            decoder_fn=decoder_fn_inference,\n",
    "            time_major=True,\n",
    "            scope=scope\n",
    "        )\n",
    "    )\n",
    "    decoder_prediction_inference = tf.argmax(decoder_logits_inference, axis=-1, name='decoder_prediction_inference')\n",
    "\n",
    "    \n",
    "logits = tf.transpose(decoder_logits_train, [1,0,2])\n",
    "targets = tf.transpose(decoder_train_targets, [1,0])\n",
    "\n",
    "generation_loss = seq2seq.sequence_loss(logits=logits, targets=targets, weights=loss_weights) \n",
    "    # average gen-loss\n",
    "latent_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_sigma_sq - tf.square(z_mean) - tf.exp(z_log_sigma_sq), 1))  \n",
    "    # average ltn-loss\n",
    "loss = generation_loss + latent_loss\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_inputs(input_seq, target_seq):\n",
    "    inputs_, inputs_length_ = batch(input_seq)\n",
    "    targets_, targets_length_ = batch(target_seq)\n",
    "    return {\n",
    "        encoder_inputs: inputs_,\n",
    "        encoder_inputs_length: inputs_length_,\n",
    "        decoder_targets: targets_,\n",
    "        decoder_targets_length: targets_length_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_track = []\n",
    "num_batches = 50000\n",
    "verbose = 1000\n",
    "try:\n",
    "    for b in range(num_batches):\n",
    "        batch_inputs = get_batch(batch_size)\n",
    "        fd = make_train_inputs(batch_inputs, batch_inputs)\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        if b==0 or b%verbose==0:\n",
    "            print('batch {}'.format(b))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            for i,(e_in, dt_pred) in enumerate(zip(\n",
    "                fd[encoder_inputs].T, # [max-time,batch_size] -> [batch_size,max-time]\n",
    "                sess.run(decoder_prediction_train, fd).T\n",
    "            )):\n",
    "                print('  sample {}:'.format(i+1))\n",
    "                print('    enc input           > {}'.format([w for w in to_sent(e_in) if w!='PAD']))\n",
    "                print('    dec train predicted > {}'.format([w for w in to_sent(dt_pred) if w!='PAD']))\n",
    "                if i>=2:\n",
    "                    break\n",
    "            print('\\n')\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')\n",
    "    \n",
    "    \n",
    "# batch 47000\n",
    "#   minibatch loss: 0.0032705413177609444\n",
    "#   sample 1:\n",
    "#     enc input           > ['four', 'nine', 'two', 'seven', 'seven', 'four', 'eight', 'eight', 'four', 'six']\n",
    "#     dec train predicted > ['four', 'nine', 'two', 'seven', 'seven', 'four', 'eight', 'eight', 'four', 'six', 'EOS']\n",
    "#   sample 2:\n",
    "#     enc input           > ['nine', 'four', 'one', 'two', 'five', 'five', 'six', 'six']\n",
    "#     dec train predicted > ['nine', 'four', 'one', 'two', 'five', 'five', 'six', 'six', 'EOS']\n",
    "#   sample 3:\n",
    "#     enc input           > ['five', 'six', 'six', 'six', 'six', 'seven', 'nine', 'two', 'ten', 'six', 'three']\n",
    "#     dec train predicted > ['five', 'six', 'six', 'six', 'six', 'seven', 'nine', 'two', 'ten', 'six', 'three', 'EOS']\n",
    "\n",
    "\n",
    "# batch 48000\n",
    "#   minibatch loss: 0.0031417866703122854\n",
    "#   sample 1:\n",
    "#     enc input           > ['two', 'seven', 'five', 'seven', 'ten', 'five', 'five']\n",
    "#     dec train predicted > ['two', 'seven', 'five', 'seven', 'ten', 'five', 'five', 'EOS']\n",
    "#   sample 2:\n",
    "#     enc input           > ['four', 'two', 'three', 'three', 'six', 'four', 'three', 'two', 'nine', 'ten', 'eight', 'eight', 'one']\n",
    "#     dec train predicted > ['four', 'two', 'three', 'three', 'six', 'four', 'three', 'two', 'nine', 'ten', 'eight', 'eight', 'one', 'EOS']\n",
    "#   sample 3:\n",
    "#     enc input           > ['two', 'two', 'ten', 'two', 'three', 'seven']\n",
    "#     dec train predicted > ['two', 'two', 'ten', 'two', 'three', 'seven', 'EOS']\n",
    "\n",
    "\n",
    "# batch 49000\n",
    "#   minibatch loss: 0.002898368751630187\n",
    "#   sample 1:\n",
    "#     enc input           > ['six', 'four', 'eight', 'one', 'seven', 'nine', 'four', 'eight', 'two']\n",
    "#     dec train predicted > ['six', 'four', 'eight', 'one', 'seven', 'nine', 'four', 'eight', 'two', 'EOS']\n",
    "#   sample 2:\n",
    "#     enc input           > ['six', 'one', 'five', 'nine', 'four', 'four', 'four', 'five', 'one']\n",
    "#     dec train predicted > ['six', 'one', 'five', 'nine', 'four', 'four', 'four', 'five', 'one', 'EOS']\n",
    "#   sample 3:\n",
    "#     enc input           > ['five', 'six', 'seven', 'ten', 'ten', 'four', 'six', 'three', 'seven', 'eight', 'one']\n",
    "#     dec train predicted > ['five', 'six', 'seven', 'ten', 'ten', 'four', 'six', 'three', 'seven', 'eight', 'one', 'EOS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
