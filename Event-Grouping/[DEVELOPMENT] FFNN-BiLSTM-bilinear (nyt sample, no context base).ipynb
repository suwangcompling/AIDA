{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/work/04233/sw33286/AIDA-SCRIPTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell, MultiRNNCell, DropoutWrapper\n",
    "\n",
    "from helpers import Indexer, batch\n",
    "from itertools import chain, product\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100001, 300)\n"
     ]
    }
   ],
   "source": [
    "# Link to NYT data folder\n",
    "\n",
    "nyt_code_dir = \"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_event_sample_code/\"\n",
    "FILE_NAMES = os.listdir(nyt_code_dir)\n",
    "\n",
    "# Link to dictionary information\n",
    "\n",
    "info_path = \"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_info/indexer_word2emb_100k.p\"\n",
    "indexer100k, word2emb100k = dill.load(open(info_path, 'rb'))\n",
    "glove_embs = []\n",
    "for i in range(len(indexer100k)):\n",
    "    glove_embs.append(word2emb100k[indexer100k.get_object(i)])\n",
    "glove_embs = np.array(glove_embs)\n",
    "print(glove_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "NUM_EVENTS = 5\n",
    "NUM_WORDS = 5\n",
    "\n",
    "def get_batch(file_idx):\n",
    "    filename = FILE_NAMES[file_idx]\n",
    "    edoc_a, edoc_b, _ = dill.load(open(nyt_code_dir+FILE_NAMES[file_idx],'rb'))\n",
    "    size_a, size_b = len(edoc_a), len(edoc_b)\n",
    "    batch_x1, batch_x2, batch_y = [], [], []\n",
    "    ys = [1,0,0,1]\n",
    "    sizes = [[size_a,size_a],[size_a,size_b],[size_b,size_a],[size_b,size_b]]\n",
    "    for _ in range(BATCH_SIZE//4):\n",
    "        for i,(eda,edb) in enumerate(product([edoc_a,edoc_b],\n",
    "                                             [edoc_a,edoc_b])):\n",
    "            batch_x1.append(eda[np.random.randint(0,sizes[i][0])])\n",
    "            batch_x2.append(edb[np.random.randint(0,sizes[i][1])])\n",
    "            batch_y.append(ys[i])\n",
    "    return np.array(batch_x1), np.array(batch_x2), np.array(batch_y) \n",
    "        # batch_x*: <bc,ne,nw>, batch_y: <bc,>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFNN-BiLSTM-bilinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "VOCAB_SIZE, EMB_SIZE = glove_embs.shape\n",
    "HID_SIZE = 100 # let event embs be of the same hid-size as role-factored arg vectors.\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "KEEP_PROB = 0.7\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "input_x1 = tf.placeholder(tf.int32, [BATCH_SIZE, NUM_EVENTS, NUM_WORDS], name='input_x1')\n",
    "input_x2 = tf.placeholder(tf.int32, [BATCH_SIZE, NUM_EVENTS, NUM_WORDS], name='input_x2')\n",
    "input_y = tf.placeholder(tf.int32, [BATCH_SIZE], name='input_y')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "with tf.variable_scope('Embedding'):\n",
    "    embeddings = tf.get_variable('embedding', [VOCAB_SIZE, EMB_SIZE],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "    glove_init = embeddings.assign(glove_embs)\n",
    "    input_x1_embedded = tf.reshape(tf.nn.embedding_lookup(embeddings, input_x1), [BATCH_SIZE, NUM_EVENTS, -1]) \n",
    "        # op1. embed words: <bc,ne,nw,emb>\n",
    "        # op2. concat words in event: <bc,ne,nw*emb>\n",
    "    input_x2_embedded = tf.reshape(tf.nn.embedding_lookup(embeddings, input_x2), [BATCH_SIZE, NUM_EVENTS, -1])\n",
    "\n",
    "def run_ffnn(inputs): # inputs=<ne,nw*emb>\n",
    "    W_ffnn = tf.get_variable('W_ffnn', [NUM_WORDS*EMB_SIZE, EMB_SIZE], \n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "    return tf.matmul(inputs, W_ffnn) # <ne,emb>, event now has the same emb length as word.\n",
    "\n",
    "with tf.variable_scope('FFNN') as scope:\n",
    "    input_x1_ffnn = tf.transpose(tf.map_fn(run_ffnn, input_x1_embedded), [1,0,2]) \n",
    "        # op1. map_fn out: <bc,ne,emb>\n",
    "        # op2. for lstm input: <max-time=ne,bc,emb>\n",
    "    scope.reuse_variables()\n",
    "    input_x2_ffnn = tf.transpose(tf.map_fn(run_ffnn, input_x2_embedded), [1,0,2])\n",
    "\n",
    "cell = MultiRNNCell([DropoutWrapper(LSTMCell(HID_SIZE),output_keep_prob=keep_prob)]*NUM_LAYERS) \n",
    "    \n",
    "def run_lstm(inputs):\n",
    "    ((fw_outputs,bw_outputs), # <max-time=ne,bc,hid>, attention later if needed.\n",
    "     (fw_final_state,bw_final_state)) = ( # <bc,hid>\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw=cell,\n",
    "                                        cell_bw=cell,\n",
    "                                        inputs=inputs,\n",
    "                                        sequence_length=[NUM_EVENTS]*BATCH_SIZE,\n",
    "                                        dtype=tf.float32, time_major=True)\n",
    "    )    \n",
    "    return tf.concat([tf.concat([fw_state_tuple.h,bw_state_tuple.h], 1) # lstm-out size *= NUM_LAYERS by stacking.\n",
    "                      for fw_state_tuple,bw_state_tuple in zip(fw_final_state,bw_final_state)], 1)\n",
    "\n",
    "with tf.variable_scope('BiLSTM') as scope:\n",
    "    final_vec_x1 = run_lstm(input_x1_ffnn) # <bc,hid*2*num-layers>\n",
    "    scope.reuse_variables()\n",
    "    final_vec_x2 = run_lstm(input_x2_ffnn)\n",
    "\n",
    "final_vec_size = HID_SIZE*2*NUM_LAYERS    \n",
    "\n",
    "def run_scores(fv_x1, fv_x2):\n",
    "    W_bi = tf.get_variable('W_bi', [final_vec_size, final_vec_size], \n",
    "                           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    return tf.nn.sigmoid(tf.diag_part(tf.matmul(tf.matmul(final_vec_x1,W_bi),tf.transpose(final_vec_x2))),\n",
    "                         name='scores')\n",
    "\n",
    "scores = run_scores(final_vec_x1, final_vec_x2)\n",
    "predictions = tf.cast(tf.round(scores), tf.int32, name='predictions')     \n",
    "    \n",
    "with tf.name_scope('Loss'):\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(input_y, tf.float32), logits=scores)\n",
    "    loss = tf.reduce_mean(losses, name='loss')\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct_predictions = tf.equal(predictions, input_y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "    \n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step, name='train_op')\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "TRAIN_SIZE = 10\n",
    "VERBOSE = 1\n",
    "# TRAIN_SIZE = len(FILE_NAMES)\n",
    "# VERBOSE = 1000\n",
    "\n",
    "loss_track, accuracy_track = [], []\n",
    "try:\n",
    "    for e in range(NUM_EPOCHS):\n",
    "        print('Epoch ', e+1)\n",
    "        print('\\n')\n",
    "        file_indices = np.random.choice(list(range(len(FILE_NAMES))), size=TRAIN_SIZE, replace=False)\n",
    "        random.shuffle(file_indices)\n",
    "        curr_loss_track, curr_accuracy_track = [], []\n",
    "        for file_idx in file_indices:\n",
    "            batch_x1, batch_x2, batch_y = get_batch(file_idx)\n",
    "            fd = {input_x1:batch_x1, input_x2:batch_x2,\n",
    "                  input_y:batch_y,\n",
    "                  keep_prob:KEEP_PROB}\n",
    "            _, step, loss_, accuracy_ = sess.run([train_op, global_step, loss, accuracy], feed_dict=fd)\n",
    "            curr_loss_track.append(loss_)\n",
    "            curr_accuracy_track.append(accuracy_)\n",
    "            if step%VERBOSE==0:\n",
    "                print(' average batch loss & accuracy at step {}: <{}, {}>'.format(step,\n",
    "                                                                                   np.mean(curr_loss_track), \n",
    "                                                                                   np.mean(curr_accuracy_track)))\n",
    "    print('\\n')\n",
    "    print('  epoch mean loss & accuracy: <{}, {}>'.format(np.mean(curr_loss_track),np.mean(curr_accuracy_track)))\n",
    "    print('\\n') \n",
    "    loss_track += curr_loss_track\n",
    "    accuracy_track += curr_accuracy_track\n",
    "except KeyboardInterrupt:\n",
    "    print('Stopped!')            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
