{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/work/04233/sw33286/AIDA-SCRIPTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell, MultiRNNCell, DropoutWrapper\n",
    "\n",
    "from helpers import Indexer, batch\n",
    "from itertools import chain, product\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100001, 300)\n"
     ]
    }
   ],
   "source": [
    "# Link to NYT data folder\n",
    "\n",
    "nyt_code_dir = \"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_event_sample_code/\"\n",
    "FILE_NAMES = os.listdir(nyt_code_dir)\n",
    "\n",
    "# Link to dictionary information\n",
    "\n",
    "info_path = \"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_info/indexer_word2emb_100k.p\"\n",
    "indexer100k, word2emb100k = dill.load(open(info_path, 'rb'))\n",
    "glove_embs = []\n",
    "for i in range(len(indexer100k)):\n",
    "    glove_embs.append(word2emb100k[indexer100k.get_object(i)])\n",
    "glove_embs = np.array(glove_embs)\n",
    "print(glove_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "NUM_EVENTS = 5\n",
    "NUM_WORDS = 5\n",
    "CTX_LEN = 20\n",
    "CTX_DUMMY = np.zeros([NUM_EVENTS, NUM_WORDS])\n",
    "\n",
    "def get_batch(file_idx):\n",
    "    filename = FILE_NAMES[file_idx]\n",
    "    edoc_a, edoc_b, batch_ctx = dill.load(open(nyt_code_dir+FILE_NAMES[file_idx],'rb'))\n",
    "    size_a, size_b, size_ctx = len(edoc_a), len(edoc_b), len(batch_ctx)\n",
    "    batch_x1, batch_x2, batch_y = [], [], []\n",
    "    ys = [1,0,0,1]\n",
    "    sizes = [[size_a,size_a],[size_a,size_b],[size_b,size_a],[size_b,size_b]]\n",
    "    for _ in range(BATCH_SIZE//4):\n",
    "        for i,(eda,edb) in enumerate(product([edoc_a,edoc_b],\n",
    "                                             [edoc_a,edoc_b])):\n",
    "            batch_x1.append(eda[np.random.randint(0,sizes[i][0])])\n",
    "            batch_x2.append(edb[np.random.randint(0,sizes[i][1])])\n",
    "            batch_y.append(ys[i])\n",
    "    batch_ctx = batch_ctx[:CTX_LEN] if size_ctx>=CTX_LEN else batch_ctx+[CTX_DUMMY]*(CTX_LEN-size_ctx)\n",
    "    return np.array(batch_x1), np.array(batch_x2), np.array(batch_ctx), np.array(batch_y) \n",
    "        # batch_x*: <bc,ne,nw>, batch_ctx batch_y: <bc,>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFNN-BiLSTM-bilinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "VOCAB_SIZE, EMB_SIZE = glove_embs.shape\n",
    "HID_SIZE = 100 # let event embs be of the same hid-size as role-factored arg vectors.\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "KEEP_PROB = 0.7\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "# hyperparams for cnn context reader.\n",
    "FILTER_SIZES = [3,4,5]\n",
    "NUM_FILTERS = 50\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "input_x1 = tf.placeholder(tf.int32, [BATCH_SIZE, NUM_EVENTS, NUM_WORDS], name='input_x1')\n",
    "input_x2 = tf.placeholder(tf.int32, [BATCH_SIZE, NUM_EVENTS, NUM_WORDS], name='input_x2')\n",
    "input_ctx = tf.placeholder(tf.int32, [CTX_LEN, NUM_EVENTS, NUM_WORDS])\n",
    "input_y = tf.placeholder(tf.int32, [BATCH_SIZE], name='input_y')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "with tf.variable_scope('Embedding'):\n",
    "    embeddings = tf.get_variable('embedding', [VOCAB_SIZE, EMB_SIZE],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "    glove_init = embeddings.assign(glove_embs)\n",
    "    input_x1_embedded = tf.reshape(tf.nn.embedding_lookup(embeddings, input_x1), [BATCH_SIZE, NUM_EVENTS, -1]) \n",
    "        # op1. embed words: <bc,ne,nw,emb>\n",
    "        # op2. concat words in event: <bc,ne,nw*emb>\n",
    "    input_x2_embedded = tf.reshape(tf.nn.embedding_lookup(embeddings, input_x2), [BATCH_SIZE, NUM_EVENTS, -1])\n",
    "    input_ctx_embedded = tf.reshape(tf.nn.embedding_lookup(embeddings, input_ctx), [CTX_LEN, NUM_EVENTS, -1])\n",
    "        # op1. embed words: <ctx,ne,nw,emb>\n",
    "        # op2. concat words in event: <ctx,ne,nw*emb>\n",
    "\n",
    "def run_ffnn(inputs): # inputs=<ne,nw*emb>\n",
    "    W_ffnn = tf.get_variable('W_ffnn', [NUM_WORDS*EMB_SIZE, EMB_SIZE], \n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "    return tf.matmul(inputs, W_ffnn) # <ne,emb>, event now has the same emb length as word.\n",
    "\n",
    "with tf.variable_scope('FFNN') as scope:\n",
    "    input_x1_ffnn = tf.transpose(tf.map_fn(run_ffnn, input_x1_embedded), [1,0,2]) \n",
    "        # op1. map_fn out: <bc,ne,emb>\n",
    "        # op2. for lstm input: <max-time=ne,bc,emb>\n",
    "    scope.reuse_variables()\n",
    "    input_x2_ffnn = tf.transpose(tf.map_fn(run_ffnn, input_x2_embedded), [1,0,2])\n",
    "    scope.reuse_variables()\n",
    "    input_ctx_ffnn = tf.expand_dims(tf.expand_dims(tf.reshape(tf.map_fn(run_ffnn, input_ctx_embedded),\n",
    "                                                              [CTX_LEN,-1]),0),-1) \n",
    "        # op1. lookup: <ctx,ne,emb>\n",
    "        # op2. concat events in context: <ctx,ne*emb>\n",
    "        # op2. expand dim for bc=1: <bc=1,height=ctx,width=ne*emb>\n",
    "        # op3. expand dim for chn=1: <bc=1,height=ctx,width=ne*emb,chn=1>\n",
    "    \n",
    "cell = MultiRNNCell([DropoutWrapper(LSTMCell(HID_SIZE),output_keep_prob=keep_prob)]*NUM_LAYERS)\n",
    "\n",
    "def run_lstm(inputs):\n",
    "    ((fw_outputs,bw_outputs), # <max-time=ne,bc,hid>, attention later if needed.\n",
    "     (fw_final_state,bw_final_state)) = ( # <bc,hid>\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw=cell,\n",
    "                                        cell_bw=cell,\n",
    "                                        inputs=inputs,\n",
    "                                        sequence_length=[NUM_EVENTS]*BATCH_SIZE,\n",
    "                                        dtype=tf.float32, time_major=True)\n",
    "    )    \n",
    "    return tf.concat([tf.concat([fw_state_tuple.h,bw_state_tuple.h], 1) # lstm-out size *= NUM_LAYERS by stacking.\n",
    "                      for fw_state_tuple,bw_state_tuple in zip(fw_final_state,bw_final_state)], 1), \\\n",
    "           tf.transpose(tf.concat([fw_outputs,bw_outputs], 2), [1,0,2])\n",
    "        # out1: <bc,hid*2*num-layers>\n",
    "        # out2: concat -> <ne,bc,hid*2> -> <bc,ne,hid*2>\n",
    "\n",
    "with tf.variable_scope('BiLSTM') as scope: \n",
    "    final_state_x1, outputs_x1 = run_lstm(input_x1_ffnn)\n",
    "        # fs_x1: <bc,hid*2*num-layers>\n",
    "        # out_x1: <bc,ne,hid*2>\n",
    "    scope.reuse_variables()\n",
    "    final_state_x2, outputs_x2 = run_lstm(input_x2_ffnn)\n",
    "    \n",
    "def run_attention(outputs, state):\n",
    "    W_d = tf.get_variable('W_d', [HID_SIZE*2, HID_SIZE*2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    W_s = tf.get_variable('W_s', [HID_SIZE*2*NUM_LAYERS, HID_SIZE*2], \n",
    "                          initializer=tf.contrib.layers.xavier_initializer())\n",
    "    d_W = tf.tensordot(outputs, W_d, axes=[[2],[0]])\n",
    "        # <bc,ne,hid*2> * <hid*2,hid*2> = <bc,ne,hid*2>\n",
    "    s_W = tf.expand_dims(tf.matmul(state, W_s), axis=1)\n",
    "        # op1. <bc,hid*2*num-layers> * <hid*2*num-layers,hid*2> -> <bc,hid*2>\n",
    "        # op2. <bc,hid*2> -> <bc,1,hid*2>\n",
    "    a_tsr = tf.nn.tanh(tf.add(d_W, s_W))\n",
    "        # op1. <bc,ne,hid*2> + <bc,1,hid*2> -> <bc,ne,hid*2>\n",
    "        # op2. elem-wise nonlinearity.\n",
    "    W_a = tf.get_variable('W_a', [HID_SIZE*2, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    a_W = tf.nn.softmax(tf.tensordot(a_tsr, W_a, axes=[[2],[0]]), dim=1)\n",
    "        # op1. <bc,ne,hid*2> * <hid*2,1> -> <bc,ne,1>\n",
    "        # op2. softmax over max-time=ne.\n",
    "    d_a = tf.reduce_sum(tf.multiply(outputs, a_W), axis=1)\n",
    "        # op1. <bc,ne,hid*2> elem* <bc,ne,1> -> <bc,ne,hid*2>\n",
    "        # op2. sum over max-time=ne (weighted sum) -> <bc,hid*2>\n",
    "    return d_a  \n",
    "\n",
    "with tf.variable_scope('Mutual-Attention') as scope:\n",
    "    x1_to_x2_att = run_attention(outputs_x2, final_state_x1) # x1 attending to x2, <bc,hid*2>\n",
    "    scope.reuse_variables()\n",
    "    x2_to_x1_att = run_attention(outputs_x1, final_state_x2) # x2 attending to x1\n",
    "    \n",
    "def run_cnn(inputs): # in: <bc=1,height=ctx,width=ne*emb,chn=1>\n",
    "    pool_outputs = []\n",
    "    for i,filter_size in enumerate(FILTER_SIZES):\n",
    "        with tf.variable_scope('CNN-ctx-%s' % filter_size):\n",
    "            filter_shape = [filter_size, NUM_EVENTS*EMB_SIZE, NUM_CHANNELS, NUM_FILTERS]\n",
    "            W = tf.get_variable('W', filter_shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.get_variable('b', [NUM_FILTERS], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            conv = tf.nn.conv2d(inputs, W, strides=[1,1,1,1], padding='VALID', name='conv')\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
    "            pool = tf.nn.max_pool(h, ksize=[1,CTX_LEN-filter_size+1,1,1], strides=[1,1,1,1], padding='VALID', name='pool')\n",
    "            pool_outputs.append(pool)\n",
    "    num_filters_total = NUM_FILTERS * len(FILTER_SIZES)\n",
    "    h_pool_flat = tf.nn.dropout(tf.reshape(tf.concat(pool_outputs, 3), [-1, num_filters_total]), keep_prob)\n",
    "    return h_pool_flat # <bc=1,num-filters*len(filter-sizes)>\n",
    "\n",
    "with tf.variable_scope('Context-reader') as scope:\n",
    "    ctx = tf.tile(run_cnn(input_ctx_ffnn), [BATCH_SIZE,1])\n",
    "        # op1. run-cnn out: <bc=1,num-filters*len(filter-sizes)>\n",
    "        # op2. create bc copies of it: <bc,num-filters*len(filter-sizes)>\n",
    "        \n",
    "def run_scores(fs_x1, fs_x2, att_12, att_21, c):\n",
    "    fv_size = HID_SIZE*2*NUM_LAYERS+HID_SIZE*2+NUM_FILTERS*len(FILTER_SIZES) \n",
    "        # sent encoding size + mutual attention size + context size.\n",
    "        # e.g. hid-size=100, num-filters=50\n",
    "        #      400 + 200 + 150 = 750\n",
    "    W_bi = tf.get_variable('W_bi', [fv_size, fv_size], \n",
    "                           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    fv_x1 = tf.concat([fs_x1,att_12,c],axis=1) \n",
    "        # concat: [<bc,hid*2*num-layers>, <bc,hid*2>, <bc,num-filters*len(filter-sizes)>]\n",
    "        #   -> <bc,fv = hid*2*num-layers + hid*2 + num-filters*len(filter-sizes)>\n",
    "    fv_x2 = tf.concat([fs_x2,att_21,c],axis=1)\n",
    "    return tf.nn.sigmoid(tf.diag_part(tf.matmul(tf.matmul(fv_x1,W_bi),tf.transpose(fv_x2))),name='scores')\n",
    "        # op1. bilinear mult: <bc,fv> * <fv,fv> * <fv,bc> -> <bc,bc>\n",
    "        # op2: match bc: <bc,>\n",
    "        # op3: sigmoid to compute scores: <bc,>\n",
    "        \n",
    "scores = run_scores(final_state_x1, final_state_x2, x1_to_x2_att, x2_to_x1_att, ctx)\n",
    "predictions = tf.cast(tf.round(scores), tf.int32, name='predictions')     \n",
    "    \n",
    "with tf.name_scope('Loss'):\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(input_y, tf.float32), logits=scores)\n",
    "    loss = tf.reduce_mean(losses, name='loss')\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct_predictions = tf.equal(predictions, input_y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "    \n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step, name='train_op')\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "TRAIN_SIZE = 10\n",
    "VERBOSE = 1\n",
    "# TRAIN_SIZE = len(FILE_NAMES)\n",
    "# VERBOSE = 1000\n",
    "\n",
    "loss_track, accuracy_track = [], []\n",
    "try:\n",
    "    for e in range(NUM_EPOCHS):\n",
    "        print('Epoch ', e+1)\n",
    "        print('\\n')\n",
    "        file_indices = np.random.choice(list(range(len(FILE_NAMES))), size=TRAIN_SIZE, replace=False)\n",
    "        random.shuffle(file_indices)\n",
    "        curr_loss_track, curr_accuracy_track = [], []\n",
    "        for file_idx in file_indices:\n",
    "            batch_x1, batch_x2, batch_ctx, batch_y = get_batch(file_idx)\n",
    "            fd = {input_x1:batch_x1, input_x2:batch_x2,\n",
    "                  input_ctx:batch_ctx,\n",
    "                  input_y:batch_y,\n",
    "                  keep_prob:KEEP_PROB}\n",
    "            _, step, loss_, accuracy_ = sess.run([train_op, global_step, loss, accuracy], feed_dict=fd)\n",
    "            curr_loss_track.append(loss_)\n",
    "            curr_accuracy_track.append(accuracy_)\n",
    "            if step%VERBOSE==0:\n",
    "                print(' average batch loss & accuracy at step {}: <{}, {}>'.format(step,\n",
    "                                                                                   np.mean(curr_loss_track), \n",
    "                                                                                   np.mean(curr_accuracy_track)))\n",
    "    print('\\n')\n",
    "    print('  epoch mean loss & accuracy: <{}, {}>'.format(np.mean(curr_loss_track),np.mean(curr_accuracy_track)))\n",
    "    print('\\n') \n",
    "    loss_track += curr_loss_track\n",
    "    accuracy_track += curr_accuracy_track\n",
    "except KeyboardInterrupt:\n",
    "    print('Stopped!')            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
