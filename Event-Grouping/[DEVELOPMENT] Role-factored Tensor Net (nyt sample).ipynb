{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role-factored Tensor Net\n",
    "\n",
    "* Adapted from Weber et al. (2018) Event Representations with Tensor-based Compositions. AAAI18\n",
    "\n",
    "* INPUT: a batch of events (5-tuples <v,s,o,p,po>), corresponding positive and negative instances (word2vec style training).\n",
    "* OUTPUT: encoded event representation (as vectors).\n",
    "* PROC:\n",
    "    * Modeling the interaction between the predicate v with each of the arguments (equation: same for other args)\n",
    "    $$v_s = v\\cdot T\\cdot s^{T}$$\n",
    "    where $v\\in R^{b\\times d_e}$, $T\\in R^{d_e,d_e,h}$, and $s\\in R^{b\\times d_e}$. This results in a batch of event vectors $\\in R^{b,h}$ after batch matching and transposing.\n",
    "    * Merging factor interactions additively\n",
    "    $$e = v_s\\cdot W_s + \\dots + v_{po}\\cdot W_{po}$$\n",
    "    * Maximizing the distance between the input event and its positive examples while minimizing the distance between it and its negative examples through a max-margin loss\n",
    "    $$\\ell = \\frac{1}{N}\\sum_{i=1}^N \\texttt{max}(0, m + \\texttt{sim}(e, e_{neg}) - \\texttt{sim}(e, e_{pos}))$$\n",
    "* COMMENTS\n",
    "    * For the simple demo I use dot product for distance metric rather than cosine as in the paper.\n",
    "    * The network is not L2-regularized as in the paper, but the loss term can be easily added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/work/04233/sw33286/AIDA-SCRIPTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from helpers import Indexer\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100001, 300)\n"
     ]
    }
   ],
   "source": [
    "# Link to NYT data folder\n",
    "\n",
    "nyt_code_dir = \"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_event_sample_code/\"\n",
    "FILE_NAMES = os.listdir(nyt_code_dir)\n",
    "\n",
    "# Link to dictionary information\n",
    "\n",
    "info_path = \"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_info/indexer_word2emb_100k.p\"\n",
    "indexer100k, word2emb100k = dill.load(open(info_path, 'rb'))\n",
    "glove_embs = []\n",
    "for i in range(len(indexer100k)):\n",
    "    glove_embs.append(word2emb100k[indexer100k.get_object(i)])\n",
    "glove_embs = np.array(glove_embs)\n",
    "print(glove_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "CONTRA_BC = 10\n",
    "\n",
    "def get_batch(edoc_a, edoc_b):\n",
    "    edoc_a = list(chain.from_iterable(edoc_a)) # to a list of events\n",
    "    edoc_b = list(chain.from_iterable(edoc_b))\n",
    "    size_a, size_b = len(edoc_a), len(edoc_b)\n",
    "    batch_x, batch_pos, batch_neg = [], [], []\n",
    "    for _ in range(BATCH_SIZE//2):\n",
    "        x_a = edoc_a[np.random.randint(0, size_a)]\n",
    "        x_b = edoc_b[np.random.randint(0, size_b)]\n",
    "        pos_a = [edoc_a[np.random.randint(0, size_a)] for _ in range(CONTRA_BC)]\n",
    "        neg_a = [edoc_b[np.random.randint(0, size_b)] for _ in range(CONTRA_BC)]\n",
    "        pos_b = [edoc_b[np.random.randint(0, size_b)] for _ in range(CONTRA_BC)]\n",
    "        neg_b = [edoc_a[np.random.randint(0, size_a)] for _ in range(CONTRA_BC)]        \n",
    "        batch_x += [x_a, x_b]\n",
    "        batch_pos += [pos_a, pos_b]\n",
    "        batch_neg += [neg_a, neg_b]\n",
    "    return np.array(batch_x), np.array(batch_pos), np.array(batch_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 5), (32, 10, 5), (32, 10, 5))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: batch shapes\n",
    "\n",
    "edoc_a, edoc_b, _ = dill.load(open(nyt_code_dir+FILE_NAMES[0],'rb'))\n",
    "a,b1,b2 = get_batch(edoc_a, edoc_b)\n",
    "a.shape, b1.shape, b2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role-factored Tensor Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "VOCAB_SIZE, EMB_SIZE = glove_embs.shape\n",
    "HID_SIZE = 100 # let event embs be of the same hid-size as role-factored arg vectors.\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "inputs = tf.placeholder(tf.int32, [BATCH_SIZE, 5], name='inputs') # <bc,nw-in-event=5>\n",
    "inputs_pos = tf.placeholder(tf.int32, [BATCH_SIZE, CONTRA_BC, 5], name='inputs_pos') # <bc,ctr-bc,nw-in-event=5>\n",
    "inputs_neg = tf.placeholder(tf.int32, [BATCH_SIZE, CONTRA_BC, 5], name='inputs_neg')\n",
    " \n",
    "with tf.variable_scope('Embedding'):\n",
    "    embeddings = tf.get_variable('embedding', [VOCAB_SIZE, EMB_SIZE],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "    glove_init = embeddings.assign(glove_embs)\n",
    "\n",
    "with tf.variable_scope('Role-factor'):\n",
    "    T = tf.get_variable('T', [EMB_SIZE, EMB_SIZE, HID_SIZE], \n",
    "                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "    W_s = tf.get_variable('W_s', [HID_SIZE, HID_SIZE], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    W_o = tf.get_variable('W_o', [HID_SIZE, HID_SIZE], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    W_p = tf.get_variable('W_p', [HID_SIZE, HID_SIZE], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    W_po = tf.get_variable('W_po', [HID_SIZE, HID_SIZE], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "def encode_events(inputs_):\n",
    "    bc,_ = tf.unstack(tf.shape(inputs_))\n",
    "    # Slicing inputs\n",
    "    input_v = tf.squeeze(tf.slice(inputs_, [0,0],[bc,1]), -1)\n",
    "        # op1. looking up the vector corresponds to the predicate: <bc,1>\n",
    "        # op2. get rid of the vacuous dimension: <bc,>\n",
    "    input_s = tf.squeeze(tf.slice(inputs_, [0,1],[bc,1]))\n",
    "    input_o = tf.squeeze(tf.slice(inputs_, [0,2],[bc,1]))\n",
    "    input_p = tf.squeeze(tf.slice(inputs_, [0,3],[bc,1]))\n",
    "    input_po = tf.squeeze(tf.slice(inputs_, [0,4],[bc,1]))\n",
    "    # Looking up\n",
    "    input_v_embedded = tf.nn.embedding_lookup(embeddings, input_v) # <bc,emb>\n",
    "    input_s_embedded = tf.transpose(tf.nn.embedding_lookup(embeddings, input_s),[1,0]) # <emb,bc>\n",
    "    input_o_embedded = tf.transpose(tf.nn.embedding_lookup(embeddings, input_o),[1,0])\n",
    "    input_p_embedded = tf.transpose(tf.nn.embedding_lookup(embeddings, input_p),[1,0])\n",
    "    input_po_embedded = tf.transpose(tf.nn.embedding_lookup(embeddings, input_po),[1,0])\n",
    "    # Role factoring\n",
    "    vT = tf.transpose(tf.tensordot(input_v_embedded, T, axes=[[1],[0]]), [0,2,1])\n",
    "        # op1. <bc,emb> * <emb,emb,hid> -> <bc,emb,hid>\n",
    "        # op2. <bc,emb,hid> -> <bc,hid,emb>\n",
    "    vTs = tf.matrix_diag_part(tf.transpose(tf.tensordot(vT, input_s_embedded, axes=[[2],[0]]), [1,0,2]))\n",
    "        # op1. <bc,hid,emb> * <emb,bc> -> <bc,hid,bc>\n",
    "        # op2. <bc,hid,bc> -> <hid,bc,bc>\n",
    "        # op3. <hid,bc>\n",
    "    vTo = tf.matrix_diag_part(tf.transpose(tf.tensordot(vT, input_o_embedded, axes=[[2],[0]]), [1,0,2]))\n",
    "    vTp = tf.matrix_diag_part(tf.transpose(tf.tensordot(vT, input_p_embedded, axes=[[2],[0]]), [1,0,2]))\n",
    "    vTpo = tf.matrix_diag_part(tf.transpose(tf.tensordot(vT, input_po_embedded, axes=[[2],[0]]), [1,0,2]))\n",
    "    # Factor merging\n",
    "    v_s = tf.matmul(W_s, vTs) # <hid,hid> * <hid,bc> -> <hid,bc>\n",
    "    v_o = tf.matmul(W_p, vTo)\n",
    "    v_p = tf.matmul(W_o, vTp)\n",
    "    v_po = tf.matmul(W_po, vTpo)\n",
    "    v_event = v_s + v_o + v_p + v_po # <hid,bc>\n",
    "    return v_event\n",
    "\n",
    "inputs_encoded = encode_events(inputs) # <hid,bc>\n",
    "inputs_pos_encoded = tf.transpose(tf.map_fn(encode_events, inputs_pos, dtype=tf.float32), [0,2,1]) # <bc,hid,ctr-bc>\n",
    "    # op1. event-encoder output: <bc,hid,ctr-bc>\n",
    "    # op2. transpose: <bc,ctr-bc,hid>\n",
    "inputs_neg_encoded = tf.transpose(tf.map_fn(encode_events, inputs_neg, dtype=tf.float32), [0,2,1])\n",
    "\n",
    "with tf.variable_scope('Encode'):\n",
    "    predictions = tf.identity(inputs_encoded, name='predictions')\n",
    "\n",
    "with tf.variable_scope('Loss'):\n",
    "    sim_pos = tf.matrix_diag_part(tf.transpose(tf.tensordot(inputs_pos_encoded, inputs_encoded, axes=[[2],[0]]), \n",
    "                                               [1,0,2]))\n",
    "        # op1. tensordot: <bc,ctr-bc,hid> * <hid,bc> -> <bc,ctr-bc,bc>\n",
    "        # op2. transpose: <ctr-bc,bc,bc>\n",
    "        # op3. match bc: <ctr-bc,bc>\n",
    "    sim_neg = tf.matrix_diag_part(tf.transpose(tf.tensordot(inputs_neg_encoded, inputs_encoded, axes=[[2],[0]]), \n",
    "                                               [1,0,2]))    \n",
    "    loss = tf.reduce_mean(tf.reduce_mean(tf.maximum(0., 1. + sim_neg - sim_pos), axis=0))\n",
    "        # op1. max(0, m + sim_neg - sim_pos), <ctr-bc,bc>\n",
    "        # op2. average loss over contra instances: <bc,>\n",
    "        # op3. average loss over batch\n",
    "        \n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step, name='train_op')\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "\n",
      "\n",
      " average batch loss at step 10: <1.0>\n",
      " average batch loss at step 20: <1.0>\n",
      " average batch loss at step 30: <1.0>\n",
      " average batch loss at step 40: <1.0>\n",
      " average batch loss at step 50: <1.0>\n",
      " average batch loss at step 60: <1.0>\n",
      " average batch loss at step 70: <1.0>\n",
      " average batch loss at step 80: <1.0>\n",
      " average batch loss at step 90: <1.0>\n",
      " average batch loss at step 100: <0.9999977350234985>\n",
      "\n",
      "\n",
      "  epoch mean loss: <0.9999977350234985>\n",
      "\n",
      "\n",
      "Epoch  2\n",
      "\n",
      "\n",
      " average batch loss at step 110: <0.9999059438705444>\n",
      " average batch loss at step 120: <0.9979122877120972>\n",
      " average batch loss at step 130: <0.9878673553466797>\n",
      " average batch loss at step 140: <0.9838888049125671>\n",
      " average batch loss at step 150: <0.982790470123291>\n",
      " average batch loss at step 160: <0.9817882776260376>\n",
      " average batch loss at step 170: <0.9811098575592041>\n",
      " average batch loss at step 180: <0.9811125993728638>\n",
      " average batch loss at step 190: <0.9831833839416504>\n",
      " average batch loss at step 200: <0.9843214154243469>\n",
      "\n",
      "\n",
      "  epoch mean loss: <0.9843214154243469>\n",
      "\n",
      "\n",
      "Epoch  3\n",
      "\n",
      "\n",
      " average batch loss at step 210: <0.9834636449813843>\n",
      " average batch loss at step 220: <0.9825418591499329>\n",
      " average batch loss at step 230: <0.9855453372001648>\n",
      " average batch loss at step 240: <0.9842951893806458>\n",
      " average batch loss at step 250: <0.9831700325012207>\n",
      " average batch loss at step 260: <0.9830158948898315>\n",
      " average batch loss at step 270: <0.980758011341095>\n",
      " average batch loss at step 280: <0.9789773225784302>\n",
      " average batch loss at step 290: <0.9792165756225586>\n",
      " average batch loss at step 300: <0.9797807931900024>\n",
      "\n",
      "\n",
      "  epoch mean loss: <0.9797807931900024>\n",
      "\n",
      "\n",
      "Epoch  4\n",
      "\n",
      "\n",
      " average batch loss at step 310: <0.9904079437255859>\n",
      " average batch loss at step 320: <0.9833084940910339>\n",
      " average batch loss at step 330: <0.9740879535675049>\n",
      " average batch loss at step 340: <0.971635639667511>\n",
      " average batch loss at step 350: <0.967495322227478>\n",
      " average batch loss at step 360: <0.9676629304885864>\n",
      " average batch loss at step 370: <0.9718629121780396>\n",
      " average batch loss at step 380: <0.9720926284790039>\n",
      " average batch loss at step 390: <0.9738986492156982>\n",
      " average batch loss at step 400: <0.9749659895896912>\n",
      "\n",
      "\n",
      "  epoch mean loss: <0.9749659895896912>\n",
      "\n",
      "\n",
      "Epoch  5\n",
      "\n",
      "\n",
      " average batch loss at step 410: <0.976570725440979>\n",
      " average batch loss at step 420: <0.9809082746505737>\n",
      " average batch loss at step 430: <0.98072350025177>\n",
      " average batch loss at step 440: <0.9813246726989746>\n",
      " average batch loss at step 450: <0.9791014790534973>\n",
      " average batch loss at step 460: <0.9787449240684509>\n",
      " average batch loss at step 470: <0.9778417348861694>\n",
      " average batch loss at step 480: <0.9753090739250183>\n",
      " average batch loss at step 490: <0.9740393757820129>\n",
      " average batch loss at step 500: <0.9743511080741882>\n",
      "\n",
      "\n",
      "  epoch mean loss: <0.9743511080741882>\n",
      "\n",
      "\n",
      "Epoch  6\n",
      "\n",
      "\n",
      " average batch loss at step 510: <0.9759342074394226>\n",
      " average batch loss at step 520: <0.9679280519485474>\n",
      " average batch loss at step 530: <0.9714466333389282>\n",
      " average batch loss at step 540: <0.9680584669113159>\n",
      " average batch loss at step 550: <0.965518057346344>\n",
      " average batch loss at step 560: <0.9663498401641846>\n",
      " average batch loss at step 570: <0.966137170791626>\n",
      " average batch loss at step 580: <0.9646528959274292>\n",
      " average batch loss at step 590: <0.9686959385871887>\n",
      " average batch loss at step 600: <0.9696147441864014>\n",
      "\n",
      "\n",
      "  epoch mean loss: <0.9696147441864014>\n",
      "\n",
      "\n",
      "Epoch  7\n",
      "\n",
      "\n",
      " average batch loss at step 610: <0.9798272252082825>\n",
      " average batch loss at step 620: <0.9716240167617798>\n",
      " average batch loss at step 630: <0.9709550142288208>\n",
      " average batch loss at step 640: <0.9769694209098816>\n",
      " average batch loss at step 650: <0.9763314723968506>\n",
      " average batch loss at step 660: <0.9688039422035217>\n",
      " average batch loss at step 670: <0.9676681756973267>\n",
      " average batch loss at step 680: <0.9652387499809265>\n",
      " average batch loss at step 690: <0.9668061137199402>\n",
      " average batch loss at step 700: <0.9655198454856873>\n",
      "\n",
      "\n",
      "  epoch mean loss: <0.9655198454856873>\n",
      "\n",
      "\n",
      "Epoch  8\n",
      "\n",
      "\n",
      " average batch loss at step 710: <0.9785410165786743>\n",
      " average batch loss at step 720: <0.9784819483757019>\n",
      " average batch loss at step 730: <0.9593242406845093>\n",
      " average batch loss at step 740: <0.9612864255905151>\n",
      " average batch loss at step 750: <0.9548342227935791>\n",
      " average batch loss at step 760: <0.9543012380599976>\n",
      " average batch loss at step 770: <0.9523269534111023>\n",
      " average batch loss at step 780: <0.9540723562240601>\n",
      " average batch loss at step 790: <0.9498797655105591>\n",
      " average batch loss at step 800: <0.9511293768882751>\n",
      "\n",
      "\n",
      "  epoch mean loss: <0.9511293768882751>\n",
      "\n",
      "\n",
      "Epoch  9\n",
      "\n",
      "\n",
      " average batch loss at step 810: <0.9820019602775574>\n",
      " average batch loss at step 820: <0.9706343412399292>\n",
      " average batch loss at step 830: <0.962350070476532>\n",
      " average batch loss at step 840: <0.9694766998291016>\n",
      " average batch loss at step 850: <0.958946704864502>\n",
      " average batch loss at step 860: <0.953153669834137>\n",
      " average batch loss at step 870: <0.9485086798667908>\n",
      " average batch loss at step 880: <0.9494230151176453>\n",
      " average batch loss at step 890: <0.9466189742088318>\n",
      " average batch loss at step 900: <0.9425225853919983>\n",
      "\n",
      "\n",
      "  epoch mean loss: <0.9425225853919983>\n",
      "\n",
      "\n",
      "Epoch  10\n",
      "\n",
      "\n",
      " average batch loss at step 910: <0.9525847434997559>\n",
      " average batch loss at step 920: <0.9711112976074219>\n",
      " average batch loss at step 930: <0.9604739546775818>\n",
      " average batch loss at step 940: <0.9518106579780579>\n",
      " average batch loss at step 950: <0.954362154006958>\n",
      " average batch loss at step 960: <0.94101482629776>\n",
      " average batch loss at step 970: <0.9416297078132629>\n",
      " average batch loss at step 980: <0.9391149282455444>\n",
      " average batch loss at step 990: <0.9338165521621704>\n",
      " average batch loss at step 1000: <0.9324533343315125>\n",
      "\n",
      "\n",
      "  epoch mean loss: <0.9324533343315125>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "# TRAIN_SIZE = 10\n",
    "# VERBOSE = 1\n",
    "TRAIN_SIZE = len(FILE_NAMES)\n",
    "VERBOSE = 10\n",
    "\n",
    "try:\n",
    "    loss_track = []\n",
    "    for e in range(NUM_EPOCHS):\n",
    "        print('Epoch ', e+1)\n",
    "        print('\\n')\n",
    "        curr_loss_track = []\n",
    "        file_indices = np.random.choice(list(range(len(FILE_NAMES))), size=TRAIN_SIZE, replace=False)\n",
    "        random.shuffle(file_indices)\n",
    "        curr_loss_track, curr_accuracy_track = [], []\n",
    "        for file_idx in file_indices:\n",
    "            edoc_a, edoc_b, _ = dill.load(open(nyt_code_dir+FILE_NAMES[file_idx],'rb')) # context not added\n",
    "            batch_x, batch_pos, batch_neg = get_batch(edoc_a, edoc_b)\n",
    "            fd = {inputs:batch_x, inputs_pos:batch_pos, inputs_neg:batch_neg}\n",
    "            _, step, loss_ = sess.run([train_op, global_step, loss], feed_dict=fd)\n",
    "            curr_loss_track.append(loss_)\n",
    "            if step%VERBOSE==0:\n",
    "                print(' average batch loss at step {}: <{}>'.format(step, np.mean(curr_loss_track)))\n",
    "        print('\\n')\n",
    "        print('  epoch mean loss: <{}>'.format(np.mean(curr_loss_track)))\n",
    "        print('\\n') \n",
    "        loss_track += curr_loss_track  \n",
    "except KeyboardInterrupt:\n",
    "    print('Stopped!')                      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
