{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise Sentence Classifier\n",
    "\n",
    "* Original work: Disentangling Story Salads (Wang, Holgate, Durrett and Erk, 2018)\n",
    "\n",
    "![](FIGS/pairwise-sentence-classifier.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 @Jacob Su Wang. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/work/04233/sw33286/AIDA-SCRIPTS\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell, MultiRNNCell, DropoutWrapper\n",
    "\n",
    "from helpers import Indexer, batch, checkpoint_model\n",
    "from itertools import chain, product\n",
    "from collections import defaultdict\n",
    "\n",
    "    \n",
    "class PairwiseSentenceClassifier:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        self.FILENAMES = os.listdir(self.config['data_dir'])\n",
    "            # each file is a '.p' filename.\n",
    "        self.indexer, self.word2emb = dill.load(open(self.config['info_path'], 'rb'))\n",
    "            # indexer: Indexer object, word<->index mapping.\n",
    "            # word2emb: word->glove-embedding mapping (300D).\n",
    "        \n",
    "        if self.config['init_with_glove']:\n",
    "            glove_embs = []\n",
    "            for i in range(len(self.indexer)):\n",
    "                glove_embs.append(self.word2emb[self.indexer.get_object(i)])\n",
    "            self.glove_embs = np.array(glove_embs)\n",
    "        else:\n",
    "            del self.word2emb\n",
    "        \n",
    "        if self.config['load_from_saved']:\n",
    "            self.__load_saved_graph()\n",
    "            print('Model loaded for continued training!')\n",
    "        else:\n",
    "            self.__build_new_graph()  \n",
    "            print('New model built for training!')\n",
    "            \n",
    "    # Build & train new graph\n",
    "        \n",
    "    def __build_new_graph(self):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.__init_placeholders()\n",
    "        self.__init_embeddings()\n",
    "        \n",
    "        self.cell = MultiRNNCell([DropoutWrapper(LSTMCell(self.config['hid_size']),\n",
    "                                                 output_keep_prob=self.keep_prob)]*self.config['n_layer'])\n",
    "        \n",
    "        self.__run_nets(add_mutual_attention=self.config['mutual_attention'],\n",
    "                        add_context_reader=self.config['context'])\n",
    "        self.__run_score_and_predictions()\n",
    "        self.__run_accuracy()\n",
    "        self.__run_optimization()\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    \n",
    "    def __init_placeholders(self):\n",
    "        \n",
    "        self.input_x1 = tf.placeholder(tf.int32, [None, None], name='input_x1') \n",
    "            # <max-time, batch-size>\n",
    "        self.input_x2 = tf.placeholder(tf.int32, [None, None], name='input_x2')\n",
    "        self.input_x1_length = tf.placeholder(tf.int32, [None], name='input_x1_length')\n",
    "        self.input_x2_length = tf.placeholder(tf.int32, [None], name='input_x2_length')\n",
    "        self.input_y  = tf.placeholder(tf.int32, [None], name='input_y')\n",
    "        \n",
    "        if self.config['context']:\n",
    "            self.input_ctx = tf.placeholder(tf.int32, [1, self.config['context_length']], name='input_ctx')\n",
    "                # <batch-size, height=max-time>\n",
    "            \n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")     \n",
    "            \n",
    "    def __init_embeddings(self):\n",
    "        \n",
    "        with tf.variable_scope('Emebeddings'):\n",
    "            self.embeddings = tf.get_variable('embeddings', [self.config['vocab_size'], self.config['emb_size']], \n",
    "                                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "            if self.config['init_with_glove']:\n",
    "                glove_init = self.embeddings.assign(self.glove_embs)\n",
    "            self.input_x1_embedded = tf.nn.embedding_lookup(self.embeddings, self.input_x1) \n",
    "                # <max-time, batch-size, emb-size>\n",
    "            self.input_x2_embedded = tf.nn.embedding_lookup(self.embeddings, self.input_x2)\n",
    "            if self.config['context']:\n",
    "                self.input_ctx_embedded = tf.expand_dims(tf.nn.embedding_lookup(self.embeddings, self.input_ctx), -1)\n",
    "                    # <batch-size, height=max-time, width=EMB_SIZE, num_channels=1>\n",
    "    \n",
    "    def __run_lstm(self, inputs, inputs_length): # lstm-out size *= 2 by bidirectionality.\n",
    "        \n",
    "        ((fw_outputs,bw_outputs), # <max-time, batch-size, hid-size>, attention later if needed.\n",
    "         (fw_final_state,bw_final_state)) = ( # <batch-size, hid-size>\n",
    "            tf.nn.bidirectional_dynamic_rnn(cell_fw=self.cell,\n",
    "                                            cell_bw=self.cell,\n",
    "                                            inputs=inputs,\n",
    "                                            sequence_length=inputs_length,\n",
    "                                            dtype=tf.float32, time_major=True) )\n",
    "        if self.config['context']:\n",
    "            return tf.concat([tf.concat([fw_state_tuple.h,bw_state_tuple.h], 1) \n",
    "                      for fw_state_tuple,bw_state_tuple in zip(fw_final_state,bw_final_state)], 1), \\\n",
    "                   tf.transpose(tf.concat([fw_outputs,bw_outputs], 2), [1,0,2])\n",
    "                # op1: <batch-size, hid-size*2*num-layers>\n",
    "                # op2: <max-time, batch-size, hid-size*2> -> <batch-size, max-time, hid-size*2>\n",
    "                # NB: batch-size = 1 here.\n",
    "        else:\n",
    "            return tf.concat([tf.concat([fw_state_tuple.h,bw_state_tuple.h], 1) \n",
    "                      for fw_state_tuple,bw_state_tuple in zip(fw_final_state,bw_final_state)], 1)\n",
    "        \n",
    "    def __run_attention(self, outputs, state):\n",
    "        \n",
    "        W_d = tf.get_variable('W_d', [self.config['hid_size']*2, self.config['hid_size']*2], \n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "        W_s = tf.get_variable('W_s', [self.config['hid_size']*2*self.config['n_layer'], self.config['hid_size']*2], \n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "        d_W = tf.tensordot(outputs, W_d, axes=[[2],[0]])\n",
    "            # <batch-size, max-time, hid-size*2> * <hid-size*2, hid-size*2> = <batch-size, max-time, hid-size*2>\n",
    "        s_W = tf.expand_dims(tf.matmul(state, W_s), axis=1)\n",
    "            # op1. <batch-size, hid-size*2*num-layers> * <hid-size*2*num-layers, hid-size*2> -> <batch-size, hid-size*2>\n",
    "            # op2. <batch-size, hid-size*2> -> <batch-size, 1, hid-size*2>\n",
    "        a_tsr = tf.nn.tanh(tf.add(d_W, s_W))\n",
    "            # op1. <batch-size, max-time, hid-size*2> + <batch-size, 1, hid-size*2> -> <batch-size, max-time, hid-size*2>\n",
    "            # op2. elem-wise nonlinearity.\n",
    "        W_a = tf.get_variable('W_a', [self.config['hid_size']*2, 1], \n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "        a_W = tf.nn.softmax(tf.tensordot(a_tsr, W_a, axes=[[2],[0]]), dim=1)\n",
    "            # op1. <batch-size, max-time, hid-size*2> * <hid-size*2, 1> -> <batch-size, max-time, 1>\n",
    "            # op2. softmax over max-time.\n",
    "        d_a = tf.reduce_sum(tf.multiply(outputs, a_W), axis=1)\n",
    "            # op1. <batch-size, max-time, hid-size*2> elem* <batch-size, max-time, 1> -> <batch-size, max-time, hid-size*2>\n",
    "            # op2. sum over max-time (weighted sum) -> <batch-size, hid-size*2>\n",
    "        return d_a  \n",
    "    \n",
    "    def __run_cnn(self, inputs):\n",
    "        \n",
    "        FILTER_SIZES = [3,4,5]\n",
    "        NUM_FILTERS = 50\n",
    "        NUM_CHANNELS = 1\n",
    "        CTX_LEN = self.config['context_length']\n",
    "        \n",
    "        pool_outputs = []\n",
    "        for i,filter_size in enumerate(FILTER_SIZES):\n",
    "            with tf.variable_scope('CNN-ctx-%s' % filter_size):\n",
    "                filter_shape = [filter_size, EMB_SIZE, NUM_CHANNELS, NUM_FILTERS]\n",
    "                W = tf.get_variable('W', filter_shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "                b = tf.get_variable('b', [NUM_FILTERS], initializer=tf.contrib.layers.xavier_initializer())\n",
    "                conv = tf.nn.conv2d(inputs, W, strides=[1,1,1,1], padding='VALID', name='conv')\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
    "                pool = tf.nn.max_pool(h, ksize=[1,CTX_LEN-filter_size+1,1,1], strides=[1,1,1,1], \n",
    "                                      padding='VALID', name='pool')\n",
    "                pool_outputs.append(pool)\n",
    "        num_filters_total = NUM_FILTERS * len(FILTER_SIZES)\n",
    "        h_pool_flat = tf.nn.dropout(tf.reshape(tf.concat(pool_outputs, 3), [-1, num_filters_total]), keep_prob)\n",
    "        return h_pool_flat     \n",
    "    \n",
    "    def __run_nets(self, add_mutual_attention=False, add_context_reader=False):\n",
    "        \n",
    "        if add_mutual_attention and add_context_reader:\n",
    "            with tf.variable_scope('Bi-LSTM') as scope:\n",
    "                final_state_x1, outputs_x1 = self.__run_lstm(self.input_x1_embedded, self.input_x1_length)\n",
    "                scope.reuse_variables() \n",
    "                final_state_x2, outputs_x2 = self.__run_lstm(self.input_x2_embedded, self.input_x2_length)\n",
    "            with tf.variable_scope('Mutual-Attention') as scope:\n",
    "                x1_to_x2_att = self.__run_attention(outputs_x2, final_state_x1)\n",
    "                scope.reuse_variables()\n",
    "                x2_to_x1_att = self.__run_attention(outputs_x1, final_state_x2)\n",
    "            with tf.variable_scope('Context-reader'):\n",
    "                bc, _ = tf.unstack(tf.shape(final_state_x1))\n",
    "                ctx = tf.tile(self.__run_cnn(self.input_ctx_embedded), [bc, 1])\n",
    "            self.final_vec_x1 = tf.concat([final_state_x1, x1_to_x2_att, ctx],axis=1)\n",
    "            self.final_vec_x2 = tf.concat([final_state_x2, x2_to_x1_att, ctx],axis=1)\n",
    "            self.final_vec_size = self.config['hid_size']*2*self.config['n_layer'] + \\\n",
    "                                  self.config['hid_size']*2 + \\\n",
    "                                  3*50 # n_filter * len(filter-sizes)\n",
    "        \n",
    "        elif add_mutual_attention:\n",
    "            with tf.variable_scope('Bi-LSTM') as scope:\n",
    "                final_state_x1, outputs_x1 = self.__run_lstm(self.input_x1_embedded, self.input_x1_length)\n",
    "                scope.reuse_variables() # both sentence inputs share the same weights.\n",
    "                final_state_x2, outputs_x2 = self.__run_lstm(self.input_x2_embedded, self.input_x2_length)\n",
    "            with tf.variable_scope('Mutual-Attention') as scope:\n",
    "                x1_to_x2_att = self.__run_attention(outputs_x2, final_state_x1)\n",
    "                    # x1 attending to x2, <batch-size, hid-size*2>\n",
    "                scope.reuse_variables()\n",
    "                x2_to_x1_att = self.__run_attention(outputs_x1, final_state_x2) \n",
    "                    # x2 attending to x1\n",
    "            self.final_vec_x1 = tf.concat([final_state_x1, x1_to_x2_att],axis=1)\n",
    "            self.final_vec_x2 = tf.concat([final_state_x2, x2_to_x1_att],axis=1)\n",
    "            self.final_vec_size = self.config['hid_size']*2*self.config['n_layer'] + \\\n",
    "                                  self.config['hid_size']*2\n",
    "                \n",
    "        elif add_context_reader:\n",
    "            with tf.variable_scope('Bi-LSTM') as scope:\n",
    "                final_state_x1, outputs_x1 = self.__run_lstm(self.input_x1_embedded, self.input_x1_length)\n",
    "                scope.reuse_variables() \n",
    "                final_state_x2, outputs_x2 = self.__run_lstm(self.input_x2_embedded, self.input_x2_length) \n",
    "            with tf.variable_scope('Context-reader'):\n",
    "                bc, _ = tf.unstack(tf.shape(final_state_x1))\n",
    "                ctx = tf.tile(self.__run_cnn(self.input_ctx_embedded), [bc, 1])\n",
    "                    # op1: <batch-size,total-num-filters>\n",
    "                    # op2: create batch-size copies of the context vec.\n",
    "            self.final_vec_x1 = tf.concat([final_state_x1, ctx],axis=1)\n",
    "            self.final_vec_x2 = tf.concat([final_state_x2, ctx],axis=1) \n",
    "            self.final_vec_size = self.config['hid_size']*2*self.config['n_layer'] + \\\n",
    "                                  3*50\n",
    "        \n",
    "        else:\n",
    "            with tf.variable_scope('Bi-LSTM') as scope:\n",
    "                final_state_x1 = self.__run_lstm(self.input_x1_embedded, self.input_x1_length)\n",
    "                scope.reuse_variables()\n",
    "                final_state_x2 = self.__run_lstm(self.input_x2_embedded, self.input_x2_length)\n",
    "            self.final_vec_x1 = final_state_x1\n",
    "            self.final_vec_x2 = final_state_x2\n",
    "            self.final_vec_size = self.config['hid_size']*2*self.config['n_layer']\n",
    "        \n",
    "    def __run_score_and_predictions(self):\n",
    "        \n",
    "        W_bi = tf.get_variable('W_bi', [self.final_vec_size, self.final_vec_size], \n",
    "                               initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.scores = tf.nn.sigmoid(tf.diag_part(tf.matmul(tf.matmul(self.final_vec_x1,W_bi),\n",
    "                                                           tf.transpose(self.final_vec_x2))), name='scores')\n",
    "        self.predictions = tf.cast(tf.round(self.scores), tf.int32, name='predictions')\n",
    "    \n",
    "    def __run_accuracy(self):\n",
    "        \n",
    "        with tf.name_scope('Accuracy'):\n",
    "            correct_predictions = tf.equal(self.predictions, self.input_y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "    \n",
    "    def __run_optimization(self):\n",
    "        \n",
    "        with tf.name_scope('Loss'):\n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(self.input_y, tf.float32), \n",
    "                                                             logits=self.scores)\n",
    "            self.loss = tf.reduce_mean(losses, name='loss')  \n",
    "    \n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(self.config['learning_rate'])\n",
    "        grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "        self.train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step, name='train_op')\n",
    "    \n",
    "    # Load and train old graph\n",
    "    \n",
    "    def __load_saved_graph(self):\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.saver = tf.train.import_meta_graph(self.config['restore_dir'] + self.config['restore_name'])\n",
    "        self.saver.restore(self.sess, tf.train.latest_checkpoint(self.config['restore_dir']))\n",
    "        self.graph = tf.get_default_graph()\n",
    "        \n",
    "        self.input_x1 = self.graph.get_tensor_by_name('input_x1:0')\n",
    "        self.input_x2 = self.graph.get_tensor_by_name('input_x2:0')\n",
    "        self.input_x1_length = self.graph.get_tensor_by_name('input_x1_length:0')\n",
    "        self.input_x2_length = self.graph.get_tensor_by_name('input_x2_length:0')\n",
    "        self.input_y = self.graph.get_tensor_by_name('input_y:0')\n",
    "        self.keep_prob = self.graph.get_tensor_by_name('keep_prob:0')\n",
    "        \n",
    "        if self.config['context']:\n",
    "            self.input_ctx = self.graph.get_tensor_by_name('input_ctx:0')\n",
    "            \n",
    "        self.scores = self.graph.get_tensor_by_name('scores:0')\n",
    "        self.predictions = self.graph.get_tensor_by_name('predictions:0')\n",
    "        self.loss = self.graph.get_tensor_by_name('Loss/loss:0')\n",
    "        self.accuracy = self.graph.get_tensor_by_name('Accuracy/accuracy:0')\n",
    "        self.global_step = self.graph.get_tensor_by_name('global_step:0')\n",
    "        self.train_op = self.graph.get_tensor_by_name('train_op:0') \n",
    "\n",
    "class DataBatcher:\n",
    "    \n",
    "    def __init__(self, data_dir, context=False):\n",
    "        \n",
    "        self.data_dir = data_dir\n",
    "        self.context = context # tuple: (bool:context-or-not, context-length)\n",
    "        \n",
    "    def __generate_pair_batch(self, doc_a, doc_b, k):\n",
    "        \n",
    "        batch_x1, batch_x2, batch_y = [], [], []\n",
    "        ys = [1,0,0,1]\n",
    "        for _ in range(k): # 4 entries added per iteration.\n",
    "            for i,(da,db) in enumerate(product([doc_a, doc_b], \n",
    "                                               [doc_a, doc_b])):\n",
    "                batch_x1.append(random.choice(da))\n",
    "                batch_x2.append(random.choice(db))\n",
    "                batch_y.append(ys[i])\n",
    "        return batch(batch_x1), batch(batch_x2), np.array(batch_y)\n",
    "    \n",
    "    def get_batch(self, filename, n=32):\n",
    "        assert n%4==0\n",
    "        doc_a, doc_b, doc_mix = dill.load(open(self.data_dir+filename, 'rb'))\n",
    "        (batch_x1,batch_x1_len), (batch_x2,batch_x2_len), batch_y = self.__generate_pair_batch(doc_a,doc_b,int(n//4))\n",
    "        if self.context[0]:\n",
    "            CTX_LEN = self.context[1]\n",
    "            doc_mix_flat = list(chain.from_iterable(doc_mix))\n",
    "            doc_mix_len = len(doc_mix_flat)\n",
    "            doc_mix_padded = np.array(doc_mix_flat[:CTX_LEN]) if doc_mix_len>=CTX_LEN \\\n",
    "                                 else np.array(doc_mix_flat+[0]*(CTX_LEN-doc_mix_len))\n",
    "            batch_ctx = np.array([doc_mix_padded])\n",
    "            return batch_x1,batch_x1_len,batch_x2,batch_x2_len,batch_ctx,batch_y\n",
    "        return batch_x1,batch_x1_len,batch_x2,batch_x2_len,batch_y\n",
    "        \n",
    "def train_pairwise_clf(config):\n",
    "    \n",
    "    clf = PairwiseSentenceClassifier(config)\n",
    "    \n",
    "    dat = DataBatcher(clf.config['data_dir'], context=(clf.config['context'], clf.config['context_length']))\n",
    "    \n",
    "    track_dir, session_id = clf.config['track_dir'], clf.config['session_id']\n",
    "    log_mode = 'w' if clf.config['new_track'] else 'a'\n",
    "    with open(track_dir+session_id+'.txt', log_mode) as f:\n",
    "        f.write('\\n\\n=== NEW SESSION ===\\n\\n')\n",
    "    loss_track, accuracy_track = [], []\n",
    "    start = time.time()\n",
    "    try:\n",
    "        for e in range(clf.config['n_epoch']):\n",
    "            with open(track_dir+session_id+'.txt', 'a') as f:\n",
    "                f.write('Epoch '+str(e+1)+'\\n')\n",
    "            file_indices = np.random.choice(list(range(len(clf.FILENAMES))), \n",
    "                                            size=clf.config['train_size'], replace=False)\n",
    "            random.shuffle(file_indices)\n",
    "            curr_loss_track, curr_accuracy_track = [], []\n",
    "            for file_idx in file_indices:\n",
    "                try:\n",
    "                    if clf.config['context']:\n",
    "                        batch_x1,batch_x1_length,batch_x2,batch_x2_length,batch_ctx,batch_y = dat.get_batch(clf.FILENAMES[file_idx],n=clf.config['batch_size'])\n",
    "                    else:\n",
    "                        batch_x1,batch_x1_length,batch_x2,batch_x2_length,batch_y = dat.get_batch(clf.FILENAMES[file_idx],n=clf.config['batch_size'])\n",
    "                except:\n",
    "                    continue\n",
    "                fd = {clf.input_x1:batch_x1, clf.input_x1_length:batch_x1_length,\n",
    "                      clf.input_x2:batch_x2, clf.input_x2_length:batch_x2_length,\n",
    "                      clf.input_y:batch_y,\n",
    "                      clf.keep_prob:clf.config['keep_prob']} \n",
    "                if clf.config['context']:\n",
    "                    fd[clf.input_ctx] = batch_ctx\n",
    "                _, step, loss_, accuracy_ = clf.sess.run([clf.train_op, clf.global_step, \n",
    "                                                          clf.loss, clf.accuracy], feed_dict=fd)\n",
    "                curr_loss_track.append(loss_)\n",
    "                curr_accuracy_track.append(accuracy_)\n",
    "                if step % clf.config['save_freq'] == 0:\n",
    "                    checkpoint_model(clf.config['save_dir'], clf.config['save_dir']+clf.config['save_name'],\n",
    "                                     clf.saver, clf.sess)\n",
    "                if step % clf.config['verbose'] == 0:\n",
    "                    with open(track_dir+session_id+'.txt', 'a') as f:\n",
    "                        avg_loss = np.mean(curr_loss_track)\n",
    "                        avg_acc = np.mean(curr_accuracy_track)\n",
    "                        loss_track.append(avg_loss)\n",
    "                        accuracy_track.append(avg_acc)\n",
    "                        f.write('loss & accuracy at step {}: <{}, {}> (time elapsed = {} secs)\\n'.format(step, \n",
    "                                                                                        round(avg_loss,5),\n",
    "                                                                                        round(avg_acc,5),\n",
    "                                                                                        round(time.time()-start,2)))\n",
    "                    start = time.time()\n",
    "                    curr_loss_track, curr_accuracy_track = [], []\n",
    "        with open(track_dir+session_id+'-final.txt', log_mode) as f:\n",
    "            f.write('final avg loss & accuracy: <{}, {}>'.format(round(np.mean(loss_track),5),\n",
    "                                                                 round(np.mean(accuracy_track),5)))\n",
    "    except KeyboardInterrupt:\n",
    "        print('Stopped!')\n",
    "                        \n",
    "    \n",
    "if __name__ == \"__main__\": \n",
    "    \n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--vocab_size', type=int, default=100001)\n",
    "    parser.add_argument('--emb_size', type=int, default=300)\n",
    "    parser.add_argument('--n_layer', type=int, default=2)\n",
    "    parser.add_argument('--hid_size', type=int, default=100)\n",
    "    parser.add_argument('--keep_prob', type=float, default=0.7)\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-5)\n",
    "    parser.add_argument('--n_epoch', type=int, default=3)\n",
    "    parser.add_argument('--train_size', type=int, default=10)\n",
    "    parser.add_argument('--verbose', type=int, default=1)\n",
    "    parser.add_argument('--save_freq', type=int, default=5)\n",
    "    parser.add_argument('--data_dir', type=str, default=\"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_code/\")\n",
    "    parser.add_argument('--info_path', type=str, default=\"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_info/indexer_word2emb_100k.p\")\n",
    "    parser.add_argument('--init_with_glove', type=bool, default=True)\n",
    "    parser.add_argument('--save_dir', type=str, default=\"/work/04233/sw33286/AIDA-INDIV-MODEL-SAVE/temp/\")\n",
    "    parser.add_argument('--save_name', type=str, default=\"temp-model\")\n",
    "    parser.add_argument('--restore_dir', type=str, default=\"/work/04233/sw33286/AIDA-INDIV-MODEL-SAVE/temp/\")\n",
    "    parser.add_argument('--restore_name', type=str, default=\"temp-model.meta\")\n",
    "    parser.add_argument('--load_from_saved', type=bool, default=True)\n",
    "    parser.add_argument('--track_dir', type=str, default=\"/work/04233/sw33286/AIDA-TRACKS/sentence-tracks/\")\n",
    "    parser.add_argument('--new_track', type=bool, default=True)\n",
    "    parser.add_argument('--session_id', type=str, default='0000')\n",
    "    parser.add_argument('--mutual_attention', type=bool, default=False)\n",
    "    parser.add_argument('--context', type=bool, default=False)\n",
    "    parser.add_argument('--context_length', type=int, default=500)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    config = {'batch_size': args.batch_size, 'vocab_size': args.vocab_size, 'emb_size': args.emb_size,\n",
    "              'n_layer': args.n_layer, 'hid_size': args.hid_size,\n",
    "              'keep_prob': args.keep_prob, 'learning_rate': args.learning_rate,\n",
    "              'n_epoch': args.n_epoch, 'train_size': args.train_size, 'verbose': args.verbose,\n",
    "              'save_freq': args.save_freq,\n",
    "              'data_dir': args.data_dir, 'info_path': args.info_path,\n",
    "              'init_with_glove': args.init_with_glove,\n",
    "              'save_dir': args.save_dir, 'save_name': args.save_name,\n",
    "              'restore_dir': args.restore_dir, 'restore_name': args.restore_name,\n",
    "              'load_from_saved': args.load_from_saved,\n",
    "              'track_dir': args.track_dir, 'new_track': args.new_track, 'session_id': args.session_id,\n",
    "              'mutual_attention': args.mutual_attention, \n",
    "              'context': args.context, 'context_length': args.context_length}\n",
    "    \n",
    "    train_pairwise_clf(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first training test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \n",
      "name: Tesla K40m\n",
      "major: 3 minor: 5 memoryClockRate (GHz) 0.745\n",
      "pciBusID 0000:08:00.0\n",
      "Total memory: 11.17GiB\n",
      "Free memory: 11.10GiB\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:08:00.0)\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\n",
      "I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\n",
      "I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\n",
      "I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform CUDA. Devices:\n",
      "I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): Tesla K40m, Compute Capability 3.5\n",
      "New model built for training!\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 7184 get requests, put_count=3407 evicted_count=1000 eviction_rate=0.293513 and unsatisfied allocation rate=0.67887\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 771 get requests, put_count=3785 evicted_count=3000 eviction_rate=0.792602 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 440 get requests, put_count=1459 evicted_count=1000 eviction_rate=0.685401 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3319 get requests, put_count=9339 evicted_count=6000 eviction_rate=0.642467 and unsatisfied allocation rate=0.000301296\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1376 get requests, put_count=4401 evicted_count=3000 eviction_rate=0.681663 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2313 get requests, put_count=4343 evicted_count=2000 eviction_rate=0.460511 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 12477 get requests, put_count=10267 evicted_count=5000 eviction_rate=0.486997 and unsatisfied allocation rate=0.580508\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 372 to 409\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1636 get requests, put_count=3676 evicted_count=2000 eviction_rate=0.54407 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 268 get requests, put_count=1317 evicted_count=1000 eviction_rate=0.759301 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3074 get requests, put_count=9127 evicted_count=6000 eviction_rate=0.65739 and unsatisfied allocation rate=0.000325309\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 526 get requests, put_count=2591 evicted_count=2000 eviction_rate=0.771903 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 797 get requests, put_count=3876 evicted_count=3000 eviction_rate=0.773994 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1921 get requests, put_count=6016 evicted_count=4000 eviction_rate=0.664894 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 856 get requests, put_count=3970 evicted_count=3000 eviction_rate=0.755668 and unsatisfied allocation rate=0.00116822\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 10514 get requests, put_count=9210 evicted_count=3000 eviction_rate=0.325733 and unsatisfied allocation rate=0.422675\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 1540 to 1694\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 310 get requests, put_count=1496 evicted_count=1000 eviction_rate=0.668449 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 812 get requests, put_count=4036 evicted_count=3000 eviction_rate=0.74331 and unsatisfied allocation rate=0.00123153\n"
     ]
    }
   ],
   "source": [
    "!python3 pairwise_classifier.py --save_dir /work/04233/sw33286/AIDA-INDIV-MODEL-SAVE-2/no-context/ --save_name no-context-00 --restore_dir /work/04233/sw33286/AIDA-INDIV-MODEL-SAVE-2/no-context/ --restore_name no-context-00.meta --track_dir /work/04233/sw33286/AIDA-TRACKS/sentence-tracks/no-context/ --session_id 05-05-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cont'd training test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \n",
      "name: Tesla K40m\n",
      "major: 3 minor: 5 memoryClockRate (GHz) 0.745\n",
      "pciBusID 0000:08:00.0\n",
      "Total memory: 11.17GiB\n",
      "Free memory: 11.10GiB\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:08:00.0)\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\n",
      "I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\n",
      "I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\n",
      "I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform CUDA. Devices:\n",
      "I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): Tesla K40m, Compute Capability 3.5\n",
      "Model loaded for continued training!\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 9505 get requests, put_count=4616 evicted_count=1000 eviction_rate=0.216638 and unsatisfied allocation rate=0.630089\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 10282 get requests, put_count=7721 evicted_count=4000 eviction_rate=0.518068 and unsatisfied allocation rate=0.63937\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 146 to 160\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1848 get requests, put_count=4867 evicted_count=3000 eviction_rate=0.616396 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2482 get requests, put_count=4505 evicted_count=2000 eviction_rate=0.443951 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2324 get requests, put_count=6349 evicted_count=4000 eviction_rate=0.63002 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1204 get requests, put_count=2234 evicted_count=1000 eviction_rate=0.447628 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1054 get requests, put_count=5086 evicted_count=4000 eviction_rate=0.786473 and unsatisfied allocation rate=0.000948767\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1218 get requests, put_count=3258 evicted_count=2000 eviction_rate=0.613874 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 12466 get requests, put_count=12314 evicted_count=6000 eviction_rate=0.48725 and unsatisfied allocation rate=0.497032\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 493 to 542\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1814 get requests, put_count=5868 evicted_count=4000 eviction_rate=0.681663 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1488 get requests, put_count=4553 evicted_count=3000 eviction_rate=0.658906 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4646 get requests, put_count=11718 evicted_count=7000 eviction_rate=0.597372 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2346 get requests, put_count=6433 evicted_count=4000 eviction_rate=0.621794 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3104 get requests, put_count=6209 evicted_count=3000 eviction_rate=0.48317 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 554 get requests, put_count=2681 evicted_count=2000 eviction_rate=0.74599 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 9992 get requests, put_count=10219 evicted_count=4000 eviction_rate=0.391428 and unsatisfied allocation rate=0.393014\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 1694 to 1863\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 918 get requests, put_count=3122 evicted_count=2000 eviction_rate=0.640615 and unsatisfied allocation rate=0\n"
     ]
    }
   ],
   "source": [
    "!python3 pairwise_classifier.py --load_from_saved 1 --save_dir /work/04233/sw33286/AIDA-INDIV-MODEL-SAVE-2/no-context/ --save_name no-context-00 --restore_dir /work/04233/sw33286/AIDA-INDIV-MODEL-SAVE-2/no-context/ --restore_name no-context-00.meta --track_dir /work/04233/sw33286/AIDA-TRACKS/sentence-tracks/no-context/ --session_id 05-05-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown option: --\r\n",
      "usage: python3 [option] ... [-c cmd | -m mod | file | -] [arg] ...\r\n",
      "Try `python -h' for more information.\r\n"
     ]
    }
   ],
   "source": [
    "!python3 pairwise_classifier.py --train_size 500000 --verbose 1000 --save_freq 50000 --load_from_saved 1 --save_dir /work/04233/sw33286/AIDA-INDIV-MODEL-SAVE/our-model-no-context/ --save_name our-model-no-context-00 --restore_dir /work/04233/sw33286/AIDA-INDIV-MODEL-SAVE/our-model-no-context/ --restore_name our-model-no-context-00.meta --track_dir /work/04233/sw33286/AIDA-TRACKS/sentence-tracks/no-context/ --session_id 05-06-18-no-context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running time estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \n",
      "name: Tesla K40m\n",
      "major: 3 minor: 5 memoryClockRate (GHz) 0.745\n",
      "pciBusID 0000:08:00.0\n",
      "Total memory: 11.17GiB\n",
      "Free memory: 11.10GiB\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:08:00.0)\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\n",
      "I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\n",
      "I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\n",
      "I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform CUDA. Devices:\n",
      "I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): Tesla K40m, Compute Capability 3.5\n",
      "New model built for training!\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 10207 get requests, put_count=4950 evicted_count=1000 eviction_rate=0.20202 and unsatisfied allocation rate=0.622808\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 766 get requests, put_count=3779 evicted_count=3000 eviction_rate=0.793861 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1886 get requests, put_count=3905 evicted_count=2000 eviction_rate=0.512164 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3360 get requests, put_count=9380 evicted_count=6000 eviction_rate=0.639659 and unsatisfied allocation rate=0.000297619\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2490 get requests, put_count=6515 evicted_count=4000 eviction_rate=0.613968 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1212 get requests, put_count=2242 evicted_count=1000 eviction_rate=0.44603 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1992 get requests, put_count=7024 evicted_count=5000 eviction_rate=0.711845 and unsatisfied allocation rate=0.000502008\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2740 get requests, put_count=5780 evicted_count=3000 eviction_rate=0.519031 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 788 get requests, put_count=1837 evicted_count=1000 eviction_rate=0.544366 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 9756 get requests, put_count=11306 evicted_count=6000 eviction_rate=0.530692 and unsatisfied allocation rate=0.461665\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 596 to 655\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3189 get requests, put_count=9253 evicted_count=6000 eviction_rate=0.648438 and unsatisfied allocation rate=0.000313578\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 783 get requests, put_count=3862 evicted_count=3000 eviction_rate=0.7768 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2163 get requests, put_count=5258 evicted_count=3000 eviction_rate=0.570559 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 508 get requests, put_count=1623 evicted_count=1000 eviction_rate=0.616143 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 288 get requests, put_count=1428 evicted_count=1000 eviction_rate=0.70028 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 294 get requests, put_count=1463 evicted_count=1000 eviction_rate=0.683527 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 406 get requests, put_count=1631 evicted_count=1000 eviction_rate=0.613121 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 13152 get requests, put_count=11485 evicted_count=2000 eviction_rate=0.17414 and unsatisfied allocation rate=0.299498\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 2997 to 3296\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 358 get requests, put_count=1756 evicted_count=1000 eviction_rate=0.569476 and unsatisfied allocation rate=0\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 17960 get requests, put_count=18115 evicted_count=1000 eviction_rate=0.0552029 and unsatisfied allocation rate=0.0827394\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 7059 to 7764\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 8824023 get requests, put_count=8825600 evicted_count=4000 eviction_rate=0.000453227 and unsatisfied allocation rate=0.000391658\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 31305939 get requests, put_count=31307515 evicted_count=14000 eviction_rate=0.000447177 and unsatisfied allocation rate=0.000429855\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 56785467 get requests, put_count=56787042 evicted_count=24000 eviction_rate=0.000422632 and unsatisfied allocation rate=0.000413099\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 82276509 get requests, put_count=82277757 evicted_count=34000 eviction_rate=0.000413234 and unsatisfied allocation rate=0.000410628\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 88420725 get requests, put_count=88422302 evicted_count=44000 eviction_rate=0.000497612 and unsatisfied allocation rate=0.000491468\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 99725233 get requests, put_count=99721847 evicted_count=54000 eviction_rate=0.000541506 and unsatisfied allocation rate=0.0005858\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 112561923 get requests, put_count=112563499 evicted_count=64000 eviction_rate=0.000568568 and unsatisfied allocation rate=0.000563752\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 123192129 get requests, put_count=123193616 evicted_count=74000 eviction_rate=0.00060068 and unsatisfied allocation rate=0.000597002\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 145736769 get requests, put_count=145738347 evicted_count=84000 eviction_rate=0.000576375 and unsatisfied allocation rate=0.000572642\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 161358885 get requests, put_count=161360461 evicted_count=94000 eviction_rate=0.000582547 and unsatisfied allocation rate=0.000579187\n"
     ]
    }
   ],
   "source": [
    "!python3 pairwise_classifier.py --train_size 5000 --verbose 1000 --save_freq 5000 --save_dir /work/04233/sw33286/AIDA-INDIV-MODEL-SAVE-2/no-context/ --save_name no-context-00 --restore_dir /work/04233/sw33286/AIDA-INDIV-MODEL-SAVE-2/no-context/ --restore_name no-context-00.meta --track_dir /work/04233/sw33286/AIDA-TRACKS/sentence-tracks/no-context/ --session_id 05-05-18"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
