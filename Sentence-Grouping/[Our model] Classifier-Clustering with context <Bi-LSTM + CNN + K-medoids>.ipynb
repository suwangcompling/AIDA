{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/work/04233/sw33286/AIDA-SCRIPTS\")\n",
    "sys.path.insert(0, \"/home/04233/sw33286/AIDA-package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell, MultiRNNCell, DropoutWrapper\n",
    "\n",
    "from kmedoids import kMedoids\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from helpers import Indexer, batch\n",
    "from itertools import chain, product\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100001, 300)\n"
     ]
    }
   ],
   "source": [
    "# Link to NYT data folder\n",
    "\n",
    "nyt_code_dir = \"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_code/\"\n",
    "FILE_NAMES = os.listdir(nyt_code_dir)\n",
    "CTX_LEN = 500\n",
    "\n",
    "# Link to dictionary information\n",
    "\n",
    "info_path = \"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_info/indexer_word2emb_100k.p\"\n",
    "indexer100k, word2emb100k = dill.load(open(info_path, 'rb'))\n",
    "glove_embs = []\n",
    "for i in range(len(indexer100k)):\n",
    "    glove_embs.append(word2emb100k[indexer100k.get_object(i)])\n",
    "glove_embs = np.array(glove_embs)\n",
    "print(glove_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data batching\n",
    "\n",
    "def batch_mixture(doc_a, doc_b, doc_mix, k=25):\n",
    "    batch_x1, batch_x2, batch_y = [], [], []\n",
    "    ys = [1,0,0,1]\n",
    "    for _ in range(k): # 4 entries added per iteration.\n",
    "        for i,(da,db) in enumerate(product([doc_a, doc_b], \n",
    "                                           [doc_a, doc_b])):\n",
    "            batch_x1.append(random.choice(da))\n",
    "            batch_x2.append(random.choice(db))\n",
    "            batch_y.append(ys[i])\n",
    "    doc_mix_flat = list(chain.from_iterable(doc_mix))\n",
    "    doc_mix_len = len(doc_mix_flat)\n",
    "    doc_mix_padded = np.array(doc_mix_flat[:CTX_LEN]) if doc_mix_len>=CTX_LEN else np.array(doc_mix_flat+[0]*(CTX_LEN-doc_mix_len))\n",
    "    return batch(batch_x1), batch(batch_x2), np.array([doc_mix_padded]), np.array(batch_y)\n",
    "\n",
    "def get_batch(file_idx):\n",
    "    filename = FILE_NAMES[file_idx]\n",
    "    doc_a, doc_b, doc_mix = dill.load(open(nyt_code_dir+filename, 'rb'))\n",
    "    (batch_x1,batch_x1_len), (batch_x2,batch_x2_len), batch_ctx, batch_y = batch_mixture(doc_a,doc_b,doc_mix)\n",
    "    return batch_x1,batch_x1_len,batch_x2,batch_x2_len,batch_ctx,batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "VOCAB_SIZE = len(indexer100k)\n",
    "EMB_SIZE = glove_embs.shape[1]\n",
    "HID_SIZE = 100\n",
    "NUM_LAYERS = 2\n",
    "KEEP_PROB = 0.7\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "# hyperparams for cnn context reader.\n",
    "FILTER_SIZES = [3,4,5]\n",
    "NUM_FILTERS = 50 # finally make 150d context info\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "input_x1 = tf.placeholder(tf.int32, [None, None], name='input_x1') # <max-time, batch-size>\n",
    "input_x2 = tf.placeholder(tf.int32, [None, None], name='input_x2')\n",
    "input_x1_length = tf.placeholder(tf.int32, [None], name='input_x1_length')\n",
    "input_x2_length = tf.placeholder(tf.int32, [None], name='input_x2_length')\n",
    "input_ctx = tf.placeholder(tf.int32, [1, CTX_LEN], name='input_ctx') # <batch-size, height=max-time>\n",
    "input_y  = tf.placeholder(tf.int32, [None], name='input_y')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "with tf.variable_scope('Embeddings'):\n",
    "    embeddings = tf.get_variable('embeddings', glove_embs.shape, \n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "    glove_init = embeddings.assign(glove_embs)\n",
    "    input_x1_embedded = tf.nn.embedding_lookup(embeddings, input_x1) # <max-time, batch-size, emb-size>\n",
    "    input_x2_embedded = tf.nn.embedding_lookup(embeddings, input_x2)\n",
    "    input_ctx_embedded = tf.expand_dims(tf.nn.embedding_lookup(embeddings, input_ctx), -1)\n",
    "        # <batch-size, height=max-time, width=EMB_SIZE, num_channels=1>\n",
    "\n",
    "cell = MultiRNNCell([DropoutWrapper(LSTMCell(HID_SIZE),output_keep_prob=keep_prob)]*NUM_LAYERS) \n",
    "\n",
    "def run_lstm(cell, inputs, inputs_length): # lstm-out size *= 2 by bidirectionality.\n",
    "    ((fw_outputs,bw_outputs), # <max-time, batch-size, hid-size>, attention later if needed.\n",
    "     (fw_final_state,bw_final_state)) = ( # <batch-size, hid-size>\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw=cell,\n",
    "                                        cell_bw=cell,\n",
    "                                        inputs=inputs,\n",
    "                                        sequence_length=inputs_length,\n",
    "                                        dtype=tf.float32, time_major=True)\n",
    "    )\n",
    "    return tf.concat([tf.concat([fw_state_tuple.h,bw_state_tuple.h], 1) # lstm-out size *= NUM_LAYERS by stacking.\n",
    "                      for fw_state_tuple,bw_state_tuple in zip(fw_final_state,bw_final_state)], 1)\n",
    "    \n",
    "with tf.variable_scope('Bi-LSTM') as scope:\n",
    "    final_state_x1 = run_lstm(cell, input_x1_embedded, input_x1_length)\n",
    "    scope.reuse_variables() # both sentence inputs share the same weights.\n",
    "    final_state_x2 = run_lstm(cell, input_x2_embedded, input_x2_length)\n",
    "\n",
    "def run_cnn(inputs):\n",
    "    pool_outputs = []\n",
    "    for i,filter_size in enumerate(FILTER_SIZES):\n",
    "        with tf.variable_scope('CNN-ctx-%s' % filter_size):\n",
    "            filter_shape = [filter_size, EMB_SIZE, NUM_CHANNELS, NUM_FILTERS]\n",
    "            W = tf.get_variable('W', filter_shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.get_variable('b', [NUM_FILTERS], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            conv = tf.nn.conv2d(inputs, W, strides=[1,1,1,1], padding='VALID', name='conv')\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
    "            pool = tf.nn.max_pool(h, ksize=[1,CTX_LEN-filter_size+1,1,1], strides=[1,1,1,1], padding='VALID', name='pool')\n",
    "            pool_outputs.append(pool)\n",
    "    num_filters_total = NUM_FILTERS * len(FILTER_SIZES)\n",
    "    h_pool_flat = tf.nn.dropout(tf.reshape(tf.concat(pool_outputs, 3), [-1, num_filters_total]), keep_prob)\n",
    "    return h_pool_flat\n",
    "\n",
    "bc, _ = tf.unstack(tf.shape(final_state_x1))\n",
    "ctx = tf.tile(run_cnn(input_ctx_embedded), [bc, 1]) \n",
    "    # op1: <batch-size=1,total-num-filters>\n",
    "    # op2: create batch-size copies of the context vec.\n",
    "\n",
    "final_vec_size = HID_SIZE*2*2 + NUM_FILTERS*len(FILTER_SIZES)\n",
    "    # op1: bidirection=*2, 2-layer stacked bi-lstm=*2.\n",
    "    # op2: compute the total number of filters.\n",
    "final_vec_x1 = tf.concat([final_state_x1, ctx], 1)\n",
    "final_vec_x2 = tf.concat([final_state_x2, ctx], 1)\n",
    "W_bi = tf.get_variable('W_bi', [final_vec_size, final_vec_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "scores = tf.nn.sigmoid(tf.diag_part(tf.matmul(tf.matmul(final_vec_x1,W_bi),tf.transpose(final_vec_x2))),name='scores')\n",
    "predictions = tf.cast(tf.round(scores), tf.int32, name='predictions') \n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(input_y, tf.float32), logits=scores)\n",
    "    loss = tf.reduce_mean(losses, name='loss')\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct_predictions = tf.equal(predictions, input_y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "    \n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step, name='train_op')\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "TRAIN_SIZE = 10\n",
    "# TRAIN_SIZE = len(FILE_NAMES)\n",
    "VERBOSE = 1000\n",
    "ERROR_LOG = []\n",
    "\n",
    "loss_track, accuracy_track = [], []\n",
    "start = time.time()\n",
    "try:\n",
    "    for e in range(NUM_EPOCHS):\n",
    "        print('Epoch ', e+1)\n",
    "        print('\\n')\n",
    "        file_indices = np.random.choice(list(range(len(FILE_NAMES))), size=TRAIN_SIZE, replace=False)\n",
    "        random.shuffle(file_indices)  \n",
    "        curr_loss_track, curr_accuracy_track = [], []\n",
    "        for file_idx in file_indices:\n",
    "            try:\n",
    "                batch_x1,batch_x1_length,batch_x2,batch_x2_length,batch_ctx,batch_y = get_batch(file_idx)\n",
    "            except:\n",
    "                ERROR_LOG.append(file_idx)\n",
    "                continue\n",
    "            fd = {input_x1:batch_x1, input_x1_length:batch_x1_length,\n",
    "                  input_x2:batch_x2, input_x2_length:batch_x2_length,\n",
    "                  input_ctx:batch_ctx,\n",
    "                  input_y:batch_y,\n",
    "                  keep_prob:KEEP_PROB}   \n",
    "            _, step, loss_, accuracy_ = sess.run([train_op, global_step, loss, accuracy], feed_dict=fd)\n",
    "            curr_loss_track.append(loss_)\n",
    "            curr_accuracy_track.append(accuracy_)\n",
    "            if step%VERBOSE==0:\n",
    "                print('  batch loss & accuracy at step {}: <{}, {}> (time elapsed = {})'.format(step, loss_, accuracy_,\n",
    "                                                                                                time.time()-start))\n",
    "                start = time.time()\n",
    "        print('\\n')\n",
    "        print('  epoch mean loss & accuracy: <{}, {}>'.format(np.mean(curr_loss_track),np.mean(curr_accuracy_track)))\n",
    "        print('\\n')    \n",
    "        loss_track += curr_loss_track\n",
    "        accuracy_track += curr_accuracy_track \n",
    "except KeyboardInterrupt:\n",
    "    print('Stopped!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_files(target_dir):\n",
    "    for filename in os.listdir(target_dir):\n",
    "        os.remove(os.path.abspath(os.path.join(target_dir, filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/04233/sw33286/AIDA-INDIV-MODEL-SAVE/our-model-with-context-kmedoids/our-model-with-context-00'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = \"/work/04233/sw33286/AIDA-INDIV-MODEL-SAVE/our-model-with-context-kmedoids/\"\n",
    "save_path = save_dir + \"our-model-with-context-00\"\n",
    "remove_all_files(save_dir)\n",
    "saver.save(sess, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM + HAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "\n",
    "def get_rand_mixture():\n",
    "    filename = random.choice(FILE_NAMES)\n",
    "    da,db, doc_mix = dill.load(open(nyt_code_dir+filename, 'rb'))\n",
    "    doc_lbs = []\n",
    "    for sentcode in doc_mix:\n",
    "        if sentcode in da:\n",
    "            doc_lbs.append(0)\n",
    "        else:\n",
    "            doc_lbs.append(1)\n",
    "    doc_mix_flat = list(chain.from_iterable(doc_mix))\n",
    "    doc_mix_len = len(doc_mix_flat)\n",
    "    ctx = np.array([doc_mix_flat[:CTX_LEN]]) if doc_mix_len>=CTX_LEN else np.array([doc_mix_flat+[0]*(CTX_LEN-doc_mix_len)])\n",
    "    return doc_mix, doc_lbs, ctx\n",
    "\n",
    "def to_labels(C, doc_len): # C: {cls:[datum_id, ...], ...}\n",
    "    lbs = [0]*doc_len\n",
    "    for idx in C[1]:\n",
    "        lbs[idx] = 1\n",
    "    return lbs\n",
    "\n",
    "def flip_clust(clust):\n",
    "    return np.array([0 if i==1 else 1 for i in clust])\n",
    "\n",
    "def clust_accuracy(true, pred):\n",
    "    return max(accuracy_score(true, pred),\n",
    "               accuracy_score(true, flip_clust(pred)))\n",
    "\n",
    "def to_sent(code):\n",
    "    return [indexer100k.get_object(idx) for idx in code]\n",
    "\n",
    "# Bi-LSTM + HAC class\n",
    "\n",
    "class ClfKM:\n",
    "    \n",
    "    def __init__(self, clf_dir, clf_filename):\n",
    "        self.sess = tf.Session()\n",
    "        saver = tf.train.import_meta_graph(clf_dir + clf_filename)\n",
    "        saver.restore(self.sess, tf.train.latest_checkpoint(clf_dir))\n",
    "        self.graph = tf.get_default_graph()\n",
    "        self.input_x1 = self.graph.get_tensor_by_name('input_x1:0')\n",
    "        self.input_x2 = self.graph.get_tensor_by_name('input_x2:0')\n",
    "        self.input_x1_length = self.graph.get_tensor_by_name('input_x1_length:0')\n",
    "        self.input_x2_length = self.graph.get_tensor_by_name('input_x2_length:0')\n",
    "        self.input_ctx = self.graph.get_tensor_by_name('input_ctx:0')\n",
    "        self.input_y = self.graph.get_tensor_by_name('input_y:0')\n",
    "        self.keep_prob = self.graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "        self.scores = self.graph.get_tensor_by_name('scores:0')\n",
    "        self.predictions = self.graph.get_tensor_by_name('predictions:0')\n",
    "        self.loss = self.graph.get_tensor_by_name('Loss/loss:0')\n",
    "        self.accuracy = self.graph.get_tensor_by_name('Accuracy/accuracy:0')\n",
    "        self.global_step = self.graph.get_tensor_by_name('global_step:0')\n",
    "        self.train_op = self.graph.get_tensor_by_name('train_op:0')\n",
    "        \n",
    "    def dist(self, x1, x2):\n",
    "        x1, x1_len = batch([x1])\n",
    "        x2, x2_len = batch([x2])\n",
    "        fd = {self.input_x1:x1, self.input_x1_length:x1_len,\n",
    "              self.input_x2:x2, self.input_x2_length:x2_len,\n",
    "              self.input_ctx:self.ctx,\n",
    "              self.keep_prob:1.0}\n",
    "        conf = self.sess.run(self.scores, feed_dict=fd)\n",
    "        return 1-conf[0]\n",
    "    \n",
    "    def evaluate(self, doc_mix, doc_lbs, ctx, method='average', plot=True):\n",
    "        self.ctx = ctx\n",
    "        doc_mix_sq, _ = batch(doc_mix)\n",
    "        doc_mix_sq = doc_mix_sq.T\n",
    "        _, doc_mix_clust = kMedoids(squareform(pdist(doc_mix_sq,metric=self.dist)), 2)\n",
    "        doc_prd = to_labels(doc_mix_clust, len(doc_mix))\n",
    "        acc = clust_accuracy(doc_lbs, doc_prd)\n",
    "        return acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_dir = \"/work/04233/sw33286/AIDA-INDIV-MODEL-SAVE/our-model-with-context-kmedoids/\"\n",
    "restore_filename = \"our-model-with-context-00.meta\"\n",
    "clf_km = ClfKM(restore_dir, restore_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_km.evaluate(*get_rand_mixture())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_evaluation(k=100):\n",
    "    accuracies = []\n",
    "    for _ in range(k):\n",
    "        _, acc = clf_km.evaluate(*get_rand_mixture(), plot=False)\n",
    "        accuracies.append(acc)\n",
    "    print('Average clustering accuracy over {} samples = {}'.format(k, np.mean(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rand_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
