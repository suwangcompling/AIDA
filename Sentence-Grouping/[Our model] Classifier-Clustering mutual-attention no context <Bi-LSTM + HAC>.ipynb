{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/work/04233/sw33286/AIDA-SCRIPTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell, MultiRNNCell, DropoutWrapper\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import colorama\n",
    "from colorama import Style\n",
    "\n",
    "from helpers import Indexer, batch\n",
    "from itertools import chain, product\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100001, 300)\n"
     ]
    }
   ],
   "source": [
    "# Link to NYT data folder\n",
    "\n",
    "nyt_code_dir = \"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_code/\"\n",
    "FILE_NAMES = os.listdir(nyt_code_dir)\n",
    "\n",
    "# Link to dictionary information\n",
    "\n",
    "info_path = \"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_info/indexer_word2emb_100k.p\"\n",
    "indexer100k, word2emb100k = dill.load(open(info_path, 'rb'))\n",
    "glove_embs = []\n",
    "for i in range(len(indexer100k)):\n",
    "    glove_embs.append(word2emb100k[indexer100k.get_object(i)])\n",
    "glove_embs = np.array(glove_embs)\n",
    "print(glove_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data batching\n",
    "\n",
    "def batch_mixture(doc_a, doc_b, k=25):\n",
    "    batch_x1, batch_x2, batch_y = [], [], []\n",
    "    ys = [1,0,0,1]\n",
    "    for _ in range(k): # 4 entries added per iteration.\n",
    "        for i,(da,db) in enumerate(product([doc_a, doc_b], \n",
    "                                           [doc_a, doc_b])):\n",
    "            batch_x1.append(random.choice(da))\n",
    "            batch_x2.append(random.choice(db))\n",
    "            batch_y.append(ys[i])\n",
    "    return batch(batch_x1), batch(batch_x2), np.array(batch_y)\n",
    "\n",
    "def get_batch(file_idx):\n",
    "    filename = FILE_NAMES[file_idx]\n",
    "    doc_a, doc_b, doc_mix = dill.load(open(nyt_code_dir+filename, 'rb'))\n",
    "    (batch_x1,batch_x1_len), (batch_x2,batch_x2_len), batch_y = batch_mixture(doc_a,doc_b)\n",
    "    return batch_x1,batch_x1_len,batch_x2,batch_x2_len,batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "VOCAB_SIZE = len(indexer100k)\n",
    "EMB_SIZE = glove_embs.shape[1]\n",
    "HID_SIZE = 100\n",
    "NUM_LAYERS = 2\n",
    "KEEP_PROB = 0.7\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "input_x1 = tf.placeholder(tf.int32, [None, None], name='input_x1') # <max-time, batch-size>\n",
    "input_x2 = tf.placeholder(tf.int32, [None, None], name='input_x2')\n",
    "input_x1_length = tf.placeholder(tf.int32, [None], name='input_x1_length')\n",
    "input_x2_length = tf.placeholder(tf.int32, [None], name='input_x2_length')\n",
    "input_y  = tf.placeholder(tf.int32, [None], name='input_y')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "with tf.variable_scope('Embeddings'):\n",
    "    embeddings = tf.get_variable('embeddings', glove_embs.shape, \n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "    glove_init = embeddings.assign(glove_embs)\n",
    "    input_x1_embedded = tf.nn.embedding_lookup(embeddings, input_x1) # <max-time, batch-size, emb-size>\n",
    "    input_x2_embedded = tf.nn.embedding_lookup(embeddings, input_x2)\n",
    "\n",
    "cell = MultiRNNCell([DropoutWrapper(LSTMCell(HID_SIZE),output_keep_prob=keep_prob)]*NUM_LAYERS) \n",
    "\n",
    "def run_lstm(cell, inputs, inputs_length): # lstm-out size *= 2 by bidirectionality.\n",
    "    ((fw_outputs,bw_outputs), # <max-time, batch-size, hid-size>, attention later if needed.\n",
    "     (fw_final_state,bw_final_state)) = ( # <batch-size, hid-size>\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw=cell,\n",
    "                                        cell_bw=cell,\n",
    "                                        inputs=inputs,\n",
    "                                        sequence_length=inputs_length,\n",
    "                                        dtype=tf.float32, time_major=True)\n",
    "    )\n",
    "    return tf.concat([tf.concat([fw_state_tuple.h,bw_state_tuple.h], 1) # lstm-out size *= NUM_LAYERS by stacking.\n",
    "                      for fw_state_tuple,bw_state_tuple in zip(fw_final_state,bw_final_state)], 1), \\\n",
    "           tf.transpose(tf.concat([fw_outputs,bw_outputs], 2), [1,0,2])\n",
    "        # op1: <batch-size, hid-size*2*num-layers>\n",
    "        # op2: <max-time, batch-size, hid-size*2> -> <batch-size, max-time, hid-size*2>\n",
    "        # NB: batch-size = 1 here.\n",
    "           \n",
    "with tf.variable_scope('Bi-LSTM') as scope:\n",
    "    final_state_x1, outputs_x1 = run_lstm(cell, input_x1_embedded, input_x1_length) \n",
    "        # fs_x1: <batch-size, hid-size*2*num-layers>\n",
    "        # o_x1: <batch-size, max-time, hid-size*2>\n",
    "    scope.reuse_variables() # both sentence inputs share the same weights.\n",
    "    final_state_x2, outputs_x2 = run_lstm(cell, input_x2_embedded, input_x2_length)\n",
    "\n",
    "def run_attention(outputs, state):\n",
    "    W_d = tf.get_variable('W_d', [HID_SIZE*2, HID_SIZE*2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    W_s = tf.get_variable('W_s', [HID_SIZE*2*NUM_LAYERS, HID_SIZE*2], \n",
    "                          initializer=tf.contrib.layers.xavier_initializer())\n",
    "    d_W = tf.tensordot(outputs, W_d, axes=[[2],[0]])\n",
    "        # <batch-size, max-time, hid-size*2> * <hid-size*2, hid-size*2> = <batch-size, max-time, hid-size*2>\n",
    "    s_W = tf.expand_dims(tf.matmul(state, W_s), axis=1)\n",
    "        # op1. <batch-size, hid-size*2*num-layers> * <hid-size*2*num-layers, hid-size*2> -> <batch-size, hid-size*2>\n",
    "        # op2. <batch-size, hid-size*2> -> <batch-size, 1, hid-size*2>\n",
    "    a_tsr = tf.nn.tanh(tf.add(d_W, s_W))\n",
    "        # op1. <batch-size, max-time, hid-size*2> + <batch-size, 1, hid-size*2> -> <batch-size, max-time, hid-size*2>\n",
    "        # op2. elem-wise nonlinearity.\n",
    "    W_a = tf.get_variable('W_a', [HID_SIZE*2, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    a_W = tf.nn.softmax(tf.tensordot(a_tsr, W_a, axes=[[2],[0]]), dim=1)\n",
    "        # op1. <batch-size, max-time, hid-size*2> * <hid-size*2, 1> -> <batch-size, max-time, 1>\n",
    "        # op2. softmax over max-time.\n",
    "    d_a = tf.reduce_sum(tf.multiply(outputs, a_W), axis=1)\n",
    "        # op1. <batch-size, max-time, hid-size*2> elem* <batch-size, max-time, 1> -> <batch-size, max-time, hid-size*2>\n",
    "        # op2. sum over max-time (weighted sum) -> <batch-size, hid-size*2>\n",
    "    return d_a    \n",
    "\n",
    "with tf.variable_scope('Mutual-Attention') as scope:\n",
    "    x1_to_x2_att = run_attention(outputs_x2, final_state_x1) # x1 attending to x2, <batch-size, hid-size*2>\n",
    "    scope.reuse_variables()\n",
    "    x2_to_x1_att = run_attention(outputs_x1, final_state_x2) # x2 attending to x1\n",
    "\n",
    "def run_scores(fs_x1, fs_x2, att_12, att_21):\n",
    "    W_bi = tf.get_variable('W_bi', [HID_SIZE*2*NUM_LAYERS+HID_SIZE*2, HID_SIZE*2*NUM_LAYERS+HID_SIZE*2], \n",
    "                           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    fv_x1 = tf.concat([fs_x1,att_12],axis=1) # <batch-size, hid-size*2>\n",
    "    fv_x2 = tf.concat([fs_x2,att_21],axis=1)\n",
    "    return tf.nn.sigmoid(tf.diag_part(tf.matmul(tf.matmul(fv_x1,W_bi),tf.transpose(fv_x2))),name='scores')\n",
    "    \n",
    "scores = run_scores(final_state_x1, final_state_x2, x1_to_x2_att, x2_to_x1_att)\n",
    "predictions = tf.cast(tf.round(scores), tf.int32, name='predictions')     \n",
    "    \n",
    "with tf.name_scope('Loss'):\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(input_y, tf.float32), logits=scores)\n",
    "    loss = tf.reduce_mean(losses, name='loss')\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct_predictions = tf.equal(predictions, input_y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "    \n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step, name='train_op')\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "TRAIN_SIZE = 10\n",
    "# TRAIN_SIZE = len(FILE_NAMES)\n",
    "VERBOSE = 1\n",
    "ERROR_LOG = []\n",
    "\n",
    "loss_track, accuracy_track = [], []\n",
    "start = time.time()\n",
    "try:\n",
    "    for e in range(NUM_EPOCHS):\n",
    "        print('Epoch ', e+1)\n",
    "        print('\\n')\n",
    "        file_indices = np.random.choice(list(range(len(FILE_NAMES))), size=TRAIN_SIZE, replace=False)\n",
    "        random.shuffle(file_indices)  \n",
    "        curr_loss_track, curr_accuracy_track = [], []\n",
    "        for file_idx in file_indices:\n",
    "            try:\n",
    "                batch_x1,batch_x1_length,batch_x2,batch_x2_length,batch_y = get_batch(file_idx)\n",
    "            except:\n",
    "                ERROR_LOG.append(file_idx)\n",
    "                continue\n",
    "            fd = {input_x1:batch_x1, input_x1_length:batch_x1_length,\n",
    "                  input_x2:batch_x2, input_x2_length:batch_x2_length,\n",
    "                  input_y:batch_y,\n",
    "                  keep_prob:KEEP_PROB}   \n",
    "            _, step, loss_, accuracy_ = sess.run([train_op, global_step, loss, accuracy], feed_dict=fd)\n",
    "            curr_loss_track.append(loss_)\n",
    "            curr_accuracy_track.append(accuracy_)\n",
    "            if step%VERBOSE==0:\n",
    "                print('  batch loss & accuracy at step {}: <{}, {}> (time elapsed = {})'.format(step, \n",
    "                                                                                                np.mean(curr_loss_track),\n",
    "                                                                                                np.mean(curr_accuracy_track),\n",
    "                                                                                                time.time()-start))\n",
    "                start = time.time()\n",
    "        print('\\n')\n",
    "        print('  epoch mean loss & accuracy: <{}, {}>'.format(np.mean(curr_loss_track),np.mean(curr_accuracy_track)))\n",
    "        print('\\n')    \n",
    "        loss_track += curr_loss_track\n",
    "        accuracy_track += curr_accuracy_track \n",
    "except KeyboardInterrupt:\n",
    "    print('Stopped!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_files(target_dir):\n",
    "    for filename in os.listdir(target_dir):\n",
    "        os.remove(os.path.abspath(os.path.join(target_dir, filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/04233/sw33286/AIDA-INDIV-MODEL-SAVE/our-model-no-context-mutual-attention/our-model-no-context-mutual-attention-00'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = \"/work/04233/sw33286/AIDA-INDIV-MODEL-SAVE/our-model-no-context-mutual-attention/\"\n",
    "save_path = save_dir + \"our-model-no-context-mutual-attention-00\"\n",
    "remove_all_files(save_dir)\n",
    "saver.save(sess, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM + HAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "\n",
    "def get_rand_mixture():\n",
    "    filename = random.choice(FILE_NAMES)\n",
    "    da,db, doc_mix = dill.load(open(nyt_code_dir+filename, 'rb'))\n",
    "    doc_lbs = []\n",
    "    for sentcode in doc_mix:\n",
    "        if sentcode in da:\n",
    "            doc_lbs.append(0)\n",
    "        else:\n",
    "            doc_lbs.append(1)\n",
    "    return doc_mix, doc_lbs\n",
    "\n",
    "def flip_clust(clust):\n",
    "    return np.array([0 if i==1 else 1 for i in clust])\n",
    "\n",
    "def clust_accuracy(true, pred):\n",
    "    return max(accuracy_score(true, pred),\n",
    "               accuracy_score(true, flip_clust(pred)))\n",
    "\n",
    "def to_sent(code):\n",
    "    return [indexer100k.get_object(idx) for idx in code]\n",
    "\n",
    "# Bi-LSTM + HAC class\n",
    "\n",
    "class ClfHAC:\n",
    "    \n",
    "    def __init__(self, clf_dir, clf_filename):\n",
    "        self.sess = tf.Session()\n",
    "        saver = tf.train.import_meta_graph(clf_dir + clf_filename)\n",
    "        saver.restore(self.sess, tf.train.latest_checkpoint(clf_dir))\n",
    "        self.graph = tf.get_default_graph()\n",
    "        self.input_x1 = self.graph.get_tensor_by_name('input_x1:0')\n",
    "        self.input_x2 = self.graph.get_tensor_by_name('input_x2:0')\n",
    "        self.input_x1_length = self.graph.get_tensor_by_name('input_x1_length:0')\n",
    "        self.input_x2_length = self.graph.get_tensor_by_name('input_x2_length:0')\n",
    "        self.input_y = self.graph.get_tensor_by_name('input_y:0')\n",
    "        self.keep_prob = self.graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "        self.scores = self.graph.get_tensor_by_name('scores:0')\n",
    "        self.predictions = self.graph.get_tensor_by_name('predictions:0')\n",
    "        self.loss = self.graph.get_tensor_by_name('Loss/loss:0')\n",
    "        self.accuracy = self.graph.get_tensor_by_name('Accuracy/accuracy:0')\n",
    "        self.global_step = self.graph.get_tensor_by_name('global_step:0')\n",
    "        self.train_op = self.graph.get_tensor_by_name('train_op:0')\n",
    "        \n",
    "        \n",
    "    def dist(self, x1, x2):\n",
    "        x1, x1_len = batch([x1])\n",
    "        x2, x2_len = batch([x2])\n",
    "        fd = {self.input_x1:x1, self.input_x1_length:x1_len,\n",
    "              self.input_x2:x2, self.input_x2_length:x2_len,\n",
    "              self.keep_prob:1.0}\n",
    "        conf = self.sess.run(self.scores, feed_dict=fd)\n",
    "        return 1-conf[0]\n",
    "    \n",
    "    def evaluate(self, doc_mix, doc_lbs, method='average', plot=True):\n",
    "        doc_mix_sq, _ = batch(doc_mix)\n",
    "        doc_mix_sq = doc_mix_sq.T\n",
    "        doc_mix_clust = linkage(doc_mix_sq, method=method, metric=self.dist)\n",
    "        # evaluate by class-based prec/rec/f1\n",
    "        doc_prd = fcluster(doc_mix_clust, 2, criterion='maxclust') - 1 # predicted assignments (label adjusted)\n",
    "        acc = clust_accuracy(doc_lbs, doc_prd)\n",
    "        if plot:\n",
    "            print('Clustering accuracy = {}'.format(acc))\n",
    "            print('\\n')\n",
    "            plt.figure(figsize=(25, 10))\n",
    "            plt.title('Hierarchical Clustering Dendrogram')\n",
    "            plt.xlabel('sample index')\n",
    "            plt.ylabel('distance')\n",
    "            dendrogram(\n",
    "                doc_mix_clust,\n",
    "                leaf_rotation=90.,  # rotates the x axis labels\n",
    "                leaf_font_size=15.,  # font size for the x axis labels\n",
    "            )\n",
    "            plt.show() \n",
    "            print('True | Pred | Sentence')\n",
    "            for label,pred_label,code in zip(doc_lbs,doc_prd,doc_mix):\n",
    "                if label==0:\n",
    "                    print('\\033[1;37;40m {}    | {}    | {}'.format(label,pred_label,to_sent(code)))\n",
    "                else:\n",
    "                    print('\\033[1;30;47m {}    | {}    | {}'.format(label,pred_label,to_sent(code)))\n",
    "            print('\\n' + Style.RESET_ALL) \n",
    "        else:\n",
    "            return doc_mix_clust, acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_dir = \"/work/04233/sw33286/AIDA-INDIV-MODEL-SAVE/our-model-no-context-mutual-attention/\"\n",
    "restore_filename = \"our-model-no-context-mutual-attention-00.meta\"\n",
    "clf_hac = ClfHAC(restore_dir, restore_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_hac.evaluate(*get_rand_mixture())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_evaluation(k=100):\n",
    "    accuracies = []\n",
    "    for _ in range(k):\n",
    "        _, acc = clf_hac.evaluate(*get_rand_mixture(), plot=False)\n",
    "        accuracies.append(acc)\n",
    "    print('Average clustering accuracy over {} samples = {}'.format(k, np.mean(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rand_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model Load-n-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100001, 300)\n"
     ]
    }
   ],
   "source": [
    "# Load facilities\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/work/04233/sw33286/AIDA-SCRIPTS\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell, MultiRNNCell, DropoutWrapper\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import colorama\n",
    "from colorama import Style\n",
    "\n",
    "from helpers import Indexer, batch\n",
    "from itertools import chain, product\n",
    "from collections import defaultdict\n",
    "\n",
    "# Link to NYT data folder\n",
    "\n",
    "nyt_code_dir = \"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_code/\"\n",
    "FILE_NAMES = os.listdir(nyt_code_dir)\n",
    "\n",
    "# Link to dictionary information\n",
    "\n",
    "info_path = \"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_info/indexer_word2emb_100k.p\"\n",
    "indexer100k, word2emb100k = dill.load(open(info_path, 'rb'))\n",
    "glove_embs = []\n",
    "for i in range(len(indexer100k)):\n",
    "    glove_embs.append(word2emb100k[indexer100k.get_object(i)])\n",
    "glove_embs = np.array(glove_embs)\n",
    "print(glove_embs.shape)\n",
    "\n",
    "# Data batching\n",
    "\n",
    "def batch_mixture(doc_a, doc_b, k=25):\n",
    "    batch_x1, batch_x2, batch_y = [], [], []\n",
    "    ys = [1,0,0,1]\n",
    "    for _ in range(k): # 4 entries added per iteration.\n",
    "        for i,(da,db) in enumerate(product([doc_a, doc_b], \n",
    "                                           [doc_a, doc_b])):\n",
    "            batch_x1.append(random.choice(da))\n",
    "            batch_x2.append(random.choice(db))\n",
    "            batch_y.append(ys[i])\n",
    "    return batch(batch_x1), batch(batch_x2), np.array(batch_y)\n",
    "\n",
    "def get_batch(file_idx):\n",
    "    filename = FILE_NAMES[file_idx]\n",
    "    doc_a, doc_b, doc_mix = dill.load(open(nyt_code_dir+filename, 'rb'))\n",
    "    (batch_x1,batch_x1_len), (batch_x2,batch_x2_len), batch_y = batch_mixture(doc_a,doc_b)\n",
    "    return batch_x1,batch_x1_len,batch_x2,batch_x2_len,batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph\n",
    "\n",
    "restore_dir = \"/work/04233/sw33286/AIDA-INDIV-MODEL-SAVE/our-model-no-context-mutual-attention/\"\n",
    "restore_filename = \"our-model-no-context-mutual-attention-00.meta\"\n",
    "\n",
    "sess = tf.Session()\n",
    "saver = tf.train.import_meta_graph(restore_dir + restore_filename)\n",
    "saver.restore(sess, tf.train.latest_checkpoint(restore_dir))\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "input_x1 = graph.get_tensor_by_name('input_x1:0')\n",
    "input_x2 = graph.get_tensor_by_name('input_x2:0')\n",
    "input_x1_length = graph.get_tensor_by_name('input_x1_length:0')\n",
    "input_x2_length = graph.get_tensor_by_name('input_x2_length:0')\n",
    "input_y = graph.get_tensor_by_name('input_y:0')\n",
    "keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "scores = graph.get_tensor_by_name('scores:0')\n",
    "predictions = graph.get_tensor_by_name('predictions:0')\n",
    "loss = graph.get_tensor_by_name('Loss/loss:0')\n",
    "accuracy = graph.get_tensor_by_name('Accuracy/accuracy:0')\n",
    "global_step = graph.get_tensor_by_name('global_step:0')\n",
    "train_op = graph.get_tensor_by_name('train_op:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "TRAIN_SIZE = 10\n",
    "# TRAIN_SIZE = len(FILE_NAMES)\n",
    "VERBOSE = 1\n",
    "ERROR_LOG = []\n",
    "\n",
    "loss_track, accuracy_track = [], []\n",
    "start = time.time()\n",
    "try:\n",
    "    for e in range(NUM_EPOCHS):\n",
    "        print('Epoch ', e+1)\n",
    "        print('\\n')\n",
    "        file_indices = np.random.choice(list(range(len(FILE_NAMES))), size=TRAIN_SIZE, replace=False)\n",
    "        random.shuffle(file_indices)  \n",
    "        curr_loss_track, curr_accuracy_track = [], []\n",
    "        for file_idx in file_indices:\n",
    "            try:\n",
    "                batch_x1,batch_x1_length,batch_x2,batch_x2_length,batch_y = get_batch(file_idx)\n",
    "            except:\n",
    "                ERROR_LOG.append(file_idx)\n",
    "                continue\n",
    "            fd = {input_x1:batch_x1, input_x1_length:batch_x1_length,\n",
    "                  input_x2:batch_x2, input_x2_length:batch_x2_length,\n",
    "                  input_y:batch_y,\n",
    "                  keep_prob:KEEP_PROB}   \n",
    "            _, step, loss_, accuracy_ = sess.run([train_op, global_step, loss, accuracy], feed_dict=fd)\n",
    "            curr_loss_track.append(loss_)\n",
    "            curr_accuracy_track.append(accuracy_)\n",
    "            if step%VERBOSE==0:\n",
    "                print('  batch loss & accuracy at step {}: <{}, {}> (time elapsed = {})'.format(step, \n",
    "                                                                                                np.mean(curr_loss_track),\n",
    "                                                                                                np.mean(curr_accuracy_track),\n",
    "                                                                                                time.time()-start))\n",
    "                start = time.time()\n",
    "        print('\\n')\n",
    "        print('  epoch mean loss & accuracy: <{}, {}>'.format(np.mean(curr_loss_track),np.mean(curr_accuracy_track)))\n",
    "        print('\\n')    \n",
    "        loss_track += curr_loss_track\n",
    "        accuracy_track += curr_accuracy_track \n",
    "except KeyboardInterrupt:\n",
    "    print('Stopped!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "\n",
    "def remove_all_files(target_dir):\n",
    "    for filename in os.listdir(target_dir):\n",
    "        os.remove(os.path.abspath(os.path.join(target_dir, filename)))\n",
    "\n",
    "save_dir = \"/work/04233/sw33286/AIDA-INDIV-MODEL-SAVE/our-model-no-context-mutual-attention/\"\n",
    "save_path = save_dir + \"our-model-no-context-mutual-attention-00\"\n",
    "remove_all_files(save_dir)\n",
    "saver.save(sess, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
