{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/work/04233/sw33286/AIDA-SCRIPTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell, MultiRNNCell, DropoutWrapper\n",
    "\n",
    "from helpers import Indexer, batch\n",
    "from itertools import chain, product\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARED_SIZE = 2 # size of noise (or, common vocab for all types).\n",
    "\n",
    "TYPES = ['ANIMAL','VEHICLE','NATURE','FURNITURE','FRUIT']\n",
    "SHARED_VOCAB = ['share'+str(i+1) for i in range(SHARED_SIZE)]\n",
    "TYPE2VOCAB = {'ANIMAL': ['cat','dog','pig','horse','deer']            + SHARED_VOCAB,\n",
    "              'VEHICLE': ['car','bike','motorcycle','train','bus']    + SHARED_VOCAB,\n",
    "              'NATURE': ['hill','mountain','lake','river','valley']   + SHARED_VOCAB,\n",
    "              'FURNITURE': ['stool','table','closet','cabinet','bed'] + SHARED_VOCAB,\n",
    "              'FRUIT': ['apple','pear','strawberry','grape','tomato'] + SHARED_VOCAB}\n",
    "VOCAB = list(chain.from_iterable(TYPE2VOCAB.values()))\n",
    "\n",
    "indexer = Indexer()\n",
    "indexer.get_index('PAD')\n",
    "for word in VOCAB:\n",
    "    indexer.get_index(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_LEN = 5\n",
    "SENT_FROM_LEN = 5\n",
    "SENT_TO_LEN = 15\n",
    "\n",
    "def to_sent(code):\n",
    "    return [indexer.get_object(idx) for idx in code]\n",
    "\n",
    "def get_rand_sent_code(sem_type, sent_len):\n",
    "    return [indexer.get_index(np.random.choice(TYPE2VOCAB[sem_type])) for _ in range(sent_len)]\n",
    "\n",
    "def get_mixture(type1, type2):\n",
    "    doc_a = [get_rand_sent_code(type1, np.random.randint(SENT_FROM_LEN, SENT_TO_LEN)) for _ in range(DOC_LEN)]\n",
    "    doc_b = [get_rand_sent_code(type2, np.random.randint(SENT_FROM_LEN, SENT_TO_LEN)) for _ in range(DOC_LEN)]\n",
    "    doc_mix = np.array(doc_a[:] + doc_b[:])\n",
    "    doc_lbs = np.array([0]*DOC_LEN + [1]*DOC_LEN)\n",
    "    indices = list(range(DOC_LEN*2))\n",
    "    random.shuffle(indices)\n",
    "    doc_mix = doc_mix[indices]\n",
    "    doc_lbs = doc_lbs[indices]\n",
    "    return doc_a, doc_b, doc_mix, doc_lbs\n",
    "    \n",
    "def batch_mixture(doc_a, doc_b, k):\n",
    "    batch_x1, batch_x2, batch_y = [], [], []\n",
    "    ys = [1,0,0,1]\n",
    "    for _ in range(k):\n",
    "        for i,(da,db) in enumerate(product([doc_a,doc_b],[doc_a,doc_b])):\n",
    "            batch_x1.append(random.choice(da))\n",
    "            batch_x2.append(random.choice(db))\n",
    "            batch_y.append(ys[i])\n",
    "    return batch(batch_x1), batch(batch_x2), np.array(batch_y)\n",
    "\n",
    "def get_batch(n=40):\n",
    "    if n%4!=0:\n",
    "        raise ValueError('The current generation scheme only supports multiples of 4 for batch size!')\n",
    "    type1, type2 = np.random.choice(TYPES, 2, replace=False)\n",
    "    doc_a, doc_b, _, _ = get_mixture(type1, type2) # document mixtures and labels aren't germane here.\n",
    "    (batch_x1,batch_x1_len), (batch_x2,batch_x2_len), batch_y = batch_mixture(doc_a,doc_b,n//4)\n",
    "    return batch_x1,batch_x1_len,batch_x2,batch_x2_len,batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "VOCAB_SIZE = len(indexer)\n",
    "EMB_SIZE = 20\n",
    "HID_SIZE = 10\n",
    "NUM_LAYERS = 2\n",
    "KEEP_PROB = 0.7\n",
    "\n",
    "input_x1 = tf.placeholder(tf.int32, [None, None], name='input_x1') # <max-time, batch-size>\n",
    "input_x2 = tf.placeholder(tf.int32, [None, None], name='input_x2')\n",
    "input_x1_length = tf.placeholder(tf.int32, [None], name='input_x1_length')\n",
    "input_x2_length = tf.placeholder(tf.int32, [None], name='input_x2_length')\n",
    "input_y  = tf.placeholder(tf.int32, [None], name='input_y')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "with tf.variable_scope('Embeddings'):\n",
    "    embeddings = tf.get_variable('embeddings', [VOCAB_SIZE, EMB_SIZE], \n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "    input_x1_embedded = tf.nn.embedding_lookup(embeddings, input_x1) # <max-time, batch-size, emb-size>\n",
    "    input_x2_embedded = tf.nn.embedding_lookup(embeddings, input_x2)\n",
    "\n",
    "cell = MultiRNNCell([DropoutWrapper(LSTMCell(HID_SIZE),output_keep_prob=keep_prob)]*NUM_LAYERS) \n",
    "\n",
    "def run_lstm(cell, inputs, inputs_length): # lstm-out size *= 2 by bidirectionality.\n",
    "    ((fw_outputs,bw_outputs), # <max-time, batch-size, hid-size>, attention later if needed.\n",
    "     (fw_final_state,bw_final_state)) = ( # <batch-size, hid-size>\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw=cell,\n",
    "                                        cell_bw=cell,\n",
    "                                        inputs=inputs,\n",
    "                                        sequence_length=inputs_length,\n",
    "                                        dtype=tf.float32, time_major=True)\n",
    "    )\n",
    "    return tf.concat([tf.concat([fw_state_tuple.h,bw_state_tuple.h], 1) # lstm-out size *= NUM_LAYERS by stacking.\n",
    "                      for fw_state_tuple,bw_state_tuple in zip(fw_final_state,bw_final_state)], 1)\n",
    "    \n",
    "with tf.variable_scope('Bi-LSTM') as scope:\n",
    "    final_state_x1 = run_lstm(cell, input_x1_embedded, input_x1_length)\n",
    "    scope.reuse_variables() # both sentence inputs share the same weights.\n",
    "    final_state_x2 = run_lstm(cell, input_x2_embedded, input_x2_length)\n",
    "    \n",
    "lstm_out_size = HID_SIZE * 2 * NUM_LAYERS\n",
    "W_bi = tf.get_variable('W_bi', [lstm_out_size, lstm_out_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "scores = tf.nn.sigmoid(tf.diag_part(tf.matmul(tf.matmul(final_state_x1,W_bi),tf.transpose(final_state_x2))),name='scores')\n",
    "predictions = tf.cast(tf.round(scores), tf.int32, name='predictions') \n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(input_y, tf.float32), logits=scores)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct_predictions = tf.equal(predictions, input_y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "    \n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(1e-5)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step, name='train_op')\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 100: <0.7246396541595459, 0.4749999940395355>\n",
      "  batch loss & accuracy at step 200: <0.7232811450958252, 0.675000011920929>\n",
      "  batch loss & accuracy at step 300: <0.7237483859062195, 0.5000000596046448>\n",
      "  batch loss & accuracy at step 400: <0.7239110469818115, 0.574999988079071>\n",
      "  batch loss & accuracy at step 500: <0.7233768105506897, 0.6499999761581421>\n",
      "  batch loss & accuracy at step 600: <0.7238601446151733, 0.550000011920929>\n",
      "  batch loss & accuracy at step 700: <0.7232406735420227, 0.6000000238418579>\n",
      "  batch loss & accuracy at step 800: <0.7238027453422546, 0.550000011920929>\n",
      "  batch loss & accuracy at step 900: <0.7217633128166199, 0.625>\n",
      "  batch loss & accuracy at step 1000: <0.7232192158699036, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.7235453128814697, 0.5474749803543091>\n",
      "\n",
      "\n",
      "Epoch  2\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 1100: <0.7230905294418335, 0.625>\n",
      "  batch loss & accuracy at step 1200: <0.723080039024353, 0.6500000357627869>\n",
      "  batch loss & accuracy at step 1300: <0.7222499251365662, 0.550000011920929>\n",
      "  batch loss & accuracy at step 1400: <0.7216138243675232, 0.5250000357627869>\n",
      "  batch loss & accuracy at step 1500: <0.7224993705749512, 0.550000011920929>\n",
      "  batch loss & accuracy at step 1600: <0.7224145531654358, 0.5000000596046448>\n",
      "  batch loss & accuracy at step 1700: <0.7201026082038879, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 1800: <0.7229353189468384, 0.5250000357627869>\n",
      "  batch loss & accuracy at step 1900: <0.7220171689987183, 0.44999998807907104>\n",
      "  batch loss & accuracy at step 2000: <0.7220671772956848, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.7222152352333069, 0.5507750511169434>\n",
      "\n",
      "\n",
      "Epoch  3\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 2100: <0.721225917339325, 0.5>\n",
      "  batch loss & accuracy at step 2200: <0.7198182940483093, 0.550000011920929>\n",
      "  batch loss & accuracy at step 2300: <0.7219752073287964, 0.5>\n",
      "  batch loss & accuracy at step 2400: <0.7180294990539551, 0.5>\n",
      "  batch loss & accuracy at step 2500: <0.7151561975479126, 0.6000000238418579>\n",
      "  batch loss & accuracy at step 2600: <0.7170983552932739, 0.5>\n",
      "  batch loss & accuracy at step 2700: <0.7194848656654358, 0.5>\n",
      "  batch loss & accuracy at step 2800: <0.7163814306259155, 0.5>\n",
      "  batch loss & accuracy at step 2900: <0.7149952054023743, 0.5>\n",
      "  batch loss & accuracy at step 3000: <0.7174056768417358, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.7181880474090576, 0.5124750137329102>\n",
      "\n",
      "\n",
      "Epoch  4\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 3100: <0.7118900418281555, 0.5>\n",
      "  batch loss & accuracy at step 3200: <0.7129426002502441, 0.5>\n",
      "  batch loss & accuracy at step 3300: <0.7144739031791687, 0.5>\n",
      "  batch loss & accuracy at step 3400: <0.7007726430892944, 0.5>\n",
      "  batch loss & accuracy at step 3500: <0.7024446725845337, 0.5>\n",
      "  batch loss & accuracy at step 3600: <0.7069973349571228, 0.5>\n",
      "  batch loss & accuracy at step 3700: <0.7048363089561462, 0.5>\n",
      "  batch loss & accuracy at step 3800: <0.7039519548416138, 0.5>\n",
      "  batch loss & accuracy at step 3900: <0.697847306728363, 0.5>\n",
      "  batch loss & accuracy at step 4000: <0.6953273415565491, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.7077280879020691, 0.5008249878883362>\n",
      "\n",
      "\n",
      "Epoch  5\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 4100: <0.7083224654197693, 0.5>\n",
      "  batch loss & accuracy at step 4200: <0.6981596946716309, 0.5>\n",
      "  batch loss & accuracy at step 4300: <0.6969338655471802, 0.5>\n",
      "  batch loss & accuracy at step 4400: <0.6899206042289734, 0.5>\n",
      "  batch loss & accuracy at step 4500: <0.6922069787979126, 0.5>\n",
      "  batch loss & accuracy at step 4600: <0.6966640949249268, 0.5>\n",
      "  batch loss & accuracy at step 4700: <0.6873674392700195, 0.5>\n",
      "  batch loss & accuracy at step 4800: <0.6925593018531799, 0.5>\n",
      "  batch loss & accuracy at step 4900: <0.699938178062439, 0.5>\n",
      "  batch loss & accuracy at step 5000: <0.6953999996185303, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6976016759872437, 0.5000249743461609>\n",
      "\n",
      "\n",
      "Epoch  6\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 5100: <0.6959978342056274, 0.5>\n",
      "  batch loss & accuracy at step 5200: <0.6873948574066162, 0.5>\n",
      "  batch loss & accuracy at step 5300: <0.6938985586166382, 0.5>\n",
      "  batch loss & accuracy at step 5400: <0.6868332624435425, 0.5>\n",
      "  batch loss & accuracy at step 5500: <0.6930291056632996, 0.5>\n",
      "  batch loss & accuracy at step 5600: <0.6925204396247864, 0.5>\n",
      "  batch loss & accuracy at step 5700: <0.6865074634552002, 0.5>\n",
      "  batch loss & accuracy at step 5800: <0.6963282227516174, 0.5>\n",
      "  batch loss & accuracy at step 5900: <0.6959317922592163, 0.5>\n",
      "  batch loss & accuracy at step 6000: <0.7003878951072693, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6939103603363037, 0.5>\n",
      "\n",
      "\n",
      "Epoch  7\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 6100: <0.7002966403961182, 0.5>\n",
      "  batch loss & accuracy at step 6200: <0.6992213129997253, 0.5>\n",
      "  batch loss & accuracy at step 6300: <0.6912098526954651, 0.5>\n",
      "  batch loss & accuracy at step 6400: <0.6937123537063599, 0.5>\n",
      "  batch loss & accuracy at step 6500: <0.6944653987884521, 0.5>\n",
      "  batch loss & accuracy at step 6600: <0.6978166103363037, 0.5>\n",
      "  batch loss & accuracy at step 6700: <0.6930661201477051, 0.5>\n",
      "  batch loss & accuracy at step 6800: <0.70881187915802, 0.5>\n",
      "  batch loss & accuracy at step 6900: <0.6981776356697083, 0.5>\n",
      "  batch loss & accuracy at step 7000: <0.692497968673706, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6924776434898376, 0.5>\n",
      "\n",
      "\n",
      "Epoch  8\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 7100: <0.6952162384986877, 0.5>\n",
      "  batch loss & accuracy at step 7200: <0.6845797300338745, 0.5>\n",
      "  batch loss & accuracy at step 7300: <0.6962435245513916, 0.5>\n",
      "  batch loss & accuracy at step 7400: <0.6940869688987732, 0.5>\n",
      "  batch loss & accuracy at step 7500: <0.6856652498245239, 0.5>\n",
      "  batch loss & accuracy at step 7600: <0.698952317237854, 0.5>\n",
      "  batch loss & accuracy at step 7700: <0.6850398778915405, 0.5>\n",
      "  batch loss & accuracy at step 7800: <0.6930765509605408, 0.5>\n",
      "  batch loss & accuracy at step 7900: <0.6944405436515808, 0.5>\n",
      "  batch loss & accuracy at step 8000: <0.6936428546905518, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6917068958282471, 0.5002750158309937>\n",
      "\n",
      "\n",
      "Epoch  9\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 8100: <0.6990960836410522, 0.5>\n",
      "  batch loss & accuracy at step 8200: <0.6921777725219727, 0.5>\n",
      "  batch loss & accuracy at step 8300: <0.6973694562911987, 0.5>\n",
      "  batch loss & accuracy at step 8400: <0.6826834082603455, 0.5>\n",
      "  batch loss & accuracy at step 8500: <0.6863265037536621, 0.5>\n",
      "  batch loss & accuracy at step 8600: <0.6851152181625366, 0.5>\n",
      "  batch loss & accuracy at step 8700: <0.6963352560997009, 0.5>\n",
      "  batch loss & accuracy at step 8800: <0.6925150156021118, 0.5>\n",
      "  batch loss & accuracy at step 8900: <0.6903291940689087, 0.5>\n",
      "  batch loss & accuracy at step 9000: <0.668296217918396, 0.625>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.68799889087677, 0.5087500214576721>\n",
      "\n",
      "\n",
      "Epoch  10\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 9100: <0.6935272216796875, 0.5>\n",
      "  batch loss & accuracy at step 9200: <0.6946296691894531, 0.5>\n",
      "  batch loss & accuracy at step 9300: <0.6958914995193481, 0.5>\n",
      "  batch loss & accuracy at step 9400: <0.696834921836853, 0.5>\n",
      "  batch loss & accuracy at step 9500: <0.6929196119308472, 0.5>\n",
      "  batch loss & accuracy at step 9600: <0.7006739377975464, 0.5>\n",
      "  batch loss & accuracy at step 9700: <0.6901980638504028, 0.5>\n",
      "  batch loss & accuracy at step 9800: <0.6418285369873047, 0.7000000476837158>\n",
      "  batch loss & accuracy at step 9900: <0.6902064085006714, 0.5>\n",
      "  batch loss & accuracy at step 10000: <0.6915212869644165, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6801372170448303, 0.5536500215530396>\n",
      "\n",
      "\n",
      "Epoch  11\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 10100: <0.6936438679695129, 0.5>\n",
      "  batch loss & accuracy at step 10200: <0.6936324834823608, 0.5>\n",
      "  batch loss & accuracy at step 10300: <0.6899307370185852, 0.5>\n",
      "  batch loss & accuracy at step 10400: <0.6900365352630615, 0.5>\n",
      "  batch loss & accuracy at step 10500: <0.6909913420677185, 0.5>\n",
      "  batch loss & accuracy at step 10600: <0.700323760509491, 0.5>\n",
      "  batch loss & accuracy at step 10700: <0.6364572048187256, 0.75>\n",
      "  batch loss & accuracy at step 10800: <0.6932682991027832, 0.5>\n",
      "  batch loss & accuracy at step 10900: <0.6850668787956238, 0.5>\n",
      "  batch loss & accuracy at step 11000: <0.6885374188423157, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6733521223068237, 0.5759000182151794>\n",
      "\n",
      "\n",
      "Epoch  12\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch loss & accuracy at step 11100: <0.6969945430755615, 0.5>\n",
      "  batch loss & accuracy at step 11200: <0.6919843554496765, 0.5>\n",
      "  batch loss & accuracy at step 11300: <0.652456521987915, 0.7000000476837158>\n",
      "  batch loss & accuracy at step 11400: <0.6914494037628174, 0.5>\n",
      "  batch loss & accuracy at step 11500: <0.634476363658905, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 11600: <0.6949870586395264, 0.5>\n",
      "  batch loss & accuracy at step 11700: <0.6203379034996033, 0.75>\n",
      "  batch loss & accuracy at step 11800: <0.6508866548538208, 0.675000011920929>\n",
      "  batch loss & accuracy at step 11900: <0.6812433004379272, 0.5>\n",
      "  batch loss & accuracy at step 12000: <0.6260727047920227, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6663870811462402, 0.5870250463485718>\n",
      "\n",
      "\n",
      "Epoch  13\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 12100: <0.605585515499115, 0.75>\n",
      "  batch loss & accuracy at step 12200: <0.6028497815132141, 0.75>\n",
      "  batch loss & accuracy at step 12300: <0.6093348860740662, 0.75>\n",
      "  batch loss & accuracy at step 12400: <0.6132618188858032, 0.75>\n",
      "  batch loss & accuracy at step 12500: <0.6966392397880554, 0.5>\n",
      "  batch loss & accuracy at step 12600: <0.6292623281478882, 0.7000000476837158>\n",
      "  batch loss & accuracy at step 12700: <0.6349680423736572, 0.625>\n",
      "  batch loss & accuracy at step 12800: <0.6908578872680664, 0.5>\n",
      "  batch loss & accuracy at step 12900: <0.6182945966720581, 0.75>\n",
      "  batch loss & accuracy at step 13000: <0.6922773718833923, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6594699621200562, 0.599299967288971>\n",
      "\n",
      "\n",
      "Epoch  14\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 13100: <0.605339527130127, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 13200: <0.6310181021690369, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 13300: <0.6718229651451111, 0.5250000357627869>\n",
      "  batch loss & accuracy at step 13400: <0.6716251969337463, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 13500: <0.6932169795036316, 0.5>\n",
      "  batch loss & accuracy at step 13600: <0.6858118772506714, 0.5>\n",
      "  batch loss & accuracy at step 13700: <0.6855975389480591, 0.625>\n",
      "  batch loss & accuracy at step 13800: <0.7127202749252319, 0.550000011920929>\n",
      "  batch loss & accuracy at step 13900: <0.7227985858917236, 0.45000001788139343>\n",
      "  batch loss & accuracy at step 14000: <0.550886869430542, 0.8500000238418579>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6503305435180664, 0.6294749975204468>\n",
      "\n",
      "\n",
      "Epoch  15\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 14100: <0.6004212498664856, 0.75>\n",
      "  batch loss & accuracy at step 14200: <0.5858293771743774, 0.75>\n",
      "  batch loss & accuracy at step 14300: <0.6367461681365967, 0.75>\n",
      "  batch loss & accuracy at step 14400: <0.6112984418869019, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 14500: <0.7012156248092651, 0.5>\n",
      "  batch loss & accuracy at step 14600: <0.5390043258666992, 0.8750000596046448>\n",
      "  batch loss & accuracy at step 14700: <0.5676981806755066, 0.824999988079071>\n",
      "  batch loss & accuracy at step 14800: <0.6891621947288513, 0.5>\n",
      "  batch loss & accuracy at step 14900: <0.6952807903289795, 0.5>\n",
      "  batch loss & accuracy at step 15000: <0.5925672054290771, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6496709585189819, 0.636275053024292>\n",
      "\n",
      "\n",
      "Epoch  16\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 15100: <0.6936848163604736, 0.5250000357627869>\n",
      "  batch loss & accuracy at step 15200: <0.5835227966308594, 0.75>\n",
      "  batch loss & accuracy at step 15300: <0.6319412589073181, 0.625>\n",
      "  batch loss & accuracy at step 15400: <0.5873593688011169, 0.75>\n",
      "  batch loss & accuracy at step 15500: <0.5955325961112976, 0.75>\n",
      "  batch loss & accuracy at step 15600: <0.6936286091804504, 0.5>\n",
      "  batch loss & accuracy at step 15700: <0.6914842128753662, 0.5>\n",
      "  batch loss & accuracy at step 15800: <0.6590603590011597, 0.6000000238418579>\n",
      "  batch loss & accuracy at step 15900: <0.7004880309104919, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 16000: <0.6903306245803833, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6463426947593689, 0.6472249627113342>\n",
      "\n",
      "\n",
      "Epoch  17\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 16100: <0.6943936944007874, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 16200: <0.6867811679840088, 0.5250000357627869>\n",
      "  batch loss & accuracy at step 16300: <0.671501874923706, 0.550000011920929>\n",
      "  batch loss & accuracy at step 16400: <0.6849957704544067, 0.5>\n",
      "  batch loss & accuracy at step 16500: <0.6613447070121765, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 16600: <0.6849762201309204, 0.7000000476837158>\n",
      "  batch loss & accuracy at step 16700: <0.6155392527580261, 0.75>\n",
      "  batch loss & accuracy at step 16800: <0.6929823756217957, 0.5>\n",
      "  batch loss & accuracy at step 16900: <0.6911824941635132, 0.5>\n",
      "  batch loss & accuracy at step 17000: <0.6869148015975952, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6435558199882507, 0.6525999903678894>\n",
      "\n",
      "\n",
      "Epoch  18\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 17100: <0.6851383447647095, 0.5>\n",
      "  batch loss & accuracy at step 17200: <0.5446180701255798, 0.8750000596046448>\n",
      "  batch loss & accuracy at step 17300: <0.690305233001709, 0.5>\n",
      "  batch loss & accuracy at step 17400: <0.6959919929504395, 0.6000000238418579>\n",
      "  batch loss & accuracy at step 17500: <0.693234920501709, 0.5>\n",
      "  batch loss & accuracy at step 17600: <0.6493443846702576, 0.6499999761581421>\n",
      "  batch loss & accuracy at step 17700: <0.6900140047073364, 0.5>\n",
      "  batch loss & accuracy at step 17800: <0.6029552221298218, 0.75>\n",
      "  batch loss & accuracy at step 17900: <0.6567827463150024, 0.6499999761581421>\n",
      "  batch loss & accuracy at step 18000: <0.6858306527137756, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6458319425582886, 0.6440999507904053>\n",
      "\n",
      "\n",
      "Epoch  19\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 18100: <0.6810055375099182, 0.5>\n",
      "  batch loss & accuracy at step 18200: <0.687507152557373, 0.5>\n",
      "  batch loss & accuracy at step 18300: <0.6906886100769043, 0.5>\n",
      "  batch loss & accuracy at step 18400: <0.5584350824356079, 0.8500000238418579>\n",
      "  batch loss & accuracy at step 18500: <0.5929064750671387, 0.75>\n",
      "  batch loss & accuracy at step 18600: <0.6925287246704102, 0.5>\n",
      "  batch loss & accuracy at step 18700: <0.5926682949066162, 0.75>\n",
      "  batch loss & accuracy at step 18800: <0.6723113059997559, 0.6000000238418579>\n",
      "  batch loss & accuracy at step 18900: <0.6834392547607422, 0.5>\n",
      "  batch loss & accuracy at step 19000: <0.6887750625610352, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6433963179588318, 0.6518999338150024>\n",
      "\n",
      "\n",
      "Epoch  20\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 19100: <0.5886009931564331, 0.75>\n",
      "  batch loss & accuracy at step 19200: <0.655450165271759, 0.675000011920929>\n",
      "  batch loss & accuracy at step 19300: <0.6927741765975952, 0.5>\n",
      "  batch loss & accuracy at step 19400: <0.6292742490768433, 0.675000011920929>\n",
      "  batch loss & accuracy at step 19500: <0.6020166873931885, 0.75>\n",
      "  batch loss & accuracy at step 19600: <0.5401429533958435, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 19700: <0.6937898397445679, 0.5>\n",
      "  batch loss & accuracy at step 19800: <0.5935923457145691, 0.75>\n",
      "  batch loss & accuracy at step 19900: <0.648705244064331, 0.625>\n",
      "  batch loss & accuracy at step 20000: <0.6519471406936646, 0.625>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6388303637504578, 0.6619499921798706>\n",
      "\n",
      "\n",
      "Epoch  21\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 20100: <0.6907410621643066, 0.5>\n",
      "  batch loss & accuracy at step 20200: <0.6167038083076477, 0.75>\n",
      "  batch loss & accuracy at step 20300: <0.6534396409988403, 0.675000011920929>\n",
      "  batch loss & accuracy at step 20400: <0.6244434714317322, 0.75>\n",
      "  batch loss & accuracy at step 20500: <0.6005058288574219, 0.75>\n",
      "  batch loss & accuracy at step 20600: <0.671808660030365, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 20700: <0.547262966632843, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 20800: <0.5915472507476807, 0.75>\n",
      "  batch loss & accuracy at step 20900: <0.6628134846687317, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 21000: <0.6574414372444153, 0.6749999523162842>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6339975595474243, 0.674875020980835>\n",
      "\n",
      "\n",
      "Epoch  22\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 21100: <0.6114680767059326, 0.75>\n",
      "  batch loss & accuracy at step 21200: <0.6047253608703613, 0.75>\n",
      "  batch loss & accuracy at step 21300: <0.6480061411857605, 0.625>\n",
      "  batch loss & accuracy at step 21400: <0.6135573387145996, 0.75>\n",
      "  batch loss & accuracy at step 21500: <0.6905957460403442, 0.5>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch loss & accuracy at step 21600: <0.5663388967514038, 0.8250000476837158>\n",
      "  batch loss & accuracy at step 21700: <0.6891533732414246, 0.5>\n",
      "  batch loss & accuracy at step 21800: <0.5999205708503723, 0.75>\n",
      "  batch loss & accuracy at step 21900: <0.5925284624099731, 0.75>\n",
      "  batch loss & accuracy at step 22000: <0.5985943078994751, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.630842387676239, 0.6814749836921692>\n",
      "\n",
      "\n",
      "Epoch  23\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 22100: <0.596637487411499, 0.75>\n",
      "  batch loss & accuracy at step 22200: <0.6040037274360657, 0.75>\n",
      "  batch loss & accuracy at step 22300: <0.6878870725631714, 0.5>\n",
      "  batch loss & accuracy at step 22400: <0.6144274473190308, 0.75>\n",
      "  batch loss & accuracy at step 22500: <0.5943541526794434, 0.75>\n",
      "  batch loss & accuracy at step 22600: <0.6042606830596924, 0.75>\n",
      "  batch loss & accuracy at step 22700: <0.6896741390228271, 0.5>\n",
      "  batch loss & accuracy at step 22800: <0.6044119596481323, 0.75>\n",
      "  batch loss & accuracy at step 22900: <0.6054353713989258, 0.75>\n",
      "  batch loss & accuracy at step 23000: <0.5278382897377014, 0.925000011920929>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6296679973602295, 0.6802499890327454>\n",
      "\n",
      "\n",
      "Epoch  24\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 23100: <0.6963062286376953, 0.5>\n",
      "  batch loss & accuracy at step 23200: <0.6037702560424805, 0.75>\n",
      "  batch loss & accuracy at step 23300: <0.6937669515609741, 0.5>\n",
      "  batch loss & accuracy at step 23400: <0.6871124505996704, 0.5>\n",
      "  batch loss & accuracy at step 23500: <0.6042781472206116, 0.75>\n",
      "  batch loss & accuracy at step 23600: <0.6946969628334045, 0.5>\n",
      "  batch loss & accuracy at step 23700: <0.6908793449401855, 0.5>\n",
      "  batch loss & accuracy at step 23800: <0.5987077355384827, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 23900: <0.6167755722999573, 0.75>\n",
      "  batch loss & accuracy at step 24000: <0.5977540612220764, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.627328097820282, 0.6844000220298767>\n",
      "\n",
      "\n",
      "Epoch  25\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 24100: <0.5184826254844666, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 24200: <0.685003399848938, 0.5>\n",
      "  batch loss & accuracy at step 24300: <0.616549015045166, 0.675000011920929>\n",
      "  batch loss & accuracy at step 24400: <0.5839556455612183, 0.75>\n",
      "  batch loss & accuracy at step 24500: <0.5846027135848999, 0.75>\n",
      "  batch loss & accuracy at step 24600: <0.6787588596343994, 0.5>\n",
      "  batch loss & accuracy at step 24700: <0.6793886423110962, 0.5>\n",
      "  batch loss & accuracy at step 24800: <0.6186015605926514, 0.75>\n",
      "  batch loss & accuracy at step 24900: <0.5937212109565735, 0.75>\n",
      "  batch loss & accuracy at step 25000: <0.5050205588340759, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6213123202323914, 0.6926999688148499>\n",
      "\n",
      "\n",
      "Epoch  26\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 25100: <0.5952236652374268, 0.7749999761581421>\n",
      "  batch loss & accuracy at step 25200: <0.6925721764564514, 0.5>\n",
      "  batch loss & accuracy at step 25300: <0.6128636598587036, 0.75>\n",
      "  batch loss & accuracy at step 25400: <0.6568034887313843, 0.6499999761581421>\n",
      "  batch loss & accuracy at step 25500: <0.6000917553901672, 0.75>\n",
      "  batch loss & accuracy at step 25600: <0.6907947659492493, 0.5>\n",
      "  batch loss & accuracy at step 25700: <0.5712354779243469, 0.7999999523162842>\n",
      "  batch loss & accuracy at step 25800: <0.5808597207069397, 0.7999999523162842>\n",
      "  batch loss & accuracy at step 25900: <0.594366192817688, 0.7750000357627869>\n",
      "  batch loss & accuracy at step 26000: <0.6185575723648071, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6160218715667725, 0.7176499962806702>\n",
      "\n",
      "\n",
      "Epoch  27\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 26100: <0.6047036647796631, 0.75>\n",
      "  batch loss & accuracy at step 26200: <0.573449969291687, 0.949999988079071>\n",
      "  batch loss & accuracy at step 26300: <0.5054855346679688, 1.0>\n",
      "  batch loss & accuracy at step 26400: <0.6870973110198975, 0.5>\n",
      "  batch loss & accuracy at step 26500: <0.6583749651908875, 0.550000011920929>\n",
      "  batch loss & accuracy at step 26600: <0.6946563124656677, 0.5>\n",
      "  batch loss & accuracy at step 26700: <0.5216336250305176, 0.949999988079071>\n",
      "  batch loss & accuracy at step 26800: <0.6049180030822754, 0.75>\n",
      "  batch loss & accuracy at step 26900: <0.5040246248245239, 1.0>\n",
      "  batch loss & accuracy at step 27000: <0.6643624901771545, 0.699999988079071>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6081513166427612, 0.7588499784469604>\n",
      "\n",
      "\n",
      "Epoch  28\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 27100: <0.5999759435653687, 0.925000011920929>\n",
      "  batch loss & accuracy at step 27200: <0.6700244545936584, 0.7000000476837158>\n",
      "  batch loss & accuracy at step 27300: <0.5649915933609009, 0.875>\n",
      "  batch loss & accuracy at step 27400: <0.5353306531906128, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 27500: <0.5037370920181274, 1.0>\n",
      "  batch loss & accuracy at step 27600: <0.587587296962738, 0.75>\n",
      "  batch loss & accuracy at step 27700: <0.612535834312439, 0.75>\n",
      "  batch loss & accuracy at step 27800: <0.5386186838150024, 0.949999988079071>\n",
      "  batch loss & accuracy at step 27900: <0.5370926260948181, 0.949999988079071>\n",
      "  batch loss & accuracy at step 28000: <0.6182769536972046, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6032191514968872, 0.774399995803833>\n",
      "\n",
      "\n",
      "Epoch  29\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 28100: <0.5457322001457214, 0.9750000238418579>\n",
      "  batch loss & accuracy at step 28200: <0.615664005279541, 0.75>\n",
      "  batch loss & accuracy at step 28300: <0.6182247400283813, 0.75>\n",
      "  batch loss & accuracy at step 28400: <0.5888724327087402, 0.75>\n",
      "  batch loss & accuracy at step 28500: <0.503269374370575, 1.0>\n",
      "  batch loss & accuracy at step 28600: <0.6876544952392578, 0.5>\n",
      "  batch loss & accuracy at step 28700: <0.5364978909492493, 0.949999988079071>\n",
      "  batch loss & accuracy at step 28800: <0.6543923616409302, 0.675000011920929>\n",
      "  batch loss & accuracy at step 28900: <0.5300361514091492, 0.9750000238418579>\n",
      "  batch loss & accuracy at step 29000: <0.6225515604019165, 0.699999988079071>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.6005536913871765, 0.7801250219345093>\n",
      "\n",
      "\n",
      "Epoch  30\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 29100: <0.6500231623649597, 0.75>\n",
      "  batch loss & accuracy at step 29200: <0.612846851348877, 0.699999988079071>\n",
      "  batch loss & accuracy at step 29300: <0.6141029596328735, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 29400: <0.5034007430076599, 1.0>\n",
      "  batch loss & accuracy at step 29500: <0.5034359693527222, 1.0>\n",
      "  batch loss & accuracy at step 29600: <0.6144644618034363, 0.75>\n",
      "  batch loss & accuracy at step 29700: <0.5074930191040039, 1.0>\n",
      "  batch loss & accuracy at step 29800: <0.631639301776886, 0.75>\n",
      "  batch loss & accuracy at step 29900: <0.5058744549751282, 1.0>\n",
      "  batch loss & accuracy at step 30000: <0.6922737956047058, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5980625152587891, 0.7787250280380249>\n",
      "\n",
      "\n",
      "Epoch  31\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 30100: <0.5987519025802612, 0.75>\n",
      "  batch loss & accuracy at step 30200: <0.641972541809082, 0.6000000238418579>\n",
      "  batch loss & accuracy at step 30300: <0.6928385496139526, 0.5>\n",
      "  batch loss & accuracy at step 30400: <0.598149299621582, 0.75>\n",
      "  batch loss & accuracy at step 30500: <0.6081398725509644, 0.75>\n",
      "  batch loss & accuracy at step 30600: <0.5989642143249512, 0.75>\n",
      "  batch loss & accuracy at step 30700: <0.6471278667449951, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 30800: <0.5438884496688843, 0.949999988079071>\n",
      "  batch loss & accuracy at step 30900: <0.6053555011749268, 0.75>\n",
      "  batch loss & accuracy at step 31000: <0.6915906071662903, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.596035897731781, 0.7807499766349792>\n",
      "\n",
      "\n",
      "Epoch  32\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 31100: <0.6126722097396851, 0.7000000476837158>\n",
      "  batch loss & accuracy at step 31200: <0.5042277574539185, 1.0>\n",
      "  batch loss & accuracy at step 31300: <0.6224124431610107, 0.699999988079071>\n",
      "  batch loss & accuracy at step 31400: <0.6202151775360107, 0.75>\n",
      "  batch loss & accuracy at step 31500: <0.5984053611755371, 0.75>\n",
      "  batch loss & accuracy at step 31600: <0.5475515127182007, 0.925000011920929>\n",
      "  batch loss & accuracy at step 31700: <0.5917576551437378, 0.8500000238418579>\n",
      "  batch loss & accuracy at step 31800: <0.5033794045448303, 1.0>\n",
      "  batch loss & accuracy at step 31900: <0.5033537745475769, 1.0>\n",
      "  batch loss & accuracy at step 32000: <0.5404744148254395, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5913613438606262, 0.7888250350952148>\n",
      "\n",
      "\n",
      "Epoch  33\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch loss & accuracy at step 32100: <0.5159870386123657, 1.0>\n",
      "  batch loss & accuracy at step 32200: <0.6049248576164246, 0.75>\n",
      "  batch loss & accuracy at step 32300: <0.5977310538291931, 0.75>\n",
      "  batch loss & accuracy at step 32400: <0.6303402185440063, 0.6499999761581421>\n",
      "  batch loss & accuracy at step 32500: <0.6011765003204346, 0.75>\n",
      "  batch loss & accuracy at step 32600: <0.6167826652526855, 0.75>\n",
      "  batch loss & accuracy at step 32700: <0.5047307014465332, 1.0>\n",
      "  batch loss & accuracy at step 32800: <0.5051450133323669, 1.0>\n",
      "  batch loss & accuracy at step 32900: <0.6135790348052979, 0.75>\n",
      "  batch loss & accuracy at step 33000: <0.5308746099472046, 0.9500000476837158>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5879615545272827, 0.7956500053405762>\n",
      "\n",
      "\n",
      "Epoch  34\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 33100: <0.6349431276321411, 0.625>\n",
      "  batch loss & accuracy at step 33200: <0.601086437702179, 0.75>\n",
      "  batch loss & accuracy at step 33300: <0.5200091004371643, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 33400: <0.598706066608429, 0.75>\n",
      "  batch loss & accuracy at step 33500: <0.5216259360313416, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 33600: <0.5198398232460022, 1.0>\n",
      "  batch loss & accuracy at step 33700: <0.6931222677230835, 0.5>\n",
      "  batch loss & accuracy at step 33800: <0.5033110976219177, 1.0>\n",
      "  batch loss & accuracy at step 33900: <0.6009603142738342, 0.75>\n",
      "  batch loss & accuracy at step 34000: <0.5992884635925293, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.591256320476532, 0.7820000052452087>\n",
      "\n",
      "\n",
      "Epoch  35\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 34100: <0.5951015949249268, 0.75>\n",
      "  batch loss & accuracy at step 34200: <0.6137384176254272, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 34300: <0.6000186204910278, 0.75>\n",
      "  batch loss & accuracy at step 34400: <0.6084741353988647, 0.75>\n",
      "  batch loss & accuracy at step 34500: <0.5032352209091187, 1.0>\n",
      "  batch loss & accuracy at step 34600: <0.5047413110733032, 1.0>\n",
      "  batch loss & accuracy at step 34700: <0.5980328321456909, 0.75>\n",
      "  batch loss & accuracy at step 34800: <0.5364105105400085, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 34900: <0.5033762454986572, 1.0>\n",
      "  batch loss & accuracy at step 35000: <0.5199546813964844, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5870180726051331, 0.7935000061988831>\n",
      "\n",
      "\n",
      "Epoch  36\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 35100: <0.5298787355422974, 0.925000011920929>\n",
      "  batch loss & accuracy at step 35200: <0.6093923449516296, 0.75>\n",
      "  batch loss & accuracy at step 35300: <0.6002939939498901, 0.75>\n",
      "  batch loss & accuracy at step 35400: <0.6007237434387207, 0.75>\n",
      "  batch loss & accuracy at step 35500: <0.5988813638687134, 0.75>\n",
      "  batch loss & accuracy at step 35600: <0.6024163365364075, 0.75>\n",
      "  batch loss & accuracy at step 35700: <0.6076595783233643, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 35800: <0.5032939910888672, 1.0>\n",
      "  batch loss & accuracy at step 35900: <0.5098851919174194, 1.0>\n",
      "  batch loss & accuracy at step 36000: <0.599535346031189, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5871812105178833, 0.7904000282287598>\n",
      "\n",
      "\n",
      "Epoch  37\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 36100: <0.5174684524536133, 1.0>\n",
      "  batch loss & accuracy at step 36200: <0.5183287858963013, 1.0>\n",
      "  batch loss & accuracy at step 36300: <0.5981970429420471, 0.75>\n",
      "  batch loss & accuracy at step 36400: <0.5322455167770386, 0.925000011920929>\n",
      "  batch loss & accuracy at step 36500: <0.5985058546066284, 0.75>\n",
      "  batch loss & accuracy at step 36600: <0.5032446980476379, 1.0>\n",
      "  batch loss & accuracy at step 36700: <0.5257025957107544, 0.925000011920929>\n",
      "  batch loss & accuracy at step 36800: <0.6923830509185791, 0.5>\n",
      "  batch loss & accuracy at step 36900: <0.602298378944397, 0.75>\n",
      "  batch loss & accuracy at step 37000: <0.6931267976760864, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5819932222366333, 0.8018499612808228>\n",
      "\n",
      "\n",
      "Epoch  38\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 37100: <0.5290734171867371, 1.0>\n",
      "  batch loss & accuracy at step 37200: <0.6933203935623169, 0.5>\n",
      "  batch loss & accuracy at step 37300: <0.5994994640350342, 0.75>\n",
      "  batch loss & accuracy at step 37400: <0.6931473612785339, 0.5>\n",
      "  batch loss & accuracy at step 37500: <0.5032987594604492, 1.0>\n",
      "  batch loss & accuracy at step 37600: <0.692638099193573, 0.5>\n",
      "  batch loss & accuracy at step 37700: <0.5989811420440674, 0.75>\n",
      "  batch loss & accuracy at step 37800: <0.6033637523651123, 0.75>\n",
      "  batch loss & accuracy at step 37900: <0.5196414589881897, 1.0>\n",
      "  batch loss & accuracy at step 38000: <0.5986336469650269, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.582664430141449, 0.7990000247955322>\n",
      "\n",
      "\n",
      "Epoch  39\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 38100: <0.5145844221115112, 1.0>\n",
      "  batch loss & accuracy at step 38200: <0.5051904916763306, 1.0>\n",
      "  batch loss & accuracy at step 38300: <0.5984613299369812, 0.75>\n",
      "  batch loss & accuracy at step 38400: <0.6003209352493286, 0.75>\n",
      "  batch loss & accuracy at step 38500: <0.5991807579994202, 0.75>\n",
      "  batch loss & accuracy at step 38600: <0.5407890677452087, 0.875>\n",
      "  batch loss & accuracy at step 38700: <0.693658709526062, 0.5>\n",
      "  batch loss & accuracy at step 38800: <0.6931484937667847, 0.5>\n",
      "  batch loss & accuracy at step 38900: <0.6040946245193481, 0.75>\n",
      "  batch loss & accuracy at step 39000: <0.5276463627815247, 0.925000011920929>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5847320556640625, 0.792574942111969>\n",
      "\n",
      "\n",
      "Epoch  40\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 39100: <0.5177924036979675, 1.0>\n",
      "  batch loss & accuracy at step 39200: <0.6931464076042175, 0.5>\n",
      "  batch loss & accuracy at step 39300: <0.6153090000152588, 0.75>\n",
      "  batch loss & accuracy at step 39400: <0.5124514102935791, 1.0>\n",
      "  batch loss & accuracy at step 39500: <0.5085099339485168, 1.0>\n",
      "  batch loss & accuracy at step 39600: <0.5982708930969238, 0.75>\n",
      "  batch loss & accuracy at step 39700: <0.6931709051132202, 0.5>\n",
      "  batch loss & accuracy at step 39800: <0.6111594438552856, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 39900: <0.5043511390686035, 1.0>\n",
      "  batch loss & accuracy at step 40000: <0.598324716091156, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5826240181922913, 0.7966750264167786>\n",
      "\n",
      "\n",
      "Epoch  41\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 40100: <0.5990060567855835, 0.75>\n",
      "  batch loss & accuracy at step 40200: <0.5983373522758484, 0.75>\n",
      "  batch loss & accuracy at step 40300: <0.5982809066772461, 0.75>\n",
      "  batch loss & accuracy at step 40400: <0.5078529119491577, 1.0>\n",
      "  batch loss & accuracy at step 40500: <0.598190426826477, 0.75>\n",
      "  batch loss & accuracy at step 40600: <0.6034675240516663, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 40700: <0.5983275175094604, 0.75>\n",
      "  batch loss & accuracy at step 40800: <0.5988131761550903, 0.75>\n",
      "  batch loss & accuracy at step 40900: <0.5997475385665894, 0.75>\n",
      "  batch loss & accuracy at step 41000: <0.5976758003234863, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5808046460151672, 0.800724983215332>\n",
      "\n",
      "\n",
      "Epoch  42\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 41100: <0.5033434629440308, 1.0>\n",
      "  batch loss & accuracy at step 41200: <0.6228624582290649, 0.6749999523162842>\n",
      "  batch loss & accuracy at step 41300: <0.5982114672660828, 0.75>\n",
      "  batch loss & accuracy at step 41400: <0.6934562921524048, 0.5>\n",
      "  batch loss & accuracy at step 41500: <0.5993930697441101, 0.75>\n",
      "  batch loss & accuracy at step 41600: <0.6929246783256531, 0.5>\n",
      "  batch loss & accuracy at step 41700: <0.5059945583343506, 1.0>\n",
      "  batch loss & accuracy at step 41800: <0.598624050617218, 0.75>\n",
      "  batch loss & accuracy at step 41900: <0.6156740784645081, 0.75>\n",
      "  batch loss & accuracy at step 42000: <0.5068645477294922, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5792561173439026, 0.8040750026702881>\n",
      "\n",
      "\n",
      "Epoch  43\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 42100: <0.5039088129997253, 1.0>\n",
      "  batch loss & accuracy at step 42200: <0.5034903287887573, 1.0>\n",
      "  batch loss & accuracy at step 42300: <0.59864741563797, 0.75>\n",
      "  batch loss & accuracy at step 42400: <0.5032305121421814, 1.0>\n",
      "  batch loss & accuracy at step 42500: <0.5983617305755615, 0.75>\n",
      "  batch loss & accuracy at step 42600: <0.5060390830039978, 1.0>\n",
      "  batch loss & accuracy at step 42700: <0.600727379322052, 0.75>\n",
      "  batch loss & accuracy at step 42800: <0.6024967432022095, 0.75>\n",
      "  batch loss & accuracy at step 42900: <0.6933695077896118, 0.5>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch loss & accuracy at step 43000: <0.5038705468177795, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5807432532310486, 0.7996500134468079>\n",
      "\n",
      "\n",
      "Epoch  44\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 43100: <0.5032052993774414, 1.0>\n",
      "  batch loss & accuracy at step 44400: <0.5982208251953125, 0.75>\n",
      "  batch loss & accuracy at step 44500: <0.5987822413444519, 0.75>\n",
      "  batch loss & accuracy at step 44600: <0.5032256841659546, 1.0>\n",
      "  batch loss & accuracy at step 44700: <0.5985952615737915, 0.75>\n",
      "  batch loss & accuracy at step 44800: <0.5988613963127136, 0.75>\n",
      "  batch loss & accuracy at step 44900: <0.5032052397727966, 1.0>\n",
      "  batch loss & accuracy at step 45000: <0.5981733202934265, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5830615162849426, 0.7926000356674194>\n",
      "\n",
      "\n",
      "Epoch  46\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 45100: <0.595386803150177, 0.75>\n",
      "  batch loss & accuracy at step 45200: <0.5990353226661682, 0.75>\n",
      "  batch loss & accuracy at step 45300: <0.5937140583992004, 0.75>\n",
      "  batch loss & accuracy at step 45400: <0.5983469486236572, 0.75>\n",
      "  batch loss & accuracy at step 45500: <0.5986828207969666, 0.75>\n",
      "  batch loss & accuracy at step 45600: <0.5989131331443787, 0.75>\n",
      "  batch loss & accuracy at step 45700: <0.693025529384613, 0.5>\n",
      "  batch loss & accuracy at step 45800: <0.5986047983169556, 0.75>\n",
      "  batch loss & accuracy at step 45900: <0.5045507550239563, 1.0>\n",
      "  batch loss & accuracy at step 46000: <0.5036662817001343, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5830224752426147, 0.7924000024795532>\n",
      "\n",
      "\n",
      "Epoch  47\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 46100: <0.6931399703025818, 0.5>\n",
      "  batch loss & accuracy at step 46200: <0.5047381520271301, 1.0>\n",
      "  batch loss & accuracy at step 46300: <0.5130602121353149, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 46400: <0.5982860922813416, 0.75>\n",
      "  batch loss & accuracy at step 46500: <0.5059484839439392, 1.0>\n",
      "  batch loss & accuracy at step 46600: <0.5037196278572083, 1.0>\n",
      "  batch loss & accuracy at step 46700: <0.6069988012313843, 0.75>\n",
      "  batch loss & accuracy at step 46800: <0.5982256531715393, 0.75>\n",
      "  batch loss & accuracy at step 46900: <0.5980974435806274, 0.75>\n",
      "  batch loss & accuracy at step 47000: <0.5979599952697754, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5816905498504639, 0.7957500219345093>\n",
      "\n",
      "\n",
      "Epoch  48\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 47100: <0.5981830358505249, 0.75>\n",
      "  batch loss & accuracy at step 47200: <0.601412296295166, 0.75>\n",
      "  batch loss & accuracy at step 47300: <0.598461389541626, 0.75>\n",
      "  batch loss & accuracy at step 47400: <0.5980269312858582, 0.75>\n",
      "  batch loss & accuracy at step 47500: <0.5032045245170593, 1.0>\n",
      "  batch loss & accuracy at step 47600: <0.5991590023040771, 0.75>\n",
      "  batch loss & accuracy at step 47700: <0.5977188944816589, 0.75>\n",
      "  batch loss & accuracy at step 47800: <0.6931295394897461, 0.5>\n",
      "  batch loss & accuracy at step 47900: <0.5117199420928955, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 48000: <0.5981980562210083, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5792620778083801, 0.8016999959945679>\n",
      "\n",
      "\n",
      "Epoch  49\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 48100: <0.504910409450531, 1.0>\n",
      "  batch loss & accuracy at step 48200: <0.5965707898139954, 0.75>\n",
      "  batch loss & accuracy at step 48300: <0.598212718963623, 0.75>\n",
      "  batch loss & accuracy at step 48400: <0.5988260507583618, 0.75>\n",
      "  batch loss & accuracy at step 48500: <0.5991374850273132, 0.75>\n",
      "  batch loss & accuracy at step 48600: <0.505372941493988, 1.0>\n",
      "  batch loss & accuracy at step 48700: <0.5981937646865845, 0.75>\n",
      "  batch loss & accuracy at step 48800: <0.5981928110122681, 0.75>\n",
      "  batch loss & accuracy at step 48900: <0.5983523726463318, 0.75>\n",
      "  batch loss & accuracy at step 49000: <0.5983893871307373, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5826676487922668, 0.7926749587059021>\n",
      "\n",
      "\n",
      "Epoch  50\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 49100: <0.6457116007804871, 0.625>\n",
      "  batch loss & accuracy at step 49200: <0.5038121938705444, 1.0>\n",
      "  batch loss & accuracy at step 49300: <0.5981656908988953, 0.75>\n",
      "  batch loss & accuracy at step 49400: <0.5982762575149536, 0.75>\n",
      "  batch loss & accuracy at step 49500: <0.5032638311386108, 1.0>\n",
      "  batch loss & accuracy at step 49600: <0.5982308983802795, 0.75>\n",
      "  batch loss & accuracy at step 49700: <0.5036330819129944, 1.0>\n",
      "  batch loss & accuracy at step 49800: <0.5032103657722473, 1.0>\n",
      "  batch loss & accuracy at step 49900: <0.5033724308013916, 1.0>\n",
      "  batch loss & accuracy at step 50000: <0.5982710123062134, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5774111151695251, 0.806475043296814>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 50\n",
    "NUM_BATCHES = 1000\n",
    "VERBOSE = 100\n",
    "\n",
    "loss_track, accuracy_track = [], []\n",
    "for e in range(NUM_EPOCHS):\n",
    "    print('Epoch ', e+1)\n",
    "    print('\\n')\n",
    "    curr_loss_track, curr_accuracy_track = [], []\n",
    "    for _ in range(NUM_BATCHES):\n",
    "        batch_x1, batch_x1_length, batch_x2, batch_x2_length, batch_y = get_batch()\n",
    "        fd = {input_x1:batch_x1, input_x1_length:batch_x1_length,\n",
    "              input_x2:batch_x2, input_x2_length:batch_x2_length,\n",
    "              input_y:batch_y,\n",
    "              keep_prob:KEEP_PROB}\n",
    "        _, step, loss_, accuracy_ = sess.run([train_op, global_step, loss, accuracy], feed_dict=fd)\n",
    "        curr_loss_track.append(loss_)\n",
    "        curr_accuracy_track.append(accuracy_)\n",
    "        if step%VERBOSE==0:\n",
    "            print('  batch loss & accuracy at step {}: <{}, {}>'.format(step, loss_, accuracy_))\n",
    "    print('\\n')\n",
    "    print('  epoch mean loss & accuracy: <{}, {}>'.format(np.mean(curr_loss_track),np.mean(curr_accuracy_track)))\n",
    "    print('\\n')    \n",
    "    loss_track += curr_loss_track\n",
    "    accuracy_track += curr_accuracy_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/04233/sw33286/AIDA-tests/MODEL-SAVE/Bi-LSTM-HAC-wo-context/stacked-bilstm-doc-mix-mock-00'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "save_dir = \"/home/04233/sw33286/AIDA-tests/MODEL-SAVE/Bi-LSTM-HAC-wo-context/\"\n",
    "save_path = save_dir + \"stacked-bilstm-doc-mix-mock-00\"\n",
    "saver.save(sess, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore & Cont'd Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0, \"/work/04233/sw33286/AIDA-SCRIPTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import utilities\n",
    "\n",
    "# import random\n",
    "# import shutil\n",
    "# import numpy as np\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.contrib.rnn import LSTMCell, MultiRNNCell, DropoutWrapper\n",
    "\n",
    "# from helpers import Indexer, batch\n",
    "# from itertools import chain, product\n",
    "\n",
    "# # Data generation block\n",
    "\n",
    "# SHARED_SIZE = 2 # size of noise (or, common vocab for all types).\n",
    "\n",
    "# TYPES = ['ANIMAL','VEHICLE','NATURE','FURNITURE','FRUIT']\n",
    "# SHARED_VOCAB = ['share'+str(i+1) for i in range(SHARED_SIZE)]\n",
    "# TYPE2VOCAB = {'ANIMAL': ['cat','dog','pig','horse','deer']            + SHARED_VOCAB,\n",
    "#               'VEHICLE': ['car','bike','motorcycle','train','bus']    + SHARED_VOCAB,\n",
    "#               'NATURE': ['hill','mountain','lake','river','valley']   + SHARED_VOCAB,\n",
    "#               'FURNITURE': ['stool','table','closet','cabinet','bed'] + SHARED_VOCAB,\n",
    "#               'FRUIT': ['apple','pear','strawberry','grape','tomato'] + SHARED_VOCAB}\n",
    "# VOCAB = list(chain.from_iterable(TYPE2VOCAB.values()))\n",
    "\n",
    "# indexer = Indexer()\n",
    "# indexer.get_index('PAD')\n",
    "# for word in VOCAB:\n",
    "#     indexer.get_index(word)\n",
    "    \n",
    "# DOC_LEN = 5\n",
    "# SENT_FROM_LEN = 5\n",
    "# SENT_TO_LEN = 15\n",
    "\n",
    "# def to_sent(code):\n",
    "#     return [indexer.get_object(idx) for idx in code]\n",
    "\n",
    "# def get_rand_sent_code(sem_type, sent_len):\n",
    "#     return [indexer.get_index(np.random.choice(TYPE2VOCAB[sem_type])) for _ in range(sent_len)]\n",
    "\n",
    "# def get_mixture(type1, type2):\n",
    "#     doc_a = [get_rand_sent_code(type1, np.random.randint(SENT_FROM_LEN, SENT_TO_LEN)) for _ in range(DOC_LEN)]\n",
    "#     doc_b = [get_rand_sent_code(type2, np.random.randint(SENT_FROM_LEN, SENT_TO_LEN)) for _ in range(DOC_LEN)]\n",
    "#     doc_mix = np.array(doc_a[:] + doc_b[:])\n",
    "#     doc_lbs = np.array([0]*DOC_LEN + [1]*DOC_LEN)\n",
    "#     indices = list(range(DOC_LEN*2))\n",
    "#     random.shuffle(indices)\n",
    "#     doc_mix = doc_mix[indices]\n",
    "#     doc_lbs = doc_lbs[indices]\n",
    "#     return doc_a, doc_b, doc_mix, doc_lbs\n",
    "    \n",
    "# def batch_mixture(doc_a, doc_b, k):\n",
    "#     batch_x1, batch_x2, batch_y = [], [], []\n",
    "#     ys = [1,0,0,1]\n",
    "#     for _ in range(k):\n",
    "#         for i,(da,db) in enumerate(product([doc_a,doc_b],[doc_a,doc_b])):\n",
    "#             batch_x1.append(random.choice(da))\n",
    "#             batch_x2.append(random.choice(db))\n",
    "#             batch_y.append(ys[i])\n",
    "#     return batch(batch_x1), batch(batch_x2), np.array(batch_y)\n",
    "\n",
    "# def get_batch(n=40):\n",
    "#     if n%4!=0:\n",
    "#         raise ValueError('The current generation scheme only supports multiples of 4 for batch size!')\n",
    "#     type1, type2 = np.random.choice(TYPES, 2, replace=False)\n",
    "#     doc_a, doc_b, _, _ = get_mixture(type1, type2) # document mixtures and labels aren't germane here.\n",
    "#     (batch_x1,batch_x1_len), (batch_x2,batch_x2_len), batch_y = batch_mixture(doc_a,doc_b,n//4)\n",
    "#     return batch_x1,batch_x1_len,batch_x2,batch_x2_len,batch_y\n",
    "# # Model restoration block\n",
    "\n",
    "# restore_dir = \"/home/04233/sw33286/AIDA-tests/MODEL-SAVE/Bi-LSTM-HAC-wo-context/\"\n",
    "\n",
    "# sess = tf.Session()\n",
    "\n",
    "# saver = tf.train.import_meta_graph(restore_dir+'stacked-bilstm-doc-mix-mock-00.meta')\n",
    "# saver.restore(sess, tf.train.latest_checkpoint(restore_dir))\n",
    "# graph = tf.get_default_graph()\n",
    "\n",
    "# VOCAB_SIZE = len(indexer)\n",
    "# EMB_SIZE = 20\n",
    "# HID_SIZE = 10\n",
    "# NUM_LAYERS = 2\n",
    "# KEEP_PROB = 0.7\n",
    "\n",
    "# input_x1 = graph.get_tensor_by_name('input_x1:0')\n",
    "# input_x2 = graph.get_tensor_by_name('input_x2:0')\n",
    "# input_x1_length = graph.get_tensor_by_name('input_x1_length:0')\n",
    "# input_x2_length = graph.get_tensor_by_name('input_x2_length:0')\n",
    "# input_y = graph.get_tensor_by_name('input_y:0')\n",
    "# keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "# scores = graph.get_tensor_by_name('scores:0')\n",
    "# predictions = graph.get_tensor_by_name('predictions:0')\n",
    "# loss = graph.get_tensor_by_name('Loss/Mean:0')\n",
    "# accuracy = graph.get_tensor_by_name('Accuracy/accuracy:0')\n",
    "# global_step = graph.get_tensor_by_name('global_step:0')\n",
    "# train_op = graph.get_tensor_by_name('train_op:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NUM_EPOCHS = 50\n",
    "# NUM_BATCHES = 1000\n",
    "# VERBOSE = 100\n",
    "\n",
    "# loss_track, accuracy_track = [], []\n",
    "# for e in range(NUM_EPOCHS):\n",
    "#     print('Epoch ', e+1)\n",
    "#     print('\\n')\n",
    "#     curr_loss_track, curr_accuracy_track = [], []\n",
    "#     for _ in range(NUM_BATCHES):\n",
    "#         batch_x1, batch_x1_length, batch_x2, batch_x2_length, batch_y = get_batch()\n",
    "#         fd = {input_x1:batch_x1, input_x1_length:batch_x1_length,\n",
    "#               input_x2:batch_x2, input_x2_length:batch_x2_length,\n",
    "#               input_y:batch_y}\n",
    "#         _, step, loss_, accuracy_ = sess.run([train_op, global_step, loss, accuracy], feed_dict=fd)\n",
    "#         curr_loss_track.append(loss_)\n",
    "#         curr_accuracy_track.append(accuracy_)\n",
    "#         if step%VERBOSE==0:\n",
    "#             print('  batch loss & accuracy at step {}: <{}, {}>'.format(step, loss_, accuracy_))\n",
    "#     print('\\n')\n",
    "#     print('  epoch mean loss & accuracy: <{}, {}>'.format(np.mean(curr_loss_track),np.mean(curr_accuracy_track)))\n",
    "#     print('\\n')    \n",
    "#     loss_track += curr_loss_track\n",
    "#     accuracy_track += curr_accuracy_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver = tf.train.Saver()\n",
    "# save_dir = \"/home/04233/sw33286/AIDA-tests/MODEL-SAVE/GROUPER-MOCK/\"\n",
    "# save_path = save_dir + \"stacked-bilstm-doc-mix-mock-00\"\n",
    "# shutil.rmtree(save_dir)\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "# saver.save(sess, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM + HAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class ClfHAC:\n",
    "    \n",
    "    def __init__(self, clf_dir, clf_filename):\n",
    "        self.sess = tf.Session()\n",
    "        saver = tf.train.import_meta_graph(clf_dir + clf_filename)\n",
    "        saver.restore(self.sess, tf.train.latest_checkpoint(clf_dir))\n",
    "        self.graph = tf.get_default_graph()\n",
    "        self.input_x1 = self.graph.get_tensor_by_name('input_x1:0')\n",
    "        self.input_x2 = self.graph.get_tensor_by_name('input_x2:0')\n",
    "        self.input_x1_length = self.graph.get_tensor_by_name('input_x1_length:0')\n",
    "        self.input_x2_length = self.graph.get_tensor_by_name('input_x2_length:0')\n",
    "        self.input_y = self.graph.get_tensor_by_name('input_y:0')\n",
    "        self.keep_prob = self.graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "        self.scores = self.graph.get_tensor_by_name('scores:0')\n",
    "        self.predictions = self.graph.get_tensor_by_name('predictions:0')\n",
    "        self.loss = self.graph.get_tensor_by_name('Loss/Mean:0')\n",
    "        self.accuracy = self.graph.get_tensor_by_name('Accuracy/accuracy:0')\n",
    "        self.global_step = self.graph.get_tensor_by_name('global_step:0')\n",
    "        self.train_op = self.graph.get_tensor_by_name('train_op:0')\n",
    "        \n",
    "        \n",
    "    def dist(self, x1, x2):\n",
    "        x1, x1_len = batch([x1])\n",
    "        x2, x2_len = batch([x2])\n",
    "        fd = {self.input_x1:x1, self.input_x1_length:x1_len,\n",
    "              self.input_x2:x2, self.input_x2_length:x2_len,\n",
    "              self.keep_prob:1.0}\n",
    "        conf = self.sess.run(self.scores, feed_dict=fd)\n",
    "        return 1-conf[0]\n",
    "    \n",
    "    def evaluate(self, doc_mix, doc_lbs, method='average', plot=True):\n",
    "        doc_mix_sq, _ = batch(doc_mix)\n",
    "        doc_mix_sq = doc_mix_sq.T\n",
    "        doc_mix_clust = linkage(doc_mix_sq, method=method, metric=self.dist)\n",
    "        # evaluate by class-based prec/rec/f1\n",
    "        doc_prd = fcluster(clust, 2, criterion='maxclust') # predicted assignments\n",
    "        eval_input = clust_to_eval_input(doc_prd, doc_lbs)\n",
    "        prec, rec = cb_prec(*eval_input), cb_rec(*eval_input)\n",
    "        f1 = cb_f1(prec, rec)\n",
    "        if plot:\n",
    "            print('Class-based clustering evaluation:')\n",
    "            print('Precision = {} | Recall = {} | F1 = {}'.format(prec,rec,f1))\n",
    "            print('\\n')\n",
    "            plt.figure(figsize=(25, 10))\n",
    "            plt.title('Hierarchical Clustering Dendrogram')\n",
    "            plt.xlabel('sample index')\n",
    "            plt.ylabel('distance')\n",
    "            dendrogram(\n",
    "                doc_mix_clust,\n",
    "                leaf_rotation=90.,  # rotates the x axis labels\n",
    "                leaf_font_size=15.,  # font size for the x axis labels\n",
    "            )\n",
    "            plt.show() \n",
    "            print('True | Pred | Sentence')\n",
    "            for label,pred_label,code in zip(doc_lbs,doc_prd,doc_mix):\n",
    "                print('{}    | {}    | {}'.format(label,pred_label,to_sent(code)))\n",
    "            print('\\n')\n",
    "        else:\n",
    "            return doc_mix_clust, prec, rec, f1\n",
    "        \n",
    "        \n",
    "def clust_to_eval_input(pred_clust, true_clust):\n",
    "    pred_ass2cls, pred_cls2ass = [], defaultdict(list)\n",
    "    true_ass2cls, true_cls2ass = [], defaultdict(list)\n",
    "    for item_id,(pred_ass,true_ass) in enumerate(zip(pred_clust,true_clust)):\n",
    "        pred_ass2cls.append((item_id,pred_ass))\n",
    "        true_ass2cls.append((item_id,true_ass))\n",
    "        pred_cls2ass[pred_ass].append(item_id)\n",
    "        true_cls2ass[true_ass].append(item_id)\n",
    "    pred_ass2cls = dict(pred_ass2cls)\n",
    "    true_ass2cls = dict(true_ass2cls)\n",
    "    return pred_ass2cls, pred_cls2ass, true_ass2cls, true_cls2ass\n",
    "\n",
    "def cb(source_a2c, source_c2a, target_a2c, target_c2a):\n",
    "    prec_num = prec_denom = 0\n",
    "    for _,ass in source_c2a.items():\n",
    "        card_ass = len(ass)\n",
    "        mem_cls = set()\n",
    "        for elem in ass:\n",
    "            for t_elem,cls in target_a2c.items():\n",
    "                if elem==t_elem:\n",
    "                    mem_cls.add(cls)\n",
    "        prec_num += card_ass - len(mem_cls)\n",
    "        prec_denom += card_ass - 1\n",
    "    return prec_num / prec_denom   \n",
    "\n",
    "def cb_prec(p_a2c, p_c2a, t_a2c, t_c2a):\n",
    "    return cb(p_a2c, p_c2a, t_a2c, t_c2a)\n",
    "\n",
    "def cb_rec(p_a2c, p_c2a, t_a2c, t_c2a):\n",
    "    return cb(t_a2c, t_c2a, p_a2c, p_c2a)\n",
    "    \n",
    "def cb_f1(prec, rec):\n",
    "    return (2*rec*prec) / (rec+prec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_dir = \"/home/04233/sw33286/AIDA-tests/MODEL-SAVE/Bi-LSTM-HAC-wo-context/\"\n",
    "restore_filename = 'stacked-bilstm-doc-mix-mock-00.meta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_hac = ClfHAC(restore_dir, restore_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-based clustering evaluation:\n",
      "Precision = 1.0 | Recall = 1.0 | F1 = 1.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB9YAAANVCAYAAAAtFBVGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXZIkSZIkSZIkSZKkSfwfWduBc5BuLMgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ae741b93b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True | Pred | Sentence\n",
      "1    | 2    | ['motorcycle', 'bus', 'motorcycle', 'share1', 'car', 'bus', 'motorcycle', 'share2']\n",
      "1    | 2    | ['bike', 'bus', 'car', 'train', 'bus', 'car', 'share1', 'share1']\n",
      "0    | 1    | ['deer', 'pig', 'share1', 'pig', 'cat', 'pig', 'share2', 'share2', 'cat', 'dog', 'horse', 'share2']\n",
      "0    | 1    | ['share1', 'deer', 'share1', 'dog', 'horse', 'dog']\n",
      "0    | 1    | ['horse', 'share1', 'pig', 'share2', 'deer', 'share1', 'dog', 'share2', 'horse', 'pig', 'cat', 'cat']\n",
      "0    | 1    | ['share2', 'horse', 'pig', 'share1', 'deer', 'share2', 'share1', 'deer']\n",
      "1    | 2    | ['car', 'motorcycle', 'train', 'bus', 'car', 'train', 'share1']\n",
      "0    | 1    | ['share2', 'deer', 'pig', 'pig', 'horse']\n",
      "1    | 2    | ['car', 'motorcycle', 'motorcycle', 'bike', 'share1', 'motorcycle', 'share2', 'car', 'bus', 'bike', 'train', 'share2', 'train', 'bike']\n",
      "1    | 2    | ['car', 'train', 'car', 'train', 'motorcycle', 'motorcycle']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# da,db,dm,dl = get_mixture('ANIMAL', 'VEHICLE')\n",
    "clf_hac.evaluate(dm, dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Evaluating with randomly generated documents\n",
    "\n",
    "# NUM_DOCS = 1000\n",
    "\n",
    "# def get_scores_for_rand_doc_mix(clf): # clf: a ClfHAC object.\n",
    "#     type1, type2 = np.random.choice(TYPES, 2, replace=False)\n",
    "#     print(type1, type2)\n",
    "#     _,_,dm,dl = get_mixture(type1, type2) # document a,b don't play into evaluation here.\n",
    "#     _, prec, rec, f1 = clf.evaluate(dm, dl, plot=False)\n",
    "#     return prec, rec, f1\n",
    "\n",
    "# precs,recs,f1s = [],[],[]\n",
    "# for i in range(NUM_DOCS):\n",
    "#     prec,rec,f1 = get_scores_for_rand_doc_mix(clf_hac)\n",
    "#     precs.append(prec)\n",
    "#     recs.append(rec)\n",
    "#     f1s.append(f1)\n",
    "#     if i%10==0:\n",
    "#         print('Average Prec/Rec/F1 at {} = {}/{}/{}'.format(i+1,np.mean(precs),np.mean(recs),np.mean(f1s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # da,db,dm,dl = get_mixture('ANIMAL', 'VEHICLE')\n",
    "# clf_hac.evaluate(dm, dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _,_,dm2,dl2 = get_mixture('ANIMAL', 'VEHICLE')\n",
    "# clf_hac.evaluate(dm2, dl2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
