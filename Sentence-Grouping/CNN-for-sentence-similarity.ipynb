{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Sentence Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let $x_1$, $x_2$ be two sentences (encoded in integer indices) of lengths $l_1$, $l_2$ respectively, then we first pad them to be of length $l$, and then embed their consisting words in $d$ dimensional space to produce two matrices $X_1^{l\\times d}, X_2^{l\\times d}$.\n",
    "* We then convolve on the matrices with filters of shapes $(3\\times d), (4\\times d)$ and $(5\\times d)$. This results in vectors of the shape $(l-3+1, 1), (l-4+1, 1)$ and $(l-5+1, 1)$ (with 'VALID', i.e. narrow convolution), for each filter size (i.e. $3, 4$ and $5$). With $k$ filters per filter size, we end up with vectors $((l-3+1)\\times k, 1), ((l-4+1)\\times k, 1)$ and $((l-5+1)\\times k, 1)$. For $x_1$, call them $h_1^3, h_1^4, h_1^5$, on which we do ReLU nonlinearity: $$h = \\texttt{ReLU}(h)$$\n",
    "* Performing max-pooling on the above and concatenate results, we get a single vector representation for each of the two sentences, of shape $(3k,)$, for the two sentences we get $h_{pool1}, h_{pool2}$, we then put them through dropout regularization with probability $p$: $$h_{pool} = \\texttt{Dropout}(h_{pool}, p)$$\n",
    "* Finally we do a bilinear transformation on these vectors to make logits: $$\\texttt{Logit}(x_{pool1}, x_{pool2}) = \\texttt{Round}(\\sigma(x_{pool1}^T W^{3k\\times 3k} x_{pool2}))$$\n",
    "from which we get the loss $\\mathcal{L}$: $$\\mathcal{L} = \\texttt{Cross-Ent}(\\texttt{Logit}(x_{pool1}, x_{pool2}), y)$$\n",
    "where $y$ is the true label.\n",
    "where \n",
    "* Finally, the prediction is made by rounding the logit: $$\\hat{y} = \\texttt{Round}(\\texttt{Logit}(x_{pool1}, x_{pool2}))$$\n",
    "where $\\texttt{Round}$ returns $1$ if the sigmoid probability is greater than $0.5$ and $0$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/jacob.su.wang/Desktop/CODER/TENSORFLOW/SCRIPTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from helpers import Indexer\n",
    "from mock_sentence_similarity import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = Indexer()\n",
    "glove_embeddings = get_dict_and_embeddings(VOCAB, indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['motorcycle', 'train', 'train', 'car', 'bus', 'truck', 'train',\n",
      "       'train', 'bus', 'car', 'train', 'train', 'truck', 'train', 'truck',\n",
      "       'car', 'motorcycle', 'car', 'train'],\n",
      "      dtype='<U10'), array(['motorcycle', 'train', 'motorcycle', 'car', 'car', 'car', 'train',\n",
      "       'motorcycle', 'truck', 'train', 'train', 'truck', 'car', 'truck',\n",
      "       'truck', 'truck', 'motorcycle', 'motorcycle', 'car'],\n",
      "      dtype='<U10'))\n",
      "(array(['cat', 'cat', 'deer', 'deer', 'pig', 'dog', 'dog', 'deer', 'dog',\n",
      "       'pig', 'pig', 'pig', 'cat', 'pig'],\n",
      "      dtype='<U5'), array(['train', 'train', 'bus', 'train', 'bus', 'bus', 'motorcycle', 'car',\n",
      "       'motorcycle', 'motorcycle', 'bus', 'motorcycle', 'train',\n",
      "       'motorcycle'],\n",
      "      dtype='<U10'))\n"
     ]
    }
   ],
   "source": [
    "print(generate_pos_datum(19))\n",
    "print(generate_neg_datum(14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  2 10 10 10]\n",
      " [ 9  4  4 10 10]]\n",
      "[[ 2  2 10 10 10]\n",
      " [ 7  2  1 10 10]]\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "x1, x2, y = generate_batch(indexer, len_from=2, len_to=5, batch_size=2)\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "LEN_FROM, LEN_TO = 5, 15\n",
    "MAX_LEN = 15\n",
    "NUM_CLASSES = 2\n",
    "VOCAB_SIZE = len(VOCAB) + 1\n",
    "EMBED_SIZE = 20\n",
    "FILTER_SIZES = [3,4,5]   # size types.\n",
    "NUM_FILTERS = 10         # #filters per size type.\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "input_x1 = tf.placeholder(tf.int32, [None, MAX_LEN], name='input_x1')\n",
    "input_x2 = tf.placeholder(tf.int32, [None, MAX_LEN], name='input_x2')\n",
    "input_y  = tf.placeholder(tf.int32, [None], name='input_y')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "with tf.device('/cpu:0'), tf.variable_scope('embeddings'): \n",
    "        # name_scope works only with tf.Variable\n",
    "        # variable_scope works with tf.get_variable\n",
    "    E = tf.get_variable('E', [VOCAB_SIZE, EMBED_SIZE], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    embed_x1 = tf.expand_dims(tf.nn.embedding_lookup(E, input_x1), -1)\n",
    "    embed_x2 = tf.expand_dims(tf.nn.embedding_lookup(E, input_x2), -1)\n",
    "        # embed_x*: [batch_size, height=MAX_LEN, width=EMBED_SIZE, num_channels=1]\n",
    "\n",
    "pool1_outputs, pool2_outputs = [], []\n",
    "for i, filter_size in enumerate(FILTER_SIZES):\n",
    "    with tf.variable_scope('conv-max-pool-%s' % filter_size): \n",
    "        filter_shape = [filter_size, EMBED_SIZE, NUM_CHANNELS, NUM_FILTERS]\n",
    "            # Filter dims: [filter_size, emb_size, num_channels, num_filters]\n",
    "        W1 = tf.get_variable('W1', filter_shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        W2 = tf.get_variable('W2', filter_shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b1 = tf.get_variable('b1', [NUM_FILTERS], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b2 = tf.get_variable('b2', [NUM_FILTERS], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        conv1 = tf.nn.conv2d(embed_x1, W1, strides=[1,1,1,1], padding='VALID', name='conv1')\n",
    "        conv2 = tf.nn.conv2d(embed_x2, W2, strides=[1,1,1,1], padding='VALID', name='conv2')\n",
    "            # Conv dims: [batch_size, height, width, num_channels]\n",
    "        h1 = tf.nn.relu(tf.nn.bias_add(conv1, b1), name='relu1')\n",
    "        h2 = tf.nn.relu(tf.nn.bias_add(conv2, b2), name='relu2')\n",
    "        pool1 = tf.nn.max_pool(h1, ksize=[1,MAX_LEN-filter_size+1,1,1], strides=[1,1,1,1], padding='VALID', name='pool1')\n",
    "        pool2 = tf.nn.max_pool(h2, ksize=[1,MAX_LEN-filter_size+1,1,1], strides=[1,1,1,1], padding='VALID', name='pool2')\n",
    "            # kernel size (ksize): [batch_size, height, width, num_channels]\n",
    "        pool1_outputs.append(pool1)\n",
    "        pool2_outputs.append(pool2)\n",
    "\n",
    "num_filters_total = NUM_FILTERS * len(FILTER_SIZES)\n",
    "h_pool1_flat = tf.nn.dropout(tf.reshape(tf.concat(pool1_outputs, 3), [-1, num_filters_total]), keep_prob)\n",
    "h_pool2_flat = tf.nn.dropout(tf.reshape(tf.concat(pool2_outputs, 3), [-1, num_filters_total]), keep_prob)\n",
    "    # flat shape: [batch_size, num_filters_total].\n",
    "W_bi = tf.get_variable('W_bi', [num_filters_total, num_filters_total],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "scores = tf.nn.sigmoid(tf.diag_part(tf.matmul(tf.matmul(h_pool1_flat, W_bi), tf.transpose(h_pool2_flat))))\n",
    "\n",
    "predictions = tf.cast(tf.round(scores), tf.int32)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(input_y, tf.float32), logits=scores)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_predictions = tf.equal(predictions, input_y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "    \n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "\n",
      "\n",
      "M.Loss = 0.7023053765296936 | M.Accuracy = 0.4996874928474426\n",
      "M.Loss = 0.7000969648361206 | M.Accuracy = 0.49976563453674316\n",
      "M.Loss = 0.6982930898666382 | M.Accuracy = 0.499895840883255\n",
      "M.Loss = 0.6973963379859924 | M.Accuracy = 0.49996092915534973\n",
      "M.Loss = 0.6965771317481995 | M.Accuracy = 0.499937504529953\n",
      "M.Loss = 0.6957278251647949 | M.Accuracy = 0.5000260472297668\n",
      "M.Loss = 0.6942504644393921 | M.Accuracy = 0.5012500286102295\n",
      "M.Loss = 0.6894691586494446 | M.Accuracy = 0.5171484351158142\n",
      "M.Loss = 0.6815314292907715 | M.Accuracy = 0.5413888692855835\n",
      "M.Loss = 0.6685624718666077 | M.Accuracy = 0.5791562795639038\n",
      "\n",
      "\n",
      "Epoch  2\n",
      "\n",
      "\n",
      "M.Loss = 0.5177417993545532 | M.Accuracy = 0.9943749904632568\n",
      "M.Loss = 0.5142209529876709 | M.Accuracy = 0.9961718916893005\n",
      "M.Loss = 0.5122621655464172 | M.Accuracy = 0.9970312714576721\n",
      "M.Loss = 0.5108184814453125 | M.Accuracy = 0.9976953268051147\n",
      "M.Loss = 0.5099108219146729 | M.Accuracy = 0.9980000257492065\n",
      "M.Loss = 0.5091516375541687 | M.Accuracy = 0.9982030987739563\n",
      "M.Loss = 0.5085728168487549 | M.Accuracy = 0.9983928799629211\n",
      "M.Loss = 0.508089005947113 | M.Accuracy = 0.99853515625\n",
      "M.Loss = 0.5076925158500671 | M.Accuracy = 0.9986805319786072\n",
      "M.Loss = 0.5073525905609131 | M.Accuracy = 0.9987812638282776\n",
      "\n",
      "\n",
      "Epoch  3\n",
      "\n",
      "\n",
      "M.Loss = 0.5041100978851318 | M.Accuracy = 1.0\n",
      "M.Loss = 0.5040546655654907 | M.Accuracy = 1.0\n",
      "M.Loss = 0.5040223598480225 | M.Accuracy = 0.9998958110809326\n",
      "M.Loss = 0.503975510597229 | M.Accuracy = 0.9999218583106995\n",
      "M.Loss = 0.5039864182472229 | M.Accuracy = 0.9998437762260437\n",
      "M.Loss = 0.503966748714447 | M.Accuracy = 0.9998177289962769\n",
      "M.Loss = 0.5039284825325012 | M.Accuracy = 0.9998437762260437\n",
      "M.Loss = 0.5039111375808716 | M.Accuracy = 0.9998632669448853\n",
      "M.Loss = 0.503888726234436 | M.Accuracy = 0.9998611211776733\n",
      "M.Loss = 0.5038537979125977 | M.Accuracy = 0.9998593926429749\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 3\n",
    "NUM_BATCHES = 1000\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(NUM_EPOCHS):\n",
    "    print('Epoch ', e+1)\n",
    "    print('\\n')\n",
    "    loss_track, accuracy_track = [], []\n",
    "    for b in range(NUM_BATCHES):\n",
    "        batch_x1, batch_x2, batch_y = generate_batch(indexer, LEN_FROM, LEN_TO, batch_size=64)\n",
    "        fd = {input_x1:batch_x1, input_x2:batch_x2, input_y:batch_y, keep_prob:0.7}\n",
    "        _, step, loss_, accuracy_ = sess.run([train_op, global_step, loss, accuracy], feed_dict=fd)\n",
    "        loss_track.append(loss_)\n",
    "        accuracy_track.append(accuracy_)\n",
    "        if step%100==0:\n",
    "            print('M.Loss = {} | M.Accuracy = {}'.format(np.mean(loss_track), np.mean(accuracy_track)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test x1 = x2; test x3 in another group.\n",
    "test_x1 = [['motorcycle', 'train', 'motorcycle', 'car', 'car', 'car', 'train']]\n",
    "test_x2 = [['motorcycle', 'train', 'train', 'car', 'bus']]\n",
    "test_x3 = [['cat', 'cat', 'deer', 'deer', 'pig', 'dog', 'dog', 'deer', 'dog']]\n",
    "test_y12 = [1]\n",
    "test_y13 = [0]\n",
    "test_y23 = [0]\n",
    "\n",
    "def to_code(x_):\n",
    "    return [[indexer.get_index(elem) for elem in pad_sentence(MAX_LEN, x_[0])]]\n",
    "\n",
    "def predict(x1_, x2_, y_, sess_):\n",
    "    fd = {input_x1:to_code(x1_), input_x2:to_code(x2_), keep_prob:1.0}\n",
    "    pred = sess_.run(predictions, feed_dict=fd)\n",
    "    print('True = {} | Pred = {}'.format(y_, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True = [1] | Pred = [1]\n",
      "True = [1] | Pred = [1]\n",
      "True = [1] | Pred = [1]\n"
     ]
    }
   ],
   "source": [
    "predict(test_x1, test_x2, test_y12, sess)\n",
    "predict(test_x1, test_x2, test_y12, sess)\n",
    "predict(test_x1, test_x2, test_y12, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
