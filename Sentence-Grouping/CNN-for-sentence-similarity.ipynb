{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Sentence Similarity\n",
    "\n",
    "** Purpose **\n",
    "\n",
    "We want to make a binary classifier that takes a pair of sentences and judge whether they belong to the same group or not, and then help a downstream cluster (e.g. HAC) w/ it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let $x_1$, $x_2$ be two sentences (encoded in integer indices) of lengths $l_1$, $l_2$ respectively, then we first pad them to be of length $l$, and then embed their consisting words in $d$ dimensional space to produce two matrices $X_1^{l\\times d}, X_2^{l\\times d}$.\n",
    "* We then convolve on the matrices with filters of shapes $(3\\times d), (4\\times d)$ and $(5\\times d)$. This results in vectors of the shape $(l-3+1, 1), (l-4+1, 1)$ and $(l-5+1, 1)$ (with 'VALID', i.e. narrow convolution), for each filter size (i.e. $3, 4$ and $5$). With $k$ filters per filter size, we end up with vectors $((l-3+1)\\times k, 1), ((l-4+1)\\times k, 1)$ and $((l-5+1)\\times k, 1)$. For $x_1$, call them $h_1^3, h_1^4, h_1^5$, on which we do ReLU nonlinearity: $$h = \\texttt{ReLU}(h)$$\n",
    "* Performing max-pooling on the above and concatenate results, we get a single vector representation for each of the two sentences, of shape $(3k,)$, for the two sentences we get $h_{pool1}, h_{pool2}$, we then put them through dropout regularization with probability $p$: $$h_{pool} = \\texttt{Dropout}(h_{pool}, p)$$\n",
    "* Finally we do a bilinear transformation on these vectors to make logits: $$\\texttt{Logit}(x_{pool1}, x_{pool2}) = \\sigma(x_{pool1}^T W^{3k\\times 3k} x_{pool2})$$\n",
    "from which we get the loss $\\mathcal{L}$: $$\\mathcal{L} = \\texttt{Cross-Ent}(\\texttt{Logit}(x_{pool1}, x_{pool2}), y)$$\n",
    "where $y$ is the true label.\n",
    "where \n",
    "* Finally, the prediction is made by rounding the logit: $$\\hat{y} = \\texttt{Round}(\\texttt{Logit}(x_{pool1}, x_{pool2}))$$\n",
    "where $\\texttt{Round}$ returns $1$ if the sigmoid probability is greater than $0.5$ and $0$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/jacob.su.wang/Desktop/CODER/TENSORFLOW/SCRIPTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from helpers import Indexer\n",
    "from mock_sentence_similarity import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexer = Indexer()\n",
    "glove_embeddings = get_dict_and_embeddings(VOCAB, indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['dog', 'pig', 'horse', 'horse', 'pig', 'pig', 'deer', 'cat', 'deer',\n",
      "       'horse', 'cat', 'dog', 'horse', 'cat', 'dog', 'horse', 'horse',\n",
      "       'cat', 'dog'],\n",
      "      dtype='<U5'), array(['deer', 'horse', 'dog', 'cat', 'cat', 'deer', 'dog', 'deer',\n",
      "       'horse', 'dog', 'cat', 'horse', 'deer', 'dog', 'cat', 'cat', 'cat',\n",
      "       'pig', 'cat'],\n",
      "      dtype='<U5'))\n",
      "(array(['pig', 'horse', 'dog', 'pig', 'dog', 'horse', 'dog', 'pig', 'cat',\n",
      "       'deer', 'horse', 'pig', 'deer', 'horse'],\n",
      "      dtype='<U5'), array(['bus', 'car', 'car', 'train', 'motorcycle', 'truck', 'motorcycle',\n",
      "       'bus', 'truck', 'train', 'bus', 'bus', 'train', 'train'],\n",
      "      dtype='<U10'))\n"
     ]
    }
   ],
   "source": [
    "print(generate_pos_datum(19))\n",
    "print(generate_neg_datum(14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0 10 10 10]\n",
      " [ 5  5  8 10 10]]\n",
      "[[ 8  5 10 10 10]\n",
      " [ 6  9  8 10 10]]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "x1, x2, y = generate_batch(indexer, len_from=2, len_to=5, batch_size=2)\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "LEN_FROM, LEN_TO = 5, 15\n",
    "MAX_LEN = 15\n",
    "NUM_CLASSES = 2\n",
    "VOCAB_SIZE = len(VOCAB) + 1\n",
    "EMBED_SIZE = 20\n",
    "FILTER_SIZES = [3,4,5]   # size types.\n",
    "NUM_FILTERS = 10         # #filters per size type.\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "# sess = tf.InteractiveSession()\n",
    "sess = tf.Session()\n",
    "\n",
    "input_x1 = tf.placeholder(tf.int32, [None, MAX_LEN], name='input_x1')\n",
    "input_x2 = tf.placeholder(tf.int32, [None, MAX_LEN], name='input_x2')\n",
    "input_y  = tf.placeholder(tf.int32, [None], name='input_y')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "with tf.device('/cpu:0'), tf.variable_scope('embeddings'): \n",
    "        # name_scope works only with tf.Variable\n",
    "        # variable_scope works with tf.get_variable\n",
    "    E = tf.get_variable('E', [VOCAB_SIZE, EMBED_SIZE], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    embed_x1 = tf.expand_dims(tf.nn.embedding_lookup(E, input_x1), -1)\n",
    "    embed_x2 = tf.expand_dims(tf.nn.embedding_lookup(E, input_x2), -1)\n",
    "        # embed_x*: [batch_size, height=MAX_LEN, width=EMBED_SIZE, num_channels=1]\n",
    "\n",
    "pool1_outputs, pool2_outputs = [], []\n",
    "for i, filter_size in enumerate(FILTER_SIZES):\n",
    "    with tf.variable_scope('conv-max-pool-%s' % filter_size): \n",
    "        filter_shape = [filter_size, EMBED_SIZE, NUM_CHANNELS, NUM_FILTERS]\n",
    "            # Filter dims: [filter_size, emb_size, num_channels, num_filters]\n",
    "        W1 = tf.get_variable('W1', filter_shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        W2 = tf.get_variable('W2', filter_shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b1 = tf.get_variable('b1', [NUM_FILTERS], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b2 = tf.get_variable('b2', [NUM_FILTERS], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        conv1 = tf.nn.conv2d(embed_x1, W1, strides=[1,1,1,1], padding='VALID', name='conv1')\n",
    "        conv2 = tf.nn.conv2d(embed_x2, W2, strides=[1,1,1,1], padding='VALID', name='conv2')\n",
    "            # Conv dims: [batch_size, height, width, num_channels]\n",
    "        h1 = tf.nn.relu(tf.nn.bias_add(conv1, b1), name='relu1')\n",
    "        h2 = tf.nn.relu(tf.nn.bias_add(conv2, b2), name='relu2')\n",
    "        pool1 = tf.nn.max_pool(h1, ksize=[1,MAX_LEN-filter_size+1,1,1], strides=[1,1,1,1], padding='VALID', name='pool1')\n",
    "        pool2 = tf.nn.max_pool(h2, ksize=[1,MAX_LEN-filter_size+1,1,1], strides=[1,1,1,1], padding='VALID', name='pool2')\n",
    "            # kernel size (ksize): [batch_size, height, width, num_channels]\n",
    "        pool1_outputs.append(pool1)\n",
    "        pool2_outputs.append(pool2)\n",
    "\n",
    "num_filters_total = NUM_FILTERS * len(FILTER_SIZES)\n",
    "h_pool1_flat = tf.nn.dropout(tf.reshape(tf.concat(pool1_outputs, 3), [-1, num_filters_total]), keep_prob)\n",
    "h_pool2_flat = tf.nn.dropout(tf.reshape(tf.concat(pool2_outputs, 3), [-1, num_filters_total]), keep_prob)\n",
    "    # flat shape: [batch_size, num_filters_total].\n",
    "W_bi = tf.get_variable('W_bi', [num_filters_total, num_filters_total],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "scores = tf.nn.sigmoid(tf.diag_part(tf.matmul(tf.matmul(h_pool1_flat, W_bi), tf.transpose(h_pool2_flat))))\n",
    "\n",
    "predictions = tf.cast(tf.round(scores), tf.int32, name='predictions')\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(input_y, tf.float32), logits=scores)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_predictions = tf.equal(predictions, input_y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "    \n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "\n",
      "\n",
      "M.Loss = 0.7222447395324707 | M.Accuracy = 0.5165625214576721\n",
      "M.Loss = 0.7139453291893005 | M.Accuracy = 0.510546863079071\n",
      "M.Loss = 0.708305299282074 | M.Accuracy = 0.5075520873069763\n",
      "M.Loss = 0.704450249671936 | M.Accuracy = 0.5064843893051147\n",
      "M.Loss = 0.7013000249862671 | M.Accuracy = 0.5061249732971191\n",
      "M.Loss = 0.6969977021217346 | M.Accuracy = 0.5095833539962769\n",
      "M.Loss = 0.6858866810798645 | M.Accuracy = 0.5452678799629211\n",
      "M.Loss = 0.6687422394752502 | M.Accuracy = 0.5979297161102295\n",
      "M.Loss = 0.652645468711853 | M.Accuracy = 0.6417534947395325\n",
      "M.Loss = 0.638826310634613 | M.Accuracy = 0.6774218678474426\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/jacob.su.wang/Desktop/CODER/TENSORFLOW/CNN/SAVED/cnn-test1-1000'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run model while saving\n",
    "#   1. run init\n",
    "#   2. init tf.train.Saver() object\n",
    "#   3. specify save path\n",
    "#   4. save after sess.run(..) # NB: global_step's imperative\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "NUM_BATCHES = 1000\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "save_path = '/Users/jacob.su.wang/Desktop/CODER/TENSORFLOW/CNN/SAVED/cnn-test1'\n",
    "\n",
    "for e in range(NUM_EPOCHS):\n",
    "    print('Epoch ', e+1)\n",
    "    print('\\n')\n",
    "    loss_track, accuracy_track = [], []\n",
    "    for b in range(NUM_BATCHES):\n",
    "        batch_x1, batch_x2, batch_y = generate_batch(indexer, LEN_FROM, LEN_TO, batch_size=64)\n",
    "        fd = {input_x1:batch_x1, input_x2:batch_x2, input_y:batch_y, keep_prob:0.7}\n",
    "        _, step, loss_, accuracy_ = sess.run([train_op, global_step, loss, accuracy], feed_dict=fd)\n",
    "        loss_track.append(loss_)\n",
    "        accuracy_track.append(accuracy_)\n",
    "        if step%100==0:\n",
    "            print('M.Loss = {} | M.Accuracy = {}'.format(np.mean(loss_track), np.mean(accuracy_track)))\n",
    "    print('\\n')\n",
    "    \n",
    "saver.save(sess, save_path, sess.run(global_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Restore the stuff\n",
    "#   1. init sess\n",
    "#   2. get graph using tf.train.import_meta_graph(path/to/meta)\n",
    "#   3. use obj created in step 2 to restore from check point\n",
    "#   4. build a graph using tf.get_default_graph()\n",
    "#   5. read input-end tensors (the shit you create feed_dict by) # don't forget naming.\n",
    "#   6. read output-end tensors # don't forget naming.\n",
    "#   7. run output-end tensor as usual\n",
    "\n",
    "sess = tf.Session()\n",
    "saver = tf.train.import_meta_graph(save_path+'-1000.meta')\n",
    "saver.restore(sess, tf.train.latest_checkpoint('/Users/jacob.su.wang/Desktop/CODER/TENSORFLOW/CNN/SAVED/'))\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "input_x1 = graph.get_tensor_by_name('input_x1:0')\n",
    "input_x2 = graph.get_tensor_by_name('input_x2:0')\n",
    "keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "test_x1 = [['motorcycle', 'train', 'motorcycle', 'car', 'car', 'car', 'train']]\n",
    "test_x2 = [['motorcycle', 'train', 'train', 'car', 'bus']]\n",
    "test_x3 = [['cat', 'cat', 'deer', 'deer', 'pig', 'dog', 'dog', 'deer', 'dog']]\n",
    "def to_code(x_):\n",
    "    return [[indexer.get_index(elem) for elem in pad_sentence(MAX_LEN, x_[0])]]\n",
    "fd = {input_x1:to_code(test_x1), input_x2:to_code(test_x2), keep_prob:1.0}\n",
    "\n",
    "predictions = graph.get_tensor_by_name('predictions:0')\n",
    "print(sess.run(predictions, feed_dict=fd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cust. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test x1 = x2; test x3 in another group.\n",
    "test_x1 = [['motorcycle', 'train', 'motorcycle', 'car', 'car', 'car', 'train']]\n",
    "test_x2 = [['motorcycle', 'train', 'train', 'car', 'bus']]\n",
    "test_x3 = [['cat', 'cat', 'deer', 'deer', 'pig', 'dog', 'dog', 'deer', 'dog']]\n",
    "test_y12 = [1]\n",
    "test_y13 = [0]\n",
    "test_y23 = [0]\n",
    "\n",
    "def to_code(x_):\n",
    "    return [[indexer.get_index(elem) for elem in pad_sentence(MAX_LEN, x_[0])]]\n",
    "\n",
    "def predict(x1_, x2_, y_, sess_):\n",
    "    fd = {input_x1:to_code(x1_), input_x2:to_code(x2_), input_y:test_y12, keep_prob:1.0}\n",
    "    pred, conf = sess_.run([predictions, scores], feed_dict=fd)\n",
    "    print('True = {} | Pred = {} (conf: {})'.format(y_, pred, conf if y_==[1] else 1-conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True = [1] | Pred = [1] (conf: [ 0.98743469])\n",
      "True = [0] | Pred = [0] (conf: [ 0.99620718])\n",
      "True = [0] | Pred = [0] (conf: [ 0.99533159])\n"
     ]
    }
   ],
   "source": [
    "predict(test_x1, test_x2, test_y12, sess)\n",
    "predict(test_x1, test_x3, test_y13, sess)\n",
    "predict(test_x2, test_x3, test_y23, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
