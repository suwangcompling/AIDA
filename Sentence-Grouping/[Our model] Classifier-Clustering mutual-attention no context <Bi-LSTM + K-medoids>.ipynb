{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/work/04233/sw33286/AIDA-SCRIPTS\")\n",
    "sys.path.insert(0, \"/home/04233/sw33286/AIDA-package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell, MultiRNNCell, DropoutWrapper\n",
    "\n",
    "from kmedoids import kMedoids\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from helpers import Indexer, batch\n",
    "from itertools import chain, product\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100001, 300)\n"
     ]
    }
   ],
   "source": [
    "# Link to NYT data folder\n",
    "\n",
    "nyt_code_dir = \"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_code/\"\n",
    "FILE_NAMES = os.listdir(nyt_code_dir)\n",
    "\n",
    "# Link to dictionary information\n",
    "\n",
    "info_path = \"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_info/indexer_word2emb_100k.p\"\n",
    "indexer100k, word2emb100k = dill.load(open(info_path, 'rb'))\n",
    "glove_embs = []\n",
    "for i in range(len(indexer100k)):\n",
    "    glove_embs.append(word2emb100k[indexer100k.get_object(i)])\n",
    "glove_embs = np.array(glove_embs)\n",
    "print(glove_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data batching\n",
    "\n",
    "def batch_doc_mix(doc_a_code, doc_b_code, k=25, max_doc=500):\n",
    "    batch_x1, batch_x2, batch_ctx, batch_y = [], [], [], []\n",
    "    ys = [1,0,0,1]\n",
    "    for _ in range(k): # 4 entries added per iteration.\n",
    "        for i,(da,db) in enumerate(product([doc_a_code, doc_b_code], \n",
    "                                           [doc_a_code, doc_b_code])):\n",
    "            batch_x1.append(random.choice(da))\n",
    "            batch_x2.append(random.choice(db))\n",
    "            batch_y.append(ys[i])\n",
    "    return batch_x1, batch_x2, batch_y\n",
    "\n",
    "def get_batch(file_idx):\n",
    "    filename = FILE_NAMES[file_idx]\n",
    "    doc_a_code, doc_b_code, salad_code = dill.load(open(nyt_code_dir+filename, 'rb'))\n",
    "    batch_x1, batch_x2, batch_y = batch_doc_mix(doc_a_code, doc_b_code)\n",
    "    return batch_x1, batch_x2, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "VOCAB_SIZE = len(indexer100k)\n",
    "EMB_SIZE = glove_embs.shape[1]\n",
    "HID_SIZE = 100\n",
    "NUM_LAYERS = 2\n",
    "KEEP_PROB = 0.7\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "input_x1 = tf.placeholder(tf.int32, [None, None], name='input_x1') # <max-time, batch-size>\n",
    "input_x2 = tf.placeholder(tf.int32, [None, None], name='input_x2')\n",
    "input_x1_length = tf.placeholder(tf.int32, [None], name='input_x1_length')\n",
    "input_x2_length = tf.placeholder(tf.int32, [None], name='input_x2_length')\n",
    "input_y  = tf.placeholder(tf.int32, [None], name='input_y')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "with tf.variable_scope('Embeddings'):\n",
    "    embeddings = tf.get_variable('embeddings', glove_embs.shape, \n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "    glove_init = embeddings.assign(glove_embs)\n",
    "    input_x1_embedded = tf.nn.embedding_lookup(embeddings, input_x1) # <max-time, batch-size, emb-size>\n",
    "    input_x2_embedded = tf.nn.embedding_lookup(embeddings, input_x2)\n",
    "\n",
    "cell = MultiRNNCell([DropoutWrapper(LSTMCell(HID_SIZE),output_keep_prob=keep_prob)]*NUM_LAYERS) \n",
    "\n",
    "def run_lstm(cell, inputs, inputs_length): # lstm-out size *= 2 by bidirectionality.\n",
    "    ((fw_outputs,bw_outputs), # <max-time, batch-size, hid-size>, attention later if needed.\n",
    "     (fw_final_state,bw_final_state)) = ( # <batch-size, hid-size>\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw=cell,\n",
    "                                        cell_bw=cell,\n",
    "                                        inputs=inputs,\n",
    "                                        sequence_length=inputs_length,\n",
    "                                        dtype=tf.float32, time_major=True)\n",
    "    )\n",
    "    return tf.concat([tf.concat([fw_state_tuple.h,bw_state_tuple.h], 1) # lstm-out size *= NUM_LAYERS by stacking.\n",
    "                      for fw_state_tuple,bw_state_tuple in zip(fw_final_state,bw_final_state)], 1), \\\n",
    "           tf.squeeze(tf.concat([fw_outputs,bw_outputs], 2), 1)\n",
    "        # op1: <batch-size, hid-size*2*num-layers>\n",
    "        # op2: <max-time, batch-size, hid-size*2> -> <max-time, hid-size*2>\n",
    "        # NB: batch-size = 1 here.\n",
    "           \n",
    "with tf.variable_scope('Bi-LSTM') as scope:\n",
    "    final_state_x1, outputs_x1 = run_lstm(cell, input_x1_embedded, input_x1_length)\n",
    "    scope.reuse_variables() # both sentence inputs share the same weights.\n",
    "    final_state_x2, outputs_x2 = run_lstm(cell, input_x2_embedded, input_x2_length)\n",
    "\n",
    "def run_attention(d_mat, s_vec):\n",
    "    W_d = tf.get_variable('W_d', [HID_SIZE*2, HID_SIZE*2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    W_s = tf.get_variable('W_s', [HID_SIZE*2*NUM_LAYERS, HID_SIZE*2], \n",
    "                          initializer=tf.contrib.layers.xavier_initializer())\n",
    "    d_W = tf.matmul(d_mat, W_d) # <max-time,hid-size*2> * <hid-size*2,hid-size*2> = <max-time,20>\n",
    "    s_W = tf.squeeze(tf.matmul(s_vec, W_s), 0) \n",
    "        # <1,hid-size*2> * <hid-size*2,hid-size*2> = <1,hid-size*2> -> <hid-size*2,>\n",
    "    a_mat = tf.nn.tanh(tf.add(d_W, s_W)) # <max-time,hid-size*2>\n",
    "    W_a = tf.get_variable('W_a', [HID_SIZE*2, 1], initializer=tf.contrib.layers.xavier_initializer()) # <hid-size*2,1>\n",
    "    a_W = tf.transpose(tf.matmul(a_mat, W_a), [1,0]) # <1,max-time>\n",
    "    d_a = tf.matmul(a_W, d_mat) # <1,hid-size*2>\n",
    "    return d_a    \n",
    "\n",
    "with tf.variable_scope('Mutual-Attention') as scope:\n",
    "    x1_to_x2_attvec = run_attention(outputs_x2, final_state_x1) # x1 attending to x2\n",
    "    scope.reuse_variables()\n",
    "    x2_to_x1_attvec = run_attention(outputs_x1, final_state_x2) # x2 attending to x1\n",
    "\n",
    "def run_scores(fs_x1, fs_x2, av_12, av_21):\n",
    "    W_bi = tf.get_variable('W_bi', [HID_SIZE*2*NUM_LAYERS+HID_SIZE*2, HID_SIZE*2*NUM_LAYERS+HID_SIZE*2], \n",
    "                           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    fv_x1 = tf.concat([fs_x1,av_12],axis=1) # <1,40>\n",
    "    fv_x2 = tf.concat([fs_x2,av_21],axis=1)\n",
    "    return tf.nn.sigmoid(tf.diag_part(tf.matmul(tf.matmul(fv_x1,W_bi),tf.transpose(fv_x2))),name='scores')\n",
    "    \n",
    "scores = run_scores(final_state_x1, final_state_x2, x1_to_x2_attvec, x2_to_x1_attvec)\n",
    "predictions = tf.cast(tf.round(scores), tf.int32, name='predictions')     \n",
    "    \n",
    "with tf.name_scope('Loss'):\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(input_y, tf.float32), logits=scores)\n",
    "    loss = tf.reduce_mean(losses, name='loss')\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct_predictions = tf.equal(predictions, input_y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "    \n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step, name='train_op')\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "TRAIN_SIZE = 10\n",
    "# TRAIN_SIZE = len(FILE_NAMES)\n",
    "VERBOSE = 1000\n",
    "ERROR_LOG = []\n",
    "\n",
    "loss_track, accuracy_track = [], []\n",
    "start = time.time()\n",
    "try:\n",
    "    for e in range(NUM_EPOCHS):\n",
    "        print('Epoch ', e+1)\n",
    "        print('\\n')\n",
    "        file_indices = np.random.choice(list(range(len(FILE_NAMES))), size=TRAIN_SIZE, replace=False)\n",
    "        random.shuffle(file_indices)  \n",
    "        curr_loss_track, curr_accuracy_track = [], []\n",
    "        for file_idx in file_indices:\n",
    "            try:\n",
    "                batch_x1, batch_x2, batch_y = get_batch(file_idx)\n",
    "            except:\n",
    "                ERROR_LOG.append(file_idx)\n",
    "                continue\n",
    "            for x1, x2, y in zip(batch_x1,batch_x2,batch_y):\n",
    "                x1,x1_len = batch([x1])\n",
    "                x2,x2_len = batch([x2])\n",
    "                y = [y]\n",
    "                fd = {input_x1:x1, input_x1_length:x1_len,\n",
    "                      input_x2:x2, input_x2_length:x2_len,\n",
    "                      input_y:y,\n",
    "                      keep_prob:KEEP_PROB} \n",
    "                _, step, loss_, accuracy_ = sess.run([train_op, global_step, loss, accuracy], feed_dict=fd)\n",
    "                curr_loss_track.append(loss_)\n",
    "                curr_accuracy_track.append(accuracy_)\n",
    "            if step%VERBOSE==0:\n",
    "                print('  batch loss & accuracy at step {}: <{}, {}> (time elapsed = {})'.format(step, loss_, accuracy_,\n",
    "                                                                                                time.time()-start))\n",
    "                start = time.time()\n",
    "        print('\\n')\n",
    "        print('  epoch mean loss & accuracy: <{}, {}>'.format(np.mean(curr_loss_track),np.mean(curr_accuracy_track)))\n",
    "        print('\\n')    \n",
    "        loss_track += curr_loss_track\n",
    "        accuracy_track += curr_accuracy_track \n",
    "except KeyboardInterrupt:\n",
    "    print('Stopped!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_files(target_dir):\n",
    "    for filename in os.listdir(target_dir):\n",
    "        os.remove(os.path.abspath(os.path.join(target_dir, filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/04233/sw33286/AIDA-INDIV-MODEL-SAVE/our-model-no-context-mutual-attention-kmedoids/our-model-no-context-mutual-attention-00'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = \"/work/04233/sw33286/AIDA-INDIV-MODEL-SAVE/our-model-no-context-mutual-attention-kmedoids/\"\n",
    "save_path = save_dir + \"our-model-no-context-mutual-attention-00\"\n",
    "remove_all_files(save_dir)\n",
    "saver.save(sess, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM + HAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "\n",
    "def get_rand_mixture():\n",
    "    filename = random.choice(FILE_NAMES)\n",
    "    da,db, doc_mix = dill.load(open(nyt_code_dir+filename, 'rb'))\n",
    "    doc_lbs = []\n",
    "    for sentcode in doc_mix:\n",
    "        if sentcode in da:\n",
    "            doc_lbs.append(0)\n",
    "        else:\n",
    "            doc_lbs.append(1)\n",
    "    return doc_mix, doc_lbs\n",
    "\n",
    "def to_labels(C, doc_len): # C: {cls:[datum_id, ...], ...}\n",
    "    lbs = [0]*doc_len\n",
    "    for idx in C[1]:\n",
    "        lbs[idx] = 1\n",
    "    return lbs\n",
    "\n",
    "def flip_clust(clust):\n",
    "    return np.array([0 if i==1 else 1 for i in clust])\n",
    "\n",
    "def clust_accuracy(true, pred):\n",
    "    return max(accuracy_score(true, pred),\n",
    "               accuracy_score(true, flip_clust(pred)))\n",
    "\n",
    "def to_sent(code):\n",
    "    return [indexer100k.get_object(idx) for idx in code]\n",
    "\n",
    "# Bi-LSTM + HAC class\n",
    "\n",
    "class ClfKM:\n",
    "    \n",
    "    def __init__(self, clf_dir, clf_filename):\n",
    "        self.sess = tf.Session()\n",
    "        saver = tf.train.import_meta_graph(clf_dir + clf_filename)\n",
    "        saver.restore(self.sess, tf.train.latest_checkpoint(clf_dir))\n",
    "        self.graph = tf.get_default_graph()\n",
    "        self.input_x1 = self.graph.get_tensor_by_name('input_x1:0')\n",
    "        self.input_x2 = self.graph.get_tensor_by_name('input_x2:0')\n",
    "        self.input_x1_length = self.graph.get_tensor_by_name('input_x1_length:0')\n",
    "        self.input_x2_length = self.graph.get_tensor_by_name('input_x2_length:0')\n",
    "        self.input_y = self.graph.get_tensor_by_name('input_y:0')\n",
    "        self.keep_prob = self.graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "        self.scores = self.graph.get_tensor_by_name('scores:0')\n",
    "        self.predictions = self.graph.get_tensor_by_name('predictions:0')\n",
    "        self.loss = self.graph.get_tensor_by_name('Loss/loss:0')\n",
    "        self.accuracy = self.graph.get_tensor_by_name('Accuracy/accuracy:0')\n",
    "        self.global_step = self.graph.get_tensor_by_name('global_step:0')\n",
    "        self.train_op = self.graph.get_tensor_by_name('train_op:0')     \n",
    "        \n",
    "    def dist(self, x1, x2):\n",
    "        x1, x1_len = batch([x1])\n",
    "        x2, x2_len = batch([x2])\n",
    "        fd = {self.input_x1:x1, self.input_x1_length:x1_len,\n",
    "              self.input_x2:x2, self.input_x2_length:x2_len,\n",
    "              self.keep_prob:1.0}\n",
    "        conf = self.sess.run(self.scores, feed_dict=fd)\n",
    "        return 1-conf[0]\n",
    "    \n",
    "    def evaluate(self, doc_mix, doc_lbs, method='average', plot=True):\n",
    "        doc_mix_sq, _ = batch(doc_mix)\n",
    "        doc_mix_sq = doc_mix_sq.T\n",
    "        _, doc_mix_clust = kMedoids(squareform(pdist(doc_mix_sq,metric=self.dist)), 2)\n",
    "        doc_prd = to_labels(doc_mix_clust, len(doc_mix))\n",
    "        acc = clust_accuracy(doc_lbs, doc_prd)\n",
    "        return acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_dir = \"/work/04233/sw33286/AIDA-INDIV-MODEL-SAVE/our-model-no-context-mutual-attention-kmedoids/\"\n",
    "restore_filename = \"our-model-no-context-mutual-attention-00.meta\"\n",
    "clf_km = ClfKM(restore_dir, restore_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_km.evaluate(*get_rand_mixture())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_evaluation(k=100):\n",
    "    accuracies = []\n",
    "    for _ in range(k):\n",
    "        _, acc = clf_km.evaluate(*get_rand_mixture(), plot=False)\n",
    "        accuracies.append(acc)\n",
    "    print('Average clustering accuracy over {} samples = {}'.format(k, np.mean(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rand_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
