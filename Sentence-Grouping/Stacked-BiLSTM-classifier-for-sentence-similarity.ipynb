{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/work/04233/sw33286/AIDA-SCRIPTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell, MultiRNNCell, DropoutWrapper\n",
    "\n",
    "from helpers import Indexer, batch\n",
    "from itertools import chain, product\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARED_SIZE = 2 # size of noise (or, common vocab for all types).\n",
    "\n",
    "TYPES = ['ANIMAL','VEHICLE','NATURE','FURNITURE','FRUIT']\n",
    "SHARED_VOCAB = ['share'+str(i+1) for i in range(SHARED_SIZE)]\n",
    "TYPE2VOCAB = {'ANIMAL': ['cat','dog','pig','horse','deer']            + SHARED_VOCAB,\n",
    "              'VEHICLE': ['car','bike','motorcycle','train','bus']    + SHARED_VOCAB,\n",
    "              'NATURE': ['hill','mountain','lake','river','valley']   + SHARED_VOCAB,\n",
    "              'FURNITURE': ['stool','table','closet','cabinet','bed'] + SHARED_VOCAB,\n",
    "              'FRUIT': ['apple','pear','strawberry','grape','tomato'] + SHARED_VOCAB}\n",
    "VOCAB = list(chain.from_iterable(TYPE2VOCAB.values()))\n",
    "\n",
    "indexer = Indexer()\n",
    "indexer.get_index('PAD')\n",
    "for word in VOCAB:\n",
    "    indexer.get_index(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_LEN = 5\n",
    "SENT_FROM_LEN = 5\n",
    "SENT_TO_LEN = 15\n",
    "\n",
    "def to_sent(code):\n",
    "    return [indexer.get_object(idx) for idx in code]\n",
    "\n",
    "def get_rand_sent_code(sem_type, sent_len):\n",
    "    return [indexer.get_index(np.random.choice(TYPE2VOCAB[sem_type])) for _ in range(sent_len)]\n",
    "\n",
    "def get_mixture(type1, type2):\n",
    "    doc_a = [get_rand_sent_code(type1, np.random.randint(SENT_FROM_LEN, SENT_TO_LEN)) for _ in range(DOC_LEN)]\n",
    "    doc_b = [get_rand_sent_code(type2, np.random.randint(SENT_FROM_LEN, SENT_TO_LEN)) for _ in range(DOC_LEN)]\n",
    "    doc_mix = np.array(doc_a[:] + doc_b[:])\n",
    "    doc_lbs = np.array([0]*DOC_LEN + [1]*DOC_LEN)\n",
    "    indices = list(range(DOC_LEN*2))\n",
    "    random.shuffle(indices)\n",
    "    doc_mix = doc_mix[indices]\n",
    "    doc_lbs = doc_lbs[indices]\n",
    "    return doc_a, doc_b, doc_mix, doc_lbs\n",
    "    \n",
    "def batch_mixture(doc_a, doc_b, k):\n",
    "    batch_x1, batch_x2, batch_y = [], [], []\n",
    "    ys = [1,0,0,1]\n",
    "    for _ in range(k):\n",
    "        for i,(da,db) in enumerate(product([doc_a,doc_b],[doc_a,doc_b])):\n",
    "            batch_x1.append(random.choice(da))\n",
    "            batch_x2.append(random.choice(db))\n",
    "            batch_y.append(ys[i])\n",
    "    return batch(batch_x1), batch(batch_x2), np.array(batch_y)\n",
    "\n",
    "def get_batch(n=40):\n",
    "    if n%4!=0:\n",
    "        raise ValueError('The current generation scheme only supports multiples of 4 for batch size!')\n",
    "    type1, type2 = np.random.choice(TYPES, 2, replace=False)\n",
    "    doc_a, doc_b, _, _ = get_mixture(type1, type2) # document mixtures and labels aren't germane here.\n",
    "    (batch_x1,batch_x1_len), (batch_x2,batch_x2_len), batch_y = batch_mixture(doc_a,doc_b,n//4)\n",
    "    return batch_x1,batch_x1_len,batch_x2,batch_x2_len,batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "VOCAB_SIZE = len(indexer)\n",
    "EMB_SIZE = 20\n",
    "HID_SIZE = 10\n",
    "NUM_LAYERS = 2\n",
    "KEEP_PROB = 0.7\n",
    "\n",
    "input_x1 = tf.placeholder(tf.int32, [None, None], name='input_x1') # <max-time, batch-size>\n",
    "input_x2 = tf.placeholder(tf.int32, [None, None], name='input_x2')\n",
    "input_x1_length = tf.placeholder(tf.int32, [None], name='input_x1_length')\n",
    "input_x2_length = tf.placeholder(tf.int32, [None], name='input_x2_length')\n",
    "input_y  = tf.placeholder(tf.int32, [None], name='input_y')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "with tf.variable_scope('Embeddings'):\n",
    "    embeddings = tf.get_variable('embeddings', [VOCAB_SIZE, EMB_SIZE], \n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "    input_x1_embedded = tf.nn.embedding_lookup(embeddings, input_x1) # <max-time, batch-size, emb-size>\n",
    "    input_x2_embedded = tf.nn.embedding_lookup(embeddings, input_x2)\n",
    "\n",
    "cell = MultiRNNCell([DropoutWrapper(LSTMCell(HID_SIZE),output_keep_prob=keep_prob)]*NUM_LAYERS) \n",
    "\n",
    "def run_lstm(cell, inputs, inputs_length): # lstm-out size *= 2 by bidirectionality.\n",
    "    ((fw_outputs,bw_outputs), # <max-time, batch-size, hid-size>, attention later if needed.\n",
    "     (fw_final_state,bw_final_state)) = ( # <batch-size, hid-size>\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw=cell,\n",
    "                                        cell_bw=cell,\n",
    "                                        inputs=inputs,\n",
    "                                        sequence_length=inputs_length,\n",
    "                                        dtype=tf.float32, time_major=True)\n",
    "    )\n",
    "    return tf.concat([tf.concat([fw_state_tuple.h,bw_state_tuple.h], 1) # lstm-out size *= NUM_LAYERS by stacking.\n",
    "                      for fw_state_tuple,bw_state_tuple in zip(fw_final_state,bw_final_state)], 1)\n",
    "    \n",
    "with tf.variable_scope('Bi-LSTM') as scope:\n",
    "    final_state_x1 = run_lstm(cell, input_x1_embedded, input_x1_length)\n",
    "    scope.reuse_variables() # both sentence inputs share the same weights.\n",
    "    final_state_x2 = run_lstm(cell, input_x2_embedded, input_x2_length)\n",
    "    \n",
    "lstm_out_size = HID_SIZE * 2 * NUM_LAYERS\n",
    "W_bi = tf.get_variable('W_bi', [lstm_out_size, lstm_out_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "scores = tf.nn.sigmoid(tf.diag_part(tf.matmul(tf.matmul(final_state_x1,W_bi),tf.transpose(final_state_x2))),name='scores')\n",
    "predictions = tf.cast(tf.round(scores), tf.int32, name='predictions') \n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(input_y, tf.float32), logits=scores)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct_predictions = tf.equal(predictions, input_y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "    \n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(1e-5)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step, name='train_op')\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "NUM_BATCHES = 1000\n",
    "VERBOSE = 100\n",
    "\n",
    "loss_track, accuracy_track = [], []\n",
    "for e in range(NUM_EPOCHS):\n",
    "    print('Epoch ', e+1)\n",
    "    print('\\n')\n",
    "    curr_loss_track, curr_accuracy_track = [], []\n",
    "    for _ in range(NUM_BATCHES):\n",
    "        batch_x1, batch_x1_length, batch_x2, batch_x2_length, batch_y = get_batch()\n",
    "        fd = {input_x1:batch_x1, input_x1_length:batch_x1_length,\n",
    "              input_x2:batch_x2, input_x2_length:batch_x2_length,\n",
    "              input_y:batch_y,\n",
    "              keep_prob:KEEP_PROB}\n",
    "        _, step, loss_, accuracy_ = sess.run([train_op, global_step, loss, accuracy], feed_dict=fd)\n",
    "        curr_loss_track.append(loss_)\n",
    "        curr_accuracy_track.append(accuracy_)\n",
    "        if step%VERBOSE==0:\n",
    "            print('  batch loss & accuracy at step {}: <{}, {}>'.format(step, loss_, accuracy_))\n",
    "    print('\\n')\n",
    "    print('  epoch mean loss & accuracy: <{}, {}>'.format(np.mean(curr_loss_track),np.mean(curr_accuracy_track)))\n",
    "    print('\\n')    \n",
    "    loss_track += curr_loss_track\n",
    "    accuracy_track += curr_accuracy_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/04233/sw33286/AIDA-tests/MODEL-SAVE/Bi-LSTM-HAC-wo-context/stacked-bilstm-doc-mix-mock-00'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "save_dir = \"/home/04233/sw33286/AIDA-tests/MODEL-SAVE/Bi-LSTM-HAC-wo-context/\"\n",
    "save_path = save_dir + \"stacked-bilstm-doc-mix-mock-00\"\n",
    "saver.save(sess, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore & Cont'd Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/work/04233/sw33286/AIDA-SCRIPTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities\n",
    "\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell, MultiRNNCell, DropoutWrapper\n",
    "\n",
    "from helpers import Indexer, batch\n",
    "from itertools import chain, product\n",
    "\n",
    "# Data generation block\n",
    "\n",
    "SHARED_SIZE = 2 # size of noise (or, common vocab for all types).\n",
    "\n",
    "TYPES = ['ANIMAL','VEHICLE','NATURE','FURNITURE','FRUIT']\n",
    "SHARED_VOCAB = ['share'+str(i+1) for i in range(SHARED_SIZE)]\n",
    "TYPE2VOCAB = {'ANIMAL': ['cat','dog','pig','horse','deer']            + SHARED_VOCAB,\n",
    "              'VEHICLE': ['car','bike','motorcycle','train','bus']    + SHARED_VOCAB,\n",
    "              'NATURE': ['hill','mountain','lake','river','valley']   + SHARED_VOCAB,\n",
    "              'FURNITURE': ['stool','table','closet','cabinet','bed'] + SHARED_VOCAB,\n",
    "              'FRUIT': ['apple','pear','strawberry','grape','tomato'] + SHARED_VOCAB}\n",
    "VOCAB = list(chain.from_iterable(TYPE2VOCAB.values()))\n",
    "\n",
    "indexer = Indexer()\n",
    "indexer.get_index('PAD')\n",
    "for word in VOCAB:\n",
    "    indexer.get_index(word)\n",
    "    \n",
    "DOC_LEN = 5\n",
    "SENT_FROM_LEN = 5\n",
    "SENT_TO_LEN = 15\n",
    "\n",
    "def to_sent(code):\n",
    "    return [indexer.get_object(idx) for idx in code]\n",
    "\n",
    "def get_rand_sent_code(sem_type, sent_len):\n",
    "    return [indexer.get_index(np.random.choice(TYPE2VOCAB[sem_type])) for _ in range(sent_len)]\n",
    "\n",
    "def get_mixture(type1, type2):\n",
    "    doc_a = [get_rand_sent_code(type1, np.random.randint(SENT_FROM_LEN, SENT_TO_LEN)) for _ in range(DOC_LEN)]\n",
    "    doc_b = [get_rand_sent_code(type2, np.random.randint(SENT_FROM_LEN, SENT_TO_LEN)) for _ in range(DOC_LEN)]\n",
    "    doc_mix = np.array(doc_a[:] + doc_b[:])\n",
    "    doc_lbs = np.array([0]*DOC_LEN + [1]*DOC_LEN)\n",
    "    indices = list(range(DOC_LEN*2))\n",
    "    random.shuffle(indices)\n",
    "    doc_mix = doc_mix[indices]\n",
    "    doc_lbs = doc_lbs[indices]\n",
    "    return doc_a, doc_b, doc_mix, doc_lbs\n",
    "    \n",
    "def batch_mixture(doc_a, doc_b, k):\n",
    "    batch_x1, batch_x2, batch_y = [], [], []\n",
    "    ys = [1,0,0,1]\n",
    "    for _ in range(k):\n",
    "        for i,(da,db) in enumerate(product([doc_a,doc_b],[doc_a,doc_b])):\n",
    "            batch_x1.append(random.choice(da))\n",
    "            batch_x2.append(random.choice(db))\n",
    "            batch_y.append(ys[i])\n",
    "    return batch(batch_x1), batch(batch_x2), np.array(batch_y)\n",
    "\n",
    "def get_batch(n=40):\n",
    "    if n%4!=0:\n",
    "        raise ValueError('The current generation scheme only supports multiples of 4 for batch size!')\n",
    "    type1, type2 = np.random.choice(TYPES, 2, replace=False)\n",
    "    doc_a, doc_b, _, _ = get_mixture(type1, type2) # document mixtures and labels aren't germane here.\n",
    "    (batch_x1,batch_x1_len), (batch_x2,batch_x2_len), batch_y = batch_mixture(doc_a,doc_b,n//4)\n",
    "    return batch_x1,batch_x1_len,batch_x2,batch_x2_len,batch_y\n",
    "# Model restoration block\n",
    "\n",
    "restore_dir = \"/home/04233/sw33286/AIDA-tests/MODEL-SAVE/Bi-LSTM-HAC-wo-context/\"\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "saver = tf.train.import_meta_graph(restore_dir+'stacked-bilstm-doc-mix-mock-00.meta')\n",
    "saver.restore(sess, tf.train.latest_checkpoint(restore_dir))\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "VOCAB_SIZE = len(indexer)\n",
    "EMB_SIZE = 20\n",
    "HID_SIZE = 10\n",
    "NUM_LAYERS = 2\n",
    "KEEP_PROB = 0.7\n",
    "\n",
    "input_x1 = graph.get_tensor_by_name('input_x1:0')\n",
    "input_x2 = graph.get_tensor_by_name('input_x2:0')\n",
    "input_x1_length = graph.get_tensor_by_name('input_x1_length:0')\n",
    "input_x2_length = graph.get_tensor_by_name('input_x2_length:0')\n",
    "input_y = graph.get_tensor_by_name('input_y:0')\n",
    "keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "scores = graph.get_tensor_by_name('scores:0')\n",
    "predictions = graph.get_tensor_by_name('predictions:0')\n",
    "loss = graph.get_tensor_by_name('Loss/Mean:0')\n",
    "accuracy = graph.get_tensor_by_name('Accuracy/accuracy:0')\n",
    "global_step = graph.get_tensor_by_name('global_step:0')\n",
    "train_op = graph.get_tensor_by_name('train_op:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 50100: <0.5032744407653809, 1.0>\n",
      "  batch loss & accuracy at step 50200: <0.6931296586990356, 0.5>\n",
      "  batch loss & accuracy at step 50300: <0.5077998638153076, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 50400: <0.5986820459365845, 0.75>\n",
      "  batch loss & accuracy at step 50500: <0.5989395380020142, 0.75>\n",
      "  batch loss & accuracy at step 50600: <0.5032198429107666, 1.0>\n",
      "  batch loss & accuracy at step 50700: <0.5983470678329468, 0.75>\n",
      "  batch loss & accuracy at step 50800: <0.6929119825363159, 0.5>\n",
      "  batch loss & accuracy at step 50900: <0.5034803152084351, 1.0>\n",
      "  batch loss & accuracy at step 51000: <0.5988529920578003, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5833315253257751, 0.7905250191688538>\n",
      "\n",
      "\n",
      "Epoch  2\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 51100: <0.597815752029419, 0.75>\n",
      "  batch loss & accuracy at step 51200: <0.5980042219161987, 0.75>\n",
      "  batch loss & accuracy at step 51300: <0.5982168316841125, 0.75>\n",
      "  batch loss & accuracy at step 51400: <0.5987054109573364, 0.75>\n",
      "  batch loss & accuracy at step 51500: <0.5985262989997864, 0.75>\n",
      "  batch loss & accuracy at step 51600: <0.6931315660476685, 0.5>\n",
      "  batch loss & accuracy at step 51700: <0.5035843849182129, 1.0>\n",
      "  batch loss & accuracy at step 51800: <0.5038792490959167, 1.0>\n",
      "  batch loss & accuracy at step 51900: <0.5981796979904175, 0.75>\n",
      "  batch loss & accuracy at step 52000: <0.598180890083313, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5785008668899536, 0.8031749725341797>\n",
      "\n",
      "\n",
      "Epoch  3\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 52100: <0.598132312297821, 0.75>\n",
      "  batch loss & accuracy at step 52200: <0.5032176971435547, 1.0>\n",
      "  batch loss & accuracy at step 52300: <0.5981265306472778, 0.75>\n",
      "  batch loss & accuracy at step 52400: <0.5032328963279724, 1.0>\n",
      "  batch loss & accuracy at step 52500: <0.5987657308578491, 0.75>\n",
      "  batch loss & accuracy at step 52600: <0.5982262492179871, 0.75>\n",
      "  batch loss & accuracy at step 52700: <0.503235399723053, 1.0>\n",
      "  batch loss & accuracy at step 52800: <0.5981844663619995, 0.75>\n",
      "  batch loss & accuracy at step 52900: <0.5981926918029785, 0.75>\n",
      "  batch loss & accuracy at step 53000: <0.598892331123352, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.579203724861145, 0.8015249967575073>\n",
      "\n",
      "\n",
      "Epoch  4\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 53100: <0.5036660432815552, 1.0>\n",
      "  batch loss & accuracy at step 53200: <0.5980605483055115, 0.75>\n",
      "  batch loss & accuracy at step 53300: <0.5982039570808411, 0.75>\n",
      "  batch loss & accuracy at step 53400: <0.5981854200363159, 0.75>\n",
      "  batch loss & accuracy at step 53500: <0.5982288718223572, 0.75>\n",
      "  batch loss & accuracy at step 53600: <0.5032196640968323, 1.0>\n",
      "  batch loss & accuracy at step 53700: <0.5965880155563354, 0.75>\n",
      "  batch loss & accuracy at step 53800: <0.5983887910842896, 0.75>\n",
      "  batch loss & accuracy at step 53900: <0.5982060432434082, 0.75>\n",
      "  batch loss & accuracy at step 54000: <0.6931491494178772, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5811848640441895, 0.7960250377655029>\n",
      "\n",
      "\n",
      "Epoch  5\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 54100: <0.5981910228729248, 0.75>\n",
      "  batch loss & accuracy at step 54200: <0.603176474571228, 0.75>\n",
      "  batch loss & accuracy at step 54300: <0.5986084342002869, 0.75>\n",
      "  batch loss & accuracy at step 54400: <0.5037387609481812, 1.0>\n",
      "  batch loss & accuracy at step 54500: <0.6931625604629517, 0.5>\n",
      "  batch loss & accuracy at step 54600: <0.5983166694641113, 0.75>\n",
      "  batch loss & accuracy at step 54700: <0.6930997967720032, 0.5>\n",
      "  batch loss & accuracy at step 54800: <0.5982041954994202, 0.75>\n",
      "  batch loss & accuracy at step 54900: <0.5982844233512878, 0.75>\n",
      "  batch loss & accuracy at step 55000: <0.5990747809410095, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5813692808151245, 0.7950999736785889>\n",
      "\n",
      "\n",
      "Epoch  6\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 55100: <0.5033170580863953, 1.0>\n",
      "  batch loss & accuracy at step 55200: <0.5032286643981934, 1.0>\n",
      "  batch loss & accuracy at step 55300: <0.6139673590660095, 0.699999988079071>\n",
      "  batch loss & accuracy at step 55400: <0.5032045841217041, 1.0>\n",
      "  batch loss & accuracy at step 55500: <0.5984394550323486, 0.75>\n",
      "  batch loss & accuracy at step 55600: <0.5074080228805542, 1.0>\n",
      "  batch loss & accuracy at step 55700: <0.5036001801490784, 1.0>\n",
      "  batch loss & accuracy at step 55800: <0.5139763951301575, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 55900: <0.6931325197219849, 0.5>\n",
      "  batch loss & accuracy at step 56000: <0.5982827544212341, 0.75>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5796006321907043, 0.7997249960899353>\n",
      "\n",
      "\n",
      "Epoch  7\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 56100: <0.6931420564651489, 0.5>\n",
      "  batch loss & accuracy at step 56200: <0.6931095719337463, 0.5>\n",
      "  batch loss & accuracy at step 56300: <0.5958511829376221, 0.75>\n",
      "  batch loss & accuracy at step 56400: <0.5982199907302856, 0.75>\n",
      "  batch loss & accuracy at step 56500: <0.5982102751731873, 0.75>\n",
      "  batch loss & accuracy at step 56600: <0.5990223288536072, 0.75>\n",
      "  batch loss & accuracy at step 56700: <0.5034483075141907, 1.0>\n",
      "  batch loss & accuracy at step 56800: <0.6024798154830933, 0.7250000238418579>\n",
      "  batch loss & accuracy at step 56900: <0.5037685036659241, 1.0>\n",
      "  batch loss & accuracy at step 57000: <0.5037038326263428, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5807299017906189, 0.7966500520706177>\n",
      "\n",
      "\n",
      "Epoch  8\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 57100: <0.597995400428772, 0.75>\n",
      "  batch loss & accuracy at step 57200: <0.5984050035476685, 0.75>\n",
      "  batch loss & accuracy at step 57300: <0.5990279912948608, 0.75>\n",
      "  batch loss & accuracy at step 57400: <0.5033739805221558, 1.0>\n",
      "  batch loss & accuracy at step 57500: <0.5982453227043152, 0.75>\n",
      "  batch loss & accuracy at step 57600: <0.5663408041000366, 0.824999988079071>\n",
      "  batch loss & accuracy at step 57700: <0.6952115297317505, 0.5>\n",
      "  batch loss & accuracy at step 57800: <0.5059982538223267, 1.0>\n",
      "  batch loss & accuracy at step 57900: <0.5033557415008545, 1.0>\n",
      "  batch loss & accuracy at step 58000: <0.5448547601699829, 0.875>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5709371566772461, 0.8220250010490417>\n",
      "\n",
      "\n",
      "Epoch  9\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 58100: <0.5032115578651428, 1.0>\n",
      "  batch loss & accuracy at step 58200: <0.5966161489486694, 0.75>\n",
      "  batch loss & accuracy at step 58300: <0.5181018114089966, 0.949999988079071>\n",
      "  batch loss & accuracy at step 58400: <0.596498966217041, 0.75>\n",
      "  batch loss & accuracy at step 58500: <0.5035960674285889, 1.0>\n",
      "  batch loss & accuracy at step 58600: <0.5233612060546875, 1.0>\n",
      "  batch loss & accuracy at step 58700: <0.5035095810890198, 1.0>\n",
      "  batch loss & accuracy at step 58800: <0.5325804948806763, 0.925000011920929>\n",
      "  batch loss & accuracy at step 58900: <0.5419880747795105, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 59000: <0.5207726359367371, 0.925000011920929>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5569009780883789, 0.8725500106811523>\n",
      "\n",
      "\n",
      "Epoch  10\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 59100: <0.5035669803619385, 1.0>\n",
      "  batch loss & accuracy at step 59200: <0.5779112577438354, 0.8250000476837158>\n",
      "  batch loss & accuracy at step 59300: <0.5206131339073181, 0.949999988079071>\n",
      "  batch loss & accuracy at step 59400: <0.5082381963729858, 1.0>\n",
      "  batch loss & accuracy at step 59500: <0.5095636248588562, 1.0>\n",
      "  batch loss & accuracy at step 59600: <0.5032049417495728, 1.0>\n",
      "  batch loss & accuracy at step 59700: <0.5075875520706177, 1.0>\n",
      "  batch loss & accuracy at step 59800: <0.551076352596283, 0.949999988079071>\n",
      "  batch loss & accuracy at step 59900: <0.567646861076355, 0.8250000476837158>\n",
      "  batch loss & accuracy at step 60000: <0.5308974981307983, 0.9749999642372131>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5448713302612305, 0.9111500382423401>\n",
      "\n",
      "\n",
      "Epoch  11\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 60100: <0.5102592706680298, 1.0>\n",
      "  batch loss & accuracy at step 60200: <0.5032044649124146, 1.0>\n",
      "  batch loss & accuracy at step 60300: <0.5457663536071777, 0.8500000238418579>\n",
      "  batch loss & accuracy at step 60400: <0.8209161758422852, 0.375>\n",
      "  batch loss & accuracy at step 60500: <0.5034400224685669, 1.0>\n",
      "  batch loss & accuracy at step 60600: <0.5312293171882629, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 60700: <0.5069800019264221, 1.0>\n",
      "  batch loss & accuracy at step 60800: <0.5040451288223267, 1.0>\n",
      "  batch loss & accuracy at step 60900: <0.5045552253723145, 1.0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch loss & accuracy at step 61000: <0.5513230562210083, 0.875>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.542340099811554, 0.926099956035614>\n",
      "\n",
      "\n",
      "Epoch  12\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 61100: <0.5195810794830322, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 61200: <0.5199826955795288, 0.949999988079071>\n",
      "  batch loss & accuracy at step 61300: <0.7998103499412537, 0.4749999940395355>\n",
      "  batch loss & accuracy at step 61400: <0.5045239329338074, 1.0>\n",
      "  batch loss & accuracy at step 61500: <0.5379450917243958, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 61600: <0.5104652047157288, 1.0>\n",
      "  batch loss & accuracy at step 61700: <0.5066905617713928, 1.0>\n",
      "  batch loss & accuracy at step 61800: <0.5364363789558411, 0.875>\n",
      "  batch loss & accuracy at step 61900: <0.5584036707878113, 0.824999988079071>\n",
      "  batch loss & accuracy at step 62000: <0.781929612159729, 0.5000000596046448>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5440362691879272, 0.9257500171661377>\n",
      "\n",
      "\n",
      "Epoch  13\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 62100: <0.5032312870025635, 1.0>\n",
      "  batch loss & accuracy at step 62200: <0.5056149959564209, 1.0>\n",
      "  batch loss & accuracy at step 62300: <0.5043463706970215, 1.0>\n",
      "  batch loss & accuracy at step 62400: <0.504387617111206, 1.0>\n",
      "  batch loss & accuracy at step 62500: <0.5138770937919617, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 62600: <0.7847051024436951, 0.44999998807907104>\n",
      "  batch loss & accuracy at step 62700: <0.523851752281189, 0.949999988079071>\n",
      "  batch loss & accuracy at step 62800: <0.5079214572906494, 1.0>\n",
      "  batch loss & accuracy at step 62900: <0.8132078051567078, 0.5>\n",
      "  batch loss & accuracy at step 63000: <0.5033590197563171, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5421320796012878, 0.9320499897003174>\n",
      "\n",
      "\n",
      "Epoch  14\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 63100: <0.5032044649124146, 1.0>\n",
      "  batch loss & accuracy at step 63200: <0.5035840272903442, 1.0>\n",
      "  batch loss & accuracy at step 63300: <0.5039136409759521, 1.0>\n",
      "  batch loss & accuracy at step 63400: <0.5037181377410889, 1.0>\n",
      "  batch loss & accuracy at step 63500: <0.8122764825820923, 0.5>\n",
      "  batch loss & accuracy at step 63600: <0.5081811547279358, 1.0>\n",
      "  batch loss & accuracy at step 63700: <0.5081872344017029, 1.0>\n",
      "  batch loss & accuracy at step 63800: <0.5046014785766602, 1.0>\n",
      "  batch loss & accuracy at step 63900: <0.5351022481918335, 0.949999988079071>\n",
      "  batch loss & accuracy at step 64000: <0.8161612749099731, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5354923605918884, 0.9452750086784363>\n",
      "\n",
      "\n",
      "Epoch  15\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 64100: <0.5040777921676636, 1.0>\n",
      "  batch loss & accuracy at step 64200: <0.8002427816390991, 0.5>\n",
      "  batch loss & accuracy at step 64300: <0.5359066128730774, 0.8999999761581421>\n",
      "  batch loss & accuracy at step 64400: <0.5054227113723755, 1.0>\n",
      "  batch loss & accuracy at step 64500: <0.5062841773033142, 1.0>\n",
      "  batch loss & accuracy at step 64600: <0.5118106603622437, 1.0>\n",
      "  batch loss & accuracy at step 64700: <0.5065423846244812, 1.0>\n",
      "  batch loss & accuracy at step 64800: <0.5033416152000427, 1.0>\n",
      "  batch loss & accuracy at step 64900: <0.5036680102348328, 1.0>\n",
      "  batch loss & accuracy at step 65000: <0.5037139654159546, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.541908860206604, 0.9349000453948975>\n",
      "\n",
      "\n",
      "Epoch  16\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 65100: <0.508624792098999, 1.0>\n",
      "  batch loss & accuracy at step 65200: <0.5217405557632446, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 65300: <0.5058128833770752, 1.0>\n",
      "  batch loss & accuracy at step 65400: <0.5036184191703796, 1.0>\n",
      "  batch loss & accuracy at step 65500: <0.8126018643379211, 0.5>\n",
      "  batch loss & accuracy at step 65600: <0.5240992903709412, 0.949999988079071>\n",
      "  batch loss & accuracy at step 65700: <0.5032044649124146, 1.0>\n",
      "  batch loss & accuracy at step 65800: <0.5037872791290283, 1.0>\n",
      "  batch loss & accuracy at step 65900: <0.5035635828971863, 1.0>\n",
      "  batch loss & accuracy at step 66000: <0.5106734037399292, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5396087765693665, 0.9390499591827393>\n",
      "\n",
      "\n",
      "Epoch  17\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 66100: <0.8203164339065552, 0.30000001192092896>\n",
      "  batch loss & accuracy at step 66200: <0.5292280912399292, 0.925000011920929>\n",
      "  batch loss & accuracy at step 66300: <0.5034298896789551, 1.0>\n",
      "  batch loss & accuracy at step 66400: <0.5043244361877441, 1.0>\n",
      "  batch loss & accuracy at step 66500: <0.5035871267318726, 1.0>\n",
      "  batch loss & accuracy at step 66600: <0.5036684274673462, 1.0>\n",
      "  batch loss & accuracy at step 66700: <0.5168877243995667, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 66800: <0.5043925642967224, 1.0>\n",
      "  batch loss & accuracy at step 66900: <0.5042358636856079, 1.0>\n",
      "  batch loss & accuracy at step 67000: <0.503393828868866, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5327327847480774, 0.9511749744415283>\n",
      "\n",
      "\n",
      "Epoch  18\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 67100: <0.5117375254631042, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 67200: <0.5033864974975586, 1.0>\n",
      "  batch loss & accuracy at step 67300: <0.5040106773376465, 1.0>\n",
      "  batch loss & accuracy at step 67400: <0.5092691779136658, 1.0>\n",
      "  batch loss & accuracy at step 67500: <0.5069143176078796, 1.0>\n",
      "  batch loss & accuracy at step 67600: <0.8103718757629395, 0.5>\n",
      "  batch loss & accuracy at step 67700: <0.509388267993927, 1.0>\n",
      "  batch loss & accuracy at step 67800: <0.5032237768173218, 1.0>\n",
      "  batch loss & accuracy at step 67900: <0.5035819411277771, 1.0>\n",
      "  batch loss & accuracy at step 68000: <0.5036165714263916, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5409015417098999, 0.9379000067710876>\n",
      "\n",
      "\n",
      "Epoch  19\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 68100: <0.5032052993774414, 1.0>\n",
      "  batch loss & accuracy at step 68200: <0.5067339539527893, 1.0>\n",
      "  batch loss & accuracy at step 68300: <0.5032045841217041, 1.0>\n",
      "  batch loss & accuracy at step 68400: <0.8064546585083008, 0.5>\n",
      "  batch loss & accuracy at step 68500: <0.5053613781929016, 1.0>\n",
      "  batch loss & accuracy at step 68600: <0.50333571434021, 1.0>\n",
      "  batch loss & accuracy at step 68700: <0.5037605166435242, 1.0>\n",
      "  batch loss & accuracy at step 68800: <0.5150681138038635, 1.0>\n",
      "  batch loss & accuracy at step 68900: <0.5032340288162231, 1.0>\n",
      "  batch loss & accuracy at step 69000: <0.5108031630516052, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5403664112091064, 0.9387500286102295>\n",
      "\n",
      "\n",
      "Epoch  20\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 69100: <0.5032044649124146, 1.0>\n",
      "  batch loss & accuracy at step 69200: <0.5033200979232788, 1.0>\n",
      "  batch loss & accuracy at step 69300: <0.508915901184082, 1.0>\n",
      "  batch loss & accuracy at step 69400: <0.5032055377960205, 1.0>\n",
      "  batch loss & accuracy at step 69500: <0.5071661472320557, 1.0>\n",
      "  batch loss & accuracy at step 69600: <0.8026083707809448, 0.5>\n",
      "  batch loss & accuracy at step 69700: <0.5036925077438354, 1.0>\n",
      "  batch loss & accuracy at step 69800: <0.5095476508140564, 1.0>\n",
      "  batch loss & accuracy at step 69900: <0.5048224925994873, 1.0>\n",
      "  batch loss & accuracy at step 70000: <0.5303685665130615, 0.949999988079071>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5369552969932556, 0.9451749920845032>\n",
      "\n",
      "\n",
      "Epoch  21\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 70100: <0.503355085849762, 1.0>\n",
      "  batch loss & accuracy at step 70200: <0.5098569393157959, 1.0>\n",
      "  batch loss & accuracy at step 70300: <0.5042358636856079, 1.0>\n",
      "  batch loss & accuracy at step 70400: <0.5032047629356384, 1.0>\n",
      "  batch loss & accuracy at step 70500: <0.5061054229736328, 1.0>\n",
      "  batch loss & accuracy at step 70600: <0.5032415390014648, 1.0>\n",
      "  batch loss & accuracy at step 70700: <0.5034024119377136, 1.0>\n",
      "  batch loss & accuracy at step 70800: <0.5035520792007446, 1.0>\n",
      "  batch loss & accuracy at step 70900: <0.5032789707183838, 1.0>\n",
      "  batch loss & accuracy at step 71000: <0.5032061338424683, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5372333526611328, 0.9446250200271606>\n",
      "\n",
      "\n",
      "Epoch  22\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 71100: <0.5126736760139465, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 71200: <0.503713071346283, 1.0>\n",
      "  batch loss & accuracy at step 71300: <0.5032044649124146, 1.0>\n",
      "  batch loss & accuracy at step 71400: <0.505910336971283, 1.0>\n",
      "  batch loss & accuracy at step 71500: <0.5088901519775391, 1.0>\n",
      "  batch loss & accuracy at step 71600: <0.5077372193336487, 1.0>\n",
      "  batch loss & accuracy at step 71700: <0.5051075220108032, 1.0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch loss & accuracy at step 71800: <0.5037268996238708, 1.0>\n",
      "  batch loss & accuracy at step 71900: <0.5032444000244141, 1.0>\n",
      "  batch loss & accuracy at step 72000: <0.5032317638397217, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5387770533561707, 0.9421250224113464>\n",
      "\n",
      "\n",
      "Epoch  23\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 72100: <0.8240206241607666, 0.42500001192092896>\n",
      "  batch loss & accuracy at step 72200: <0.5042980909347534, 1.0>\n",
      "  batch loss & accuracy at step 72300: <0.5035394430160522, 1.0>\n",
      "  batch loss & accuracy at step 72400: <0.5234317183494568, 0.9500000476837158>\n",
      "  batch loss & accuracy at step 72500: <0.5032566785812378, 1.0>\n",
      "  batch loss & accuracy at step 72600: <0.5211626291275024, 0.9750000238418579>\n",
      "  batch loss & accuracy at step 72700: <0.5039443969726562, 1.0>\n",
      "  batch loss & accuracy at step 72800: <0.5033490657806396, 1.0>\n",
      "  batch loss & accuracy at step 72900: <0.5195371508598328, 0.949999988079071>\n",
      "  batch loss & accuracy at step 73000: <0.5034896731376648, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5376089215278625, 0.9441249966621399>\n",
      "\n",
      "\n",
      "Epoch  24\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 73100: <0.5045740008354187, 1.0>\n",
      "  batch loss & accuracy at step 73200: <0.8132497668266296, 0.5>\n",
      "  batch loss & accuracy at step 73300: <0.5032045245170593, 1.0>\n",
      "  batch loss & accuracy at step 73400: <0.5034043788909912, 1.0>\n",
      "  batch loss & accuracy at step 73500: <0.5039199590682983, 1.0>\n",
      "  batch loss & accuracy at step 73600: <0.511736273765564, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 73700: <0.5034335851669312, 1.0>\n",
      "  batch loss & accuracy at step 73800: <0.5033626556396484, 1.0>\n",
      "  batch loss & accuracy at step 73900: <0.5032044649124146, 1.0>\n",
      "  batch loss & accuracy at step 74000: <0.5034042596817017, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.539650022983551, 0.9407250285148621>\n",
      "\n",
      "\n",
      "Epoch  25\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 74100: <0.5036841630935669, 1.0>\n",
      "  batch loss & accuracy at step 74200: <0.5032061338424683, 1.0>\n",
      "  batch loss & accuracy at step 74300: <0.8085842728614807, 0.5>\n",
      "  batch loss & accuracy at step 74400: <0.5033748149871826, 1.0>\n",
      "  batch loss & accuracy at step 74500: <0.5033847689628601, 1.0>\n",
      "  batch loss & accuracy at step 74600: <0.503713846206665, 1.0>\n",
      "  batch loss & accuracy at step 74700: <0.5032044649124146, 1.0>\n",
      "  batch loss & accuracy at step 74800: <0.503903865814209, 1.0>\n",
      "  batch loss & accuracy at step 74900: <0.5035427212715149, 1.0>\n",
      "  batch loss & accuracy at step 75000: <0.5097454786300659, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5357853770256042, 0.9470999836921692>\n",
      "\n",
      "\n",
      "Epoch  26\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 75100: <0.503338634967804, 1.0>\n",
      "  batch loss & accuracy at step 75200: <0.5032737851142883, 1.0>\n",
      "  batch loss & accuracy at step 75300: <0.5032577514648438, 1.0>\n",
      "  batch loss & accuracy at step 75400: <0.5032044649124146, 1.0>\n",
      "  batch loss & accuracy at step 75500: <0.5038945078849792, 1.0>\n",
      "  batch loss & accuracy at step 75600: <0.5056937336921692, 1.0>\n",
      "  batch loss & accuracy at step 75700: <0.503868043422699, 1.0>\n",
      "  batch loss & accuracy at step 75800: <0.5053749680519104, 1.0>\n",
      "  batch loss & accuracy at step 75900: <0.5041118264198303, 1.0>\n",
      "  batch loss & accuracy at step 76000: <0.5038207173347473, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5336669683456421, 0.9509000182151794>\n",
      "\n",
      "\n",
      "Epoch  27\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 76100: <0.5404714345932007, 0.9000000357627869>\n",
      "  batch loss & accuracy at step 76200: <0.8143500089645386, 0.5>\n",
      "  batch loss & accuracy at step 76300: <0.5089931488037109, 1.0>\n",
      "  batch loss & accuracy at step 76400: <0.5043859481811523, 1.0>\n",
      "  batch loss & accuracy at step 76500: <0.5032957792282104, 1.0>\n",
      "  batch loss & accuracy at step 76600: <0.5065761208534241, 1.0>\n",
      "  batch loss & accuracy at step 76700: <0.5033661723136902, 1.0>\n",
      "  batch loss & accuracy at step 76800: <0.5036398768424988, 1.0>\n",
      "  batch loss & accuracy at step 76900: <0.5032071471214294, 1.0>\n",
      "  batch loss & accuracy at step 77000: <0.5034351944923401, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5352689027786255, 0.9478999972343445>\n",
      "\n",
      "\n",
      "Epoch  28\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 77100: <0.504433810710907, 1.0>\n",
      "  batch loss & accuracy at step 77200: <0.5035996437072754, 1.0>\n",
      "  batch loss & accuracy at step 77300: <0.504025936126709, 1.0>\n",
      "  batch loss & accuracy at step 77400: <0.5032046437263489, 1.0>\n",
      "  batch loss & accuracy at step 77500: <0.5032352209091187, 1.0>\n",
      "  batch loss & accuracy at step 77600: <0.5205134749412537, 0.9750000238418579>\n",
      "  batch loss & accuracy at step 77700: <0.5032569766044617, 1.0>\n",
      "  batch loss & accuracy at step 77800: <0.5034492015838623, 1.0>\n",
      "  batch loss & accuracy at step 77900: <0.503848671913147, 1.0>\n",
      "  batch loss & accuracy at step 78000: <0.5032057166099548, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5340378880500793, 0.9497750401496887>\n",
      "\n",
      "\n",
      "Epoch  29\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 78100: <0.5033501386642456, 1.0>\n",
      "  batch loss & accuracy at step 78200: <0.5034774541854858, 1.0>\n",
      "  batch loss & accuracy at step 78300: <0.5047811269760132, 1.0>\n",
      "  batch loss & accuracy at step 78400: <0.5032045841217041, 1.0>\n",
      "  batch loss & accuracy at step 78500: <0.5032129287719727, 1.0>\n",
      "  batch loss & accuracy at step 78600: <0.5032049417495728, 1.0>\n",
      "  batch loss & accuracy at step 78700: <0.503524124622345, 1.0>\n",
      "  batch loss & accuracy at step 78800: <0.5049197673797607, 1.0>\n",
      "  batch loss & accuracy at step 78900: <0.5032045245170593, 1.0>\n",
      "  batch loss & accuracy at step 79000: <0.5033547282218933, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5336300134658813, 0.9506500363349915>\n",
      "\n",
      "\n",
      "Epoch  30\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 79100: <0.8132822513580322, 0.5>\n",
      "  batch loss & accuracy at step 79200: <0.5032063722610474, 1.0>\n",
      "  batch loss & accuracy at step 79300: <0.504046618938446, 1.0>\n",
      "  batch loss & accuracy at step 79400: <0.5032045841217041, 1.0>\n",
      "  batch loss & accuracy at step 79500: <0.5032044649124146, 1.0>\n",
      "  batch loss & accuracy at step 79600: <0.5041287541389465, 1.0>\n",
      "  batch loss & accuracy at step 79700: <0.5034889578819275, 1.0>\n",
      "  batch loss & accuracy at step 79800: <0.5033136606216431, 1.0>\n",
      "  batch loss & accuracy at step 79900: <0.5123006701469421, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 80000: <0.5033220648765564, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.536613404750824, 0.9455000162124634>\n",
      "\n",
      "\n",
      "Epoch  31\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 80100: <0.5043377876281738, 1.0>\n",
      "  batch loss & accuracy at step 80200: <0.5033108592033386, 1.0>\n",
      "  batch loss & accuracy at step 80300: <0.5032055377960205, 1.0>\n",
      "  batch loss & accuracy at step 80400: <0.5032044649124146, 1.0>\n",
      "  batch loss & accuracy at step 80500: <0.5032048225402832, 1.0>\n",
      "  batch loss & accuracy at step 80600: <0.5041248798370361, 1.0>\n",
      "  batch loss & accuracy at step 80700: <0.5032897591590881, 1.0>\n",
      "  batch loss & accuracy at step 80800: <0.5036510825157166, 1.0>\n",
      "  batch loss & accuracy at step 80900: <0.5208747982978821, 0.949999988079071>\n",
      "  batch loss & accuracy at step 81000: <0.5128352046012878, 0.9749999642372131>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5372126698493958, 0.9449499845504761>\n",
      "\n",
      "\n",
      "Epoch  32\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 81100: <0.5036988258361816, 1.0>\n",
      "  batch loss & accuracy at step 81200: <0.5032072067260742, 1.0>\n",
      "  batch loss & accuracy at step 81300: <0.5301886796951294, 0.925000011920929>\n",
      "  batch loss & accuracy at step 81400: <0.5041372179985046, 1.0>\n",
      "  batch loss & accuracy at step 81500: <0.5032765865325928, 1.0>\n",
      "  batch loss & accuracy at step 81600: <0.5149559378623962, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 81700: <0.5035483837127686, 1.0>\n",
      "  batch loss & accuracy at step 81800: <0.5032117962837219, 1.0>\n",
      "  batch loss & accuracy at step 81900: <0.8132562637329102, 0.5>\n",
      "  batch loss & accuracy at step 82000: <0.5033386945724487, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.536747395992279, 0.9458000659942627>\n",
      "\n",
      "\n",
      "Epoch  33\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 82100: <0.5036728382110596, 1.0>\n",
      "  batch loss & accuracy at step 82200: <0.503208339214325, 1.0>\n",
      "  batch loss & accuracy at step 82300: <0.5035524368286133, 1.0>\n",
      "  batch loss & accuracy at step 82400: <0.5032045245170593, 1.0>\n",
      "  batch loss & accuracy at step 82500: <0.5032100677490234, 1.0>\n",
      "  batch loss & accuracy at step 82600: <0.5034337639808655, 1.0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch loss & accuracy at step 82700: <0.5032255053520203, 1.0>\n",
      "  batch loss & accuracy at step 82800: <0.5037341117858887, 1.0>\n",
      "  batch loss & accuracy at step 82900: <0.5032244324684143, 1.0>\n",
      "  batch loss & accuracy at step 83000: <0.5032044649124146, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5313560962677002, 0.9544249773025513>\n",
      "\n",
      "\n",
      "Epoch  34\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 83100: <0.5033372044563293, 1.0>\n",
      "  batch loss & accuracy at step 83200: <0.5035825967788696, 1.0>\n",
      "  batch loss & accuracy at step 83300: <0.5032381415367126, 1.0>\n",
      "  batch loss & accuracy at step 83400: <0.5033731460571289, 1.0>\n",
      "  batch loss & accuracy at step 83500: <0.5032044649124146, 1.0>\n",
      "  batch loss & accuracy at step 83600: <0.5033466815948486, 1.0>\n",
      "  batch loss & accuracy at step 83700: <0.5034987926483154, 1.0>\n",
      "  batch loss & accuracy at step 83800: <0.8132626414299011, 0.5>\n",
      "  batch loss & accuracy at step 83900: <0.5032839179039001, 1.0>\n",
      "  batch loss & accuracy at step 84000: <0.5032044649124146, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5356003642082214, 0.9474000334739685>\n",
      "\n",
      "\n",
      "Epoch  35\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 84100: <0.5032089948654175, 1.0>\n",
      "  batch loss & accuracy at step 84200: <0.7891258001327515, 0.5750000476837158>\n",
      "  batch loss & accuracy at step 84300: <0.5143817067146301, 1.0>\n",
      "  batch loss & accuracy at step 84400: <0.5035894513130188, 1.0>\n",
      "  batch loss & accuracy at step 84500: <0.5032317638397217, 1.0>\n",
      "  batch loss & accuracy at step 84600: <0.5041143894195557, 1.0>\n",
      "  batch loss & accuracy at step 84700: <0.5032899379730225, 1.0>\n",
      "  batch loss & accuracy at step 84800: <0.5033169984817505, 1.0>\n",
      "  batch loss & accuracy at step 84900: <0.5033239126205444, 1.0>\n",
      "  batch loss & accuracy at step 85000: <0.8111042976379395, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5405258536338806, 0.9391500353813171>\n",
      "\n",
      "\n",
      "Epoch  36\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 85100: <0.5032087564468384, 1.0>\n",
      "  batch loss & accuracy at step 85200: <0.5032050609588623, 1.0>\n",
      "  batch loss & accuracy at step 85300: <0.8044443130493164, 0.5>\n",
      "  batch loss & accuracy at step 85400: <0.5032154321670532, 1.0>\n",
      "  batch loss & accuracy at step 85500: <0.5032952427864075, 1.0>\n",
      "  batch loss & accuracy at step 85600: <0.5033271312713623, 1.0>\n",
      "  batch loss & accuracy at step 85700: <0.5032602548599243, 1.0>\n",
      "  batch loss & accuracy at step 85800: <0.5041285753250122, 1.0>\n",
      "  batch loss & accuracy at step 85900: <0.5033356547355652, 1.0>\n",
      "  batch loss & accuracy at step 86000: <0.5032106041908264, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5343025922775269, 0.9496500492095947>\n",
      "\n",
      "\n",
      "Epoch  37\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 86100: <0.5033742189407349, 1.0>\n",
      "  batch loss & accuracy at step 86200: <0.811919629573822, 0.5>\n",
      "  batch loss & accuracy at step 86300: <0.5039147138595581, 1.0>\n",
      "  batch loss & accuracy at step 86400: <0.5068409442901611, 1.0>\n",
      "  batch loss & accuracy at step 86500: <0.5032540559768677, 1.0>\n",
      "  batch loss & accuracy at step 86600: <0.5089331865310669, 1.0>\n",
      "  batch loss & accuracy at step 86700: <0.5033605098724365, 1.0>\n",
      "  batch loss & accuracy at step 86800: <0.5034359097480774, 1.0>\n",
      "  batch loss & accuracy at step 86900: <0.5032123327255249, 1.0>\n",
      "  batch loss & accuracy at step 87000: <0.5054147243499756, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5332642197608948, 0.9512999653816223>\n",
      "\n",
      "\n",
      "Epoch  38\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 87100: <0.5032106637954712, 1.0>\n",
      "  batch loss & accuracy at step 87200: <0.505057692527771, 1.0>\n",
      "  batch loss & accuracy at step 87300: <0.5032051205635071, 1.0>\n",
      "  batch loss & accuracy at step 87400: <0.5034321546554565, 1.0>\n",
      "  batch loss & accuracy at step 87500: <0.5035398006439209, 1.0>\n",
      "  batch loss & accuracy at step 87600: <0.5032058954238892, 1.0>\n",
      "  batch loss & accuracy at step 87700: <0.5036042332649231, 1.0>\n",
      "  batch loss & accuracy at step 87800: <0.5032743811607361, 1.0>\n",
      "  batch loss & accuracy at step 87900: <0.5046951770782471, 1.0>\n",
      "  batch loss & accuracy at step 88000: <0.5032052397727966, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5381122827529907, 0.9429500102996826>\n",
      "\n",
      "\n",
      "Epoch  39\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 88100: <0.5032057762145996, 1.0>\n",
      "  batch loss & accuracy at step 88200: <0.5032322406768799, 1.0>\n",
      "  batch loss & accuracy at step 88300: <0.5040150284767151, 1.0>\n",
      "  batch loss & accuracy at step 88400: <0.5051640272140503, 1.0>\n",
      "  batch loss & accuracy at step 88500: <0.5032060146331787, 1.0>\n",
      "  batch loss & accuracy at step 88600: <0.5032045245170593, 1.0>\n",
      "  batch loss & accuracy at step 88700: <0.508876621723175, 1.0>\n",
      "  batch loss & accuracy at step 88800: <0.5041162371635437, 1.0>\n",
      "  batch loss & accuracy at step 88900: <0.5032448172569275, 1.0>\n",
      "  batch loss & accuracy at step 89000: <0.8134078979492188, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.531614363193512, 0.9540999531745911>\n",
      "\n",
      "\n",
      "Epoch  40\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 89100: <0.5035240054130554, 1.0>\n",
      "  batch loss & accuracy at step 89200: <0.5034077763557434, 1.0>\n",
      "  batch loss & accuracy at step 89300: <0.5032185912132263, 1.0>\n",
      "  batch loss & accuracy at step 89400: <0.8132433891296387, 0.5>\n",
      "  batch loss & accuracy at step 89500: <0.8128448128700256, 0.5>\n",
      "  batch loss & accuracy at step 89600: <0.5032044649124146, 1.0>\n",
      "  batch loss & accuracy at step 89700: <0.5032287836074829, 1.0>\n",
      "  batch loss & accuracy at step 89800: <0.5241701602935791, 0.949999988079071>\n",
      "  batch loss & accuracy at step 89900: <0.5033084154129028, 1.0>\n",
      "  batch loss & accuracy at step 90000: <0.8132612109184265, 0.5>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5361998081207275, 0.9463750123977661>\n",
      "\n",
      "\n",
      "Epoch  41\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 90100: <0.5032055974006653, 1.0>\n",
      "  batch loss & accuracy at step 90200: <0.5034641027450562, 1.0>\n",
      "  batch loss & accuracy at step 90300: <0.5035218000411987, 1.0>\n",
      "  batch loss & accuracy at step 90400: <0.5084288716316223, 1.0>\n",
      "  batch loss & accuracy at step 90500: <0.5197910070419312, 0.9749999642372131>\n",
      "  batch loss & accuracy at step 90600: <0.5032047033309937, 1.0>\n",
      "  batch loss & accuracy at step 90700: <0.5033597350120544, 1.0>\n",
      "  batch loss & accuracy at step 90800: <0.5034908056259155, 1.0>\n",
      "  batch loss & accuracy at step 90900: <0.5032941102981567, 1.0>\n",
      "  batch loss & accuracy at step 91000: <0.5032045245170593, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5370526313781738, 0.9451999664306641>\n",
      "\n",
      "\n",
      "Epoch  42\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 91100: <0.5032181739807129, 1.0>\n",
      "  batch loss & accuracy at step 91200: <0.5032045841217041, 1.0>\n",
      "  batch loss & accuracy at step 91300: <0.5036705136299133, 1.0>\n",
      "  batch loss & accuracy at step 91400: <0.503291666507721, 1.0>\n",
      "  batch loss & accuracy at step 91500: <0.5035090446472168, 1.0>\n",
      "  batch loss & accuracy at step 91600: <0.5032343864440918, 1.0>\n",
      "  batch loss & accuracy at step 91700: <0.5032929182052612, 1.0>\n",
      "  batch loss & accuracy at step 91800: <0.5039654970169067, 1.0>\n",
      "  batch loss & accuracy at step 91900: <0.5032044649124146, 1.0>\n",
      "  batch loss & accuracy at step 92000: <0.5032049417495728, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5375544428825378, 0.9446250200271606>\n",
      "\n",
      "\n",
      "Epoch  43\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 92100: <0.5032411813735962, 1.0>\n",
      "  batch loss & accuracy at step 92200: <0.5032212734222412, 1.0>\n",
      "  batch loss & accuracy at step 92300: <0.5034452676773071, 1.0>\n",
      "  batch loss & accuracy at step 92400: <0.5033401846885681, 1.0>\n",
      "  batch loss & accuracy at step 92500: <0.8196025490760803, 0.4749999940395355>\n",
      "  batch loss & accuracy at step 92600: <0.5032191276550293, 1.0>\n",
      "  batch loss & accuracy at step 92700: <0.5032668709754944, 1.0>\n",
      "  batch loss & accuracy at step 92800: <0.5032076239585876, 1.0>\n",
      "  batch loss & accuracy at step 92900: <0.5032060146331787, 1.0>\n",
      "  batch loss & accuracy at step 93000: <0.5032181739807129, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5296701192855835, 0.957349956035614>\n",
      "\n",
      "\n",
      "Epoch  44\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 93100: <0.5032048225402832, 1.0>\n",
      "  batch loss & accuracy at step 93200: <0.5035660862922668, 1.0>\n",
      "  batch loss & accuracy at step 93300: <0.5032166838645935, 1.0>\n",
      "  batch loss & accuracy at step 93400: <0.5032045841217041, 1.0>\n",
      "  batch loss & accuracy at step 93500: <0.5036435127258301, 1.0>\n",
      "  batch loss & accuracy at step 93600: <0.503206729888916, 1.0>\n",
      "  batch loss & accuracy at step 93700: <0.503225564956665, 1.0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch loss & accuracy at step 93800: <0.5033110976219177, 1.0>\n",
      "  batch loss & accuracy at step 93900: <0.5032103657722473, 1.0>\n",
      "  batch loss & accuracy at step 94000: <0.503207802772522, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5304788947105408, 0.9558749794960022>\n",
      "\n",
      "\n",
      "Epoch  45\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 94100: <0.5047842264175415, 1.0>\n",
      "  batch loss & accuracy at step 94200: <0.5035998225212097, 1.0>\n",
      "  batch loss & accuracy at step 94300: <0.5032224059104919, 1.0>\n",
      "  batch loss & accuracy at step 94400: <0.503242552280426, 1.0>\n",
      "  batch loss & accuracy at step 94500: <0.5033499598503113, 1.0>\n",
      "  batch loss & accuracy at step 94600: <0.5032823085784912, 1.0>\n",
      "  batch loss & accuracy at step 94700: <0.5033107995986938, 1.0>\n",
      "  batch loss & accuracy at step 94800: <0.5034437775611877, 1.0>\n",
      "  batch loss & accuracy at step 94900: <0.5032169818878174, 1.0>\n",
      "  batch loss & accuracy at step 95000: <0.5034767389297485, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5318898558616638, 0.9535749554634094>\n",
      "\n",
      "\n",
      "Epoch  46\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 95100: <0.505193293094635, 1.0>\n",
      "  batch loss & accuracy at step 95200: <0.5032050609588623, 1.0>\n",
      "  batch loss & accuracy at step 95300: <0.5032227635383606, 1.0>\n",
      "  batch loss & accuracy at step 95400: <0.5034725069999695, 1.0>\n",
      "  batch loss & accuracy at step 95500: <0.5033705234527588, 1.0>\n",
      "  batch loss & accuracy at step 95600: <0.5032049417495728, 1.0>\n",
      "  batch loss & accuracy at step 95700: <0.5032134652137756, 1.0>\n",
      "  batch loss & accuracy at step 95800: <0.5032959580421448, 1.0>\n",
      "  batch loss & accuracy at step 95900: <0.5032408833503723, 1.0>\n",
      "  batch loss & accuracy at step 96000: <0.5032045245170593, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5315065383911133, 0.9542499780654907>\n",
      "\n",
      "\n",
      "Epoch  47\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 96100: <0.5032467842102051, 1.0>\n",
      "  batch loss & accuracy at step 96200: <0.8132618069648743, 0.5>\n",
      "  batch loss & accuracy at step 96300: <0.5032075643539429, 1.0>\n",
      "  batch loss & accuracy at step 96400: <0.5032050609588623, 1.0>\n",
      "  batch loss & accuracy at step 96500: <0.5032045841217041, 1.0>\n",
      "  batch loss & accuracy at step 96600: <0.5032044649124146, 1.0>\n",
      "  batch loss & accuracy at step 96700: <0.8132343292236328, 0.5>\n",
      "  batch loss & accuracy at step 96800: <0.5032407641410828, 1.0>\n",
      "  batch loss & accuracy at step 96900: <0.8138895034790039, 0.5>\n",
      "  batch loss & accuracy at step 97000: <0.503266453742981, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5353036522865295, 0.9479250311851501>\n",
      "\n",
      "\n",
      "Epoch  48\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 97100: <0.5032046437263489, 1.0>\n",
      "  batch loss & accuracy at step 97200: <0.5046069622039795, 1.0>\n",
      "  batch loss & accuracy at step 97300: <0.5032697319984436, 1.0>\n",
      "  batch loss & accuracy at step 97400: <0.5032986998558044, 1.0>\n",
      "  batch loss & accuracy at step 97500: <0.5033027529716492, 1.0>\n",
      "  batch loss & accuracy at step 97600: <0.5032269954681396, 1.0>\n",
      "  batch loss & accuracy at step 97700: <0.5033828616142273, 1.0>\n",
      "  batch loss & accuracy at step 97800: <0.8132922053337097, 0.5>\n",
      "  batch loss & accuracy at step 97900: <0.5032153725624084, 1.0>\n",
      "  batch loss & accuracy at step 98000: <0.503283679485321, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5318642258644104, 0.9534749984741211>\n",
      "\n",
      "\n",
      "Epoch  49\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 98100: <0.5032044649124146, 1.0>\n",
      "  batch loss & accuracy at step 98200: <0.5032580494880676, 1.0>\n",
      "  batch loss & accuracy at step 98300: <0.5032289028167725, 1.0>\n",
      "  batch loss & accuracy at step 98400: <0.50320965051651, 1.0>\n",
      "  batch loss & accuracy at step 98500: <0.503291130065918, 1.0>\n",
      "  batch loss & accuracy at step 98600: <0.5032060146331787, 1.0>\n",
      "  batch loss & accuracy at step 98700: <0.503240704536438, 1.0>\n",
      "  batch loss & accuracy at step 98800: <0.5046326518058777, 1.0>\n",
      "  batch loss & accuracy at step 98900: <0.5032045245170593, 1.0>\n",
      "  batch loss & accuracy at step 99000: <0.5033201575279236, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5330799221992493, 0.9513499736785889>\n",
      "\n",
      "\n",
      "Epoch  50\n",
      "\n",
      "\n",
      "  batch loss & accuracy at step 99100: <0.8132539987564087, 0.5>\n",
      "  batch loss & accuracy at step 99200: <0.5032044649124146, 1.0>\n",
      "  batch loss & accuracy at step 99300: <0.5032048225402832, 1.0>\n",
      "  batch loss & accuracy at step 99400: <0.5228309631347656, 0.949999988079071>\n",
      "  batch loss & accuracy at step 99500: <0.5047103762626648, 1.0>\n",
      "  batch loss & accuracy at step 99600: <0.5032058954238892, 1.0>\n",
      "  batch loss & accuracy at step 99700: <0.5032244920730591, 1.0>\n",
      "  batch loss & accuracy at step 99800: <0.5037469267845154, 1.0>\n",
      "  batch loss & accuracy at step 99900: <0.5035728216171265, 1.0>\n",
      "  batch loss & accuracy at step 100000: <0.5032069683074951, 1.0>\n",
      "\n",
      "\n",
      "  epoch mean loss & accuracy: <0.5379207730293274, 0.9438000321388245>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 50\n",
    "NUM_BATCHES = 1000\n",
    "VERBOSE = 100\n",
    "\n",
    "loss_track, accuracy_track = [], []\n",
    "for e in range(NUM_EPOCHS):\n",
    "    print('Epoch ', e+1)\n",
    "    print('\\n')\n",
    "    curr_loss_track, curr_accuracy_track = [], []\n",
    "    for _ in range(NUM_BATCHES):\n",
    "        batch_x1, batch_x1_length, batch_x2, batch_x2_length, batch_y = get_batch()\n",
    "        fd = {input_x1:batch_x1, input_x1_length:batch_x1_length,\n",
    "              input_x2:batch_x2, input_x2_length:batch_x2_length,\n",
    "              input_y:batch_y,\n",
    "              keep_prob:KEEP_PROB}\n",
    "        _, step, loss_, accuracy_ = sess.run([train_op, global_step, loss, accuracy], feed_dict=fd)\n",
    "        curr_loss_track.append(loss_)\n",
    "        curr_accuracy_track.append(accuracy_)\n",
    "        if step%VERBOSE==0:\n",
    "            print('  batch loss & accuracy at step {}: <{}, {}>'.format(step, loss_, accuracy_))\n",
    "    print('\\n')\n",
    "    print('  epoch mean loss & accuracy: <{}, {}>'.format(np.mean(curr_loss_track),np.mean(curr_accuracy_track)))\n",
    "    print('\\n')    \n",
    "    loss_track += curr_loss_track\n",
    "    accuracy_track += curr_accuracy_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver = tf.train.Saver()\n",
    "# save_dir = \"/home/04233/sw33286/AIDA-tests/MODEL-SAVE/GROUPER-MOCK/\"\n",
    "# save_path = save_dir + \"stacked-bilstm-doc-mix-mock-00\"\n",
    "# shutil.rmtree(save_dir)\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "# saver.save(sess, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM + HAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class ClfHAC:\n",
    "    \n",
    "    def __init__(self, clf_dir, clf_filename):\n",
    "        self.sess = tf.Session()\n",
    "        saver = tf.train.import_meta_graph(clf_dir + clf_filename)\n",
    "        saver.restore(self.sess, tf.train.latest_checkpoint(clf_dir))\n",
    "        self.graph = tf.get_default_graph()\n",
    "        self.input_x1 = self.graph.get_tensor_by_name('input_x1:0')\n",
    "        self.input_x2 = self.graph.get_tensor_by_name('input_x2:0')\n",
    "        self.input_x1_length = self.graph.get_tensor_by_name('input_x1_length:0')\n",
    "        self.input_x2_length = self.graph.get_tensor_by_name('input_x2_length:0')\n",
    "        self.input_y = self.graph.get_tensor_by_name('input_y:0')\n",
    "        self.keep_prob = self.graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "        self.scores = self.graph.get_tensor_by_name('scores:0')\n",
    "        self.predictions = self.graph.get_tensor_by_name('predictions:0')\n",
    "        self.loss = self.graph.get_tensor_by_name('Loss/Mean:0')\n",
    "        self.accuracy = self.graph.get_tensor_by_name('Accuracy/accuracy:0')\n",
    "        self.global_step = self.graph.get_tensor_by_name('global_step:0')\n",
    "        self.train_op = self.graph.get_tensor_by_name('train_op:0')\n",
    "        \n",
    "        \n",
    "    def dist(self, x1, x2):\n",
    "        x1, x1_len = batch([x1])\n",
    "        x2, x2_len = batch([x2])\n",
    "        fd = {self.input_x1:x1, self.input_x1_length:x1_len,\n",
    "              self.input_x2:x2, self.input_x2_length:x2_len,\n",
    "              self.keep_prob:1.0}\n",
    "        conf = self.sess.run(self.scores, feed_dict=fd)\n",
    "        return 1-conf[0]\n",
    "    \n",
    "    def evaluate(self, doc_mix, doc_lbs, method='average', plot=True):\n",
    "        doc_mix_sq, _ = batch(doc_mix)\n",
    "        doc_mix_sq = doc_mix_sq.T\n",
    "        doc_mix_clust = linkage(doc_mix_sq, method=method, metric=self.dist)\n",
    "        # evaluate by class-based prec/rec/f1\n",
    "        doc_prd = fcluster(doc_mix_clust, 2, criterion='maxclust') # predicted assignments\n",
    "        eval_input = clust_to_eval_input(doc_prd, doc_lbs)\n",
    "        prec, rec = cb_prec(*eval_input), cb_rec(*eval_input)\n",
    "        f1 = cb_f1(prec, rec)\n",
    "        if plot:\n",
    "            print('Class-based clustering evaluation:')\n",
    "            print('Precision = {} | Recall = {} | F1 = {}'.format(prec,rec,f1))\n",
    "            print('\\n')\n",
    "            plt.figure(figsize=(25, 10))\n",
    "            plt.title('Hierarchical Clustering Dendrogram')\n",
    "            plt.xlabel('sample index')\n",
    "            plt.ylabel('distance')\n",
    "            dendrogram(\n",
    "                doc_mix_clust,\n",
    "                leaf_rotation=90.,  # rotates the x axis labels\n",
    "                leaf_font_size=15.,  # font size for the x axis labels\n",
    "            )\n",
    "            plt.show() \n",
    "            print('True | Pred | Sentence')\n",
    "            for label,pred_label,code in zip(doc_lbs,doc_prd,doc_mix):\n",
    "                print('{}    | {}    | {}'.format(label,pred_label,to_sent(code)))\n",
    "            print('\\n')\n",
    "        else:\n",
    "            return doc_mix_clust, prec, rec, f1\n",
    "        \n",
    "        \n",
    "def clust_to_eval_input(pred_clust, true_clust):\n",
    "    pred_ass2cls, pred_cls2ass = [], defaultdict(list)\n",
    "    true_ass2cls, true_cls2ass = [], defaultdict(list)\n",
    "    for item_id,(pred_ass,true_ass) in enumerate(zip(pred_clust,true_clust)):\n",
    "        pred_ass2cls.append((item_id,pred_ass))\n",
    "        true_ass2cls.append((item_id,true_ass))\n",
    "        pred_cls2ass[pred_ass].append(item_id)\n",
    "        true_cls2ass[true_ass].append(item_id)\n",
    "    pred_ass2cls = dict(pred_ass2cls)\n",
    "    true_ass2cls = dict(true_ass2cls)\n",
    "    return pred_ass2cls, pred_cls2ass, true_ass2cls, true_cls2ass\n",
    "\n",
    "def cb(source_a2c, source_c2a, target_a2c, target_c2a):\n",
    "    prec_num = prec_denom = 0\n",
    "    for _,ass in source_c2a.items():\n",
    "        card_ass = len(ass)\n",
    "        mem_cls = set()\n",
    "        for elem in ass:\n",
    "            for t_elem,cls in target_a2c.items():\n",
    "                if elem==t_elem:\n",
    "                    mem_cls.add(cls)\n",
    "        prec_num += card_ass - len(mem_cls)\n",
    "        prec_denom += card_ass - 1\n",
    "    return prec_num / prec_denom   \n",
    "\n",
    "def cb_prec(p_a2c, p_c2a, t_a2c, t_c2a):\n",
    "    return cb(p_a2c, p_c2a, t_a2c, t_c2a)\n",
    "\n",
    "def cb_rec(p_a2c, p_c2a, t_a2c, t_c2a):\n",
    "    return cb(t_a2c, t_c2a, p_a2c, p_c2a)\n",
    "    \n",
    "def cb_f1(prec, rec):\n",
    "    return (2*rec*prec) / (rec+prec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_dir = \"/home/04233/sw33286/AIDA-tests/MODEL-SAVE/Bi-LSTM-HAC-wo-context/\"\n",
    "restore_filename = 'stacked-bilstm-doc-mix-mock-00.meta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_hac = ClfHAC(restore_dir, restore_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-based clustering evaluation:\n",
      "Precision = 0.875 | Recall = 0.875 | F1 = 0.875\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB9YAAANVCAYAAAAtFBVGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJmoSJdUmSJEmSJEmSJEmSJvG/YvS1XfF5Uq8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ae743c1fa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True | Pred | Sentence\n",
      "0    | 1    | ['pig', 'share2', 'cat', 'deer', 'dog']\n",
      "0    | 1    | ['dog', 'pig', 'pig', 'share1', 'cat', 'deer', 'share2']\n",
      "0    | 1    | ['pig', 'share1', 'share2', 'cat', 'share1', 'share1', 'deer']\n",
      "0    | 1    | ['pig', 'horse', 'pig', 'share1', 'share2', 'dog', 'horse']\n",
      "1    | 2    | ['car', 'car', 'share2', 'share1', 'bus', 'share2', 'train', 'share1', 'bus', 'bus', 'bus', 'bike', 'bike']\n",
      "1    | 2    | ['bike', 'share2', 'bus', 'train', 'motorcycle']\n",
      "1    | 2    | ['bike', 'motorcycle', 'bike', 'car', 'train', 'bike', 'car', 'car', 'train', 'bike', 'car', 'share2', 'share1', 'bike']\n",
      "1    | 2    | ['train', 'bus', 'train', 'train', 'bus', 'motorcycle', 'bike', 'share1', 'share2', 'bike', 'car', 'share2']\n",
      "0    | 1    | ['share2', 'dog', 'deer', 'deer', 'share1', 'share1', 'deer', 'dog', 'deer', 'horse', 'share2']\n",
      "1    | 1    | ['bike', 'bus', 'train', 'car', 'bus', 'bus', 'bus', 'train', 'car', 'motorcycle']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# da,db,dm,dl = get_mixture('ANIMAL', 'VEHICLE')\n",
    "clf_hac.evaluate(dm, dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Prec/Rec/F1 at 1 = 0.875/0.875/0.875\n",
      "Average Prec/Rec/F1 at 11 = 0.9204545454545454/0.9204545454545454/0.9204545454545454\n",
      "Average Prec/Rec/F1 at 21 = 0.9285714285714286/0.9285714285714286/0.9285714285714286\n",
      "Average Prec/Rec/F1 at 31 = 0.9435483870967742/0.9435483870967742/0.9435483870967742\n",
      "Average Prec/Rec/F1 at 41 = 0.9329268292682927/0.9329268292682927/0.9329268292682927\n",
      "Average Prec/Rec/F1 at 51 = 0.9362745098039216/0.9362745098039216/0.9362745098039216\n",
      "Average Prec/Rec/F1 at 61 = 0.9282786885245902/0.9282786885245902/0.9282786885245902\n",
      "Average Prec/Rec/F1 at 71 = 0.9278169014084507/0.9278169014084507/0.9278169014084507\n",
      "Average Prec/Rec/F1 at 81 = 0.9243827160493827/0.9243827160493827/0.9243827160493827\n",
      "Average Prec/Rec/F1 at 91 = 0.9230769230769231/0.9230769230769231/0.9230769230769231\n",
      "Average Prec/Rec/F1 at 101 = 0.9195544554455446/0.9195544554455446/0.9195544554455446\n",
      "Average Prec/Rec/F1 at 111 = 0.9177927927927928/0.9177927927927928/0.9177927927927928\n",
      "Average Prec/Rec/F1 at 121 = 0.9152892561983471/0.9152892561983471/0.9152892561983471\n",
      "Average Prec/Rec/F1 at 131 = 0.9122137404580153/0.9122137404580153/0.9122137404580153\n",
      "Average Prec/Rec/F1 at 141 = 0.9122340425531915/0.9122340425531915/0.9122340425531915\n",
      "Average Prec/Rec/F1 at 151 = 0.9130794701986755/0.9130794701986755/0.9130794701986755\n",
      "Average Prec/Rec/F1 at 161 = 0.9107142857142857/0.9107142857142857/0.9107142857142857\n",
      "Average Prec/Rec/F1 at 171 = 0.9122807017543859/0.9122807017543859/0.9122807017543859\n",
      "Average Prec/Rec/F1 at 181 = 0.9129834254143646/0.9129834254143646/0.9129834254143646\n",
      "Average Prec/Rec/F1 at 191 = 0.9136125654450262/0.9136125654450262/0.9136125654450262\n",
      "Average Prec/Rec/F1 at 201 = 0.9154228855721394/0.9154228855721394/0.9154228855721394\n",
      "Average Prec/Rec/F1 at 211 = 0.9164691943127962/0.9164691943127962/0.9164691943127962\n",
      "Average Prec/Rec/F1 at 221 = 0.917420814479638/0.917420814479638/0.917420814479638\n",
      "Average Prec/Rec/F1 at 231 = 0.9161255411255411/0.9161255411255411/0.9161255411255411\n",
      "Average Prec/Rec/F1 at 241 = 0.9164937759336099/0.9164937759336099/0.9164937759336099\n",
      "Average Prec/Rec/F1 at 251 = 0.9168326693227091/0.9168326693227091/0.9168326693227091\n",
      "Average Prec/Rec/F1 at 261 = 0.9171455938697318/0.9171455938697318/0.9171455938697318\n",
      "Average Prec/Rec/F1 at 271 = 0.9174354243542435/0.9174354243542435/0.9174354243542435\n",
      "Average Prec/Rec/F1 at 281 = 0.9163701067615658/0.9163701067615658/0.9163701067615658\n",
      "Average Prec/Rec/F1 at 291 = 0.9170962199312714/0.9170962199312714/0.9170962199312714\n",
      "Average Prec/Rec/F1 at 301 = 0.9169435215946844/0.9169435215946844/0.9169435215946844\n",
      "Average Prec/Rec/F1 at 311 = 0.9168006430868167/0.9168006430868167/0.9168006430868167\n",
      "Average Prec/Rec/F1 at 321 = 0.9174454828660437/0.9174454828660437/0.9174454828660437\n",
      "Average Prec/Rec/F1 at 331 = 0.9172960725075529/0.9172960725075529/0.9172960725075529\n",
      "Average Prec/Rec/F1 at 341 = 0.9171554252199413/0.9171554252199413/0.9171554252199413\n",
      "Average Prec/Rec/F1 at 351 = 0.917022792022792/0.917022792022792/0.917022792022792\n",
      "Average Prec/Rec/F1 at 361 = 0.917590027700831/0.917590027700831/0.917590027700831\n",
      "Average Prec/Rec/F1 at 371 = 0.9177897574123989/0.9177897574123989/0.9177897574123989\n",
      "Average Prec/Rec/F1 at 381 = 0.9176509186351706/0.9176509186351706/0.9176509186351706\n",
      "Average Prec/Rec/F1 at 391 = 0.9171994884910486/0.9171994884910486/0.9171994884910486\n",
      "Average Prec/Rec/F1 at 401 = 0.9170822942643392/0.9170822942643392/0.9170822942643392\n",
      "Average Prec/Rec/F1 at 411 = 0.9172749391727494/0.9172749391727494/0.9172749391727494\n",
      "Average Prec/Rec/F1 at 421 = 0.9165676959619953/0.9165676959619953/0.9165676959619953\n",
      "Average Prec/Rec/F1 at 431 = 0.9170533642691415/0.9170533642691415/0.9170533642691415\n",
      "Average Prec/Rec/F1 at 441 = 0.9172335600907029/0.9172335600907029/0.9172335600907029\n",
      "Average Prec/Rec/F1 at 451 = 0.9174057649667405/0.9174057649667405/0.9174057649667405\n",
      "Average Prec/Rec/F1 at 461 = 0.9172993492407809/0.9172993492407809/0.9172993492407809\n",
      "Average Prec/Rec/F1 at 471 = 0.9171974522292994/0.9171974522292994/0.9171974522292994\n",
      "Average Prec/Rec/F1 at 481 = 0.9170997920997921/0.9170997920997921/0.9170997920997921\n",
      "Average Prec/Rec/F1 at 491 = 0.9172606924643585/0.9172606924643585/0.9172606924643585\n",
      "Average Prec/Rec/F1 at 501 = 0.9176646706586826/0.9176646706586826/0.9176646706586826\n",
      "Average Prec/Rec/F1 at 511 = 0.9175636007827789/0.9175636007827789/0.9175636007827789\n",
      "Average Prec/Rec/F1 at 521 = 0.9179462571976967/0.9179462571976967/0.9179462571976967\n",
      "Average Prec/Rec/F1 at 531 = 0.917608286252354/0.917608286252354/0.917608286252354\n",
      "Average Prec/Rec/F1 at 541 = 0.91728280961183/0.91728280961183/0.91728280961183\n",
      "Average Prec/Rec/F1 at 551 = 0.9169691470054446/0.9169691470054446/0.9169691470054446\n",
      "Average Prec/Rec/F1 at 561 = 0.9166666666666666/0.9166666666666666/0.9166666666666666\n",
      "Average Prec/Rec/F1 at 571 = 0.9165936952714536/0.9165936952714536/0.9165936952714536\n",
      "Average Prec/Rec/F1 at 581 = 0.9171686746987951/0.9171686746987951/0.9171686746987951\n",
      "Average Prec/Rec/F1 at 591 = 0.9173011844331641/0.9173011844331641/0.9173011844331641\n",
      "Average Prec/Rec/F1 at 601 = 0.9172212978369384/0.9172212978369384/0.9172212978369384\n",
      "Average Prec/Rec/F1 at 611 = 0.9171440261865794/0.9171440261865794/0.9171440261865794\n",
      "Average Prec/Rec/F1 at 621 = 0.9168679549114331/0.9168679549114331/0.9168679549114331\n",
      "Average Prec/Rec/F1 at 631 = 0.9164025356576863/0.9164025356576863/0.9164025356576863\n",
      "Average Prec/Rec/F1 at 641 = 0.9163416536661466/0.9163416536661466/0.9163416536661466\n",
      "Average Prec/Rec/F1 at 651 = 0.9162826420890937/0.9162826420890937/0.9162826420890937\n",
      "Average Prec/Rec/F1 at 661 = 0.916036308623298/0.916036308623298/0.916036308623298\n",
      "Average Prec/Rec/F1 at 671 = 0.9157973174366617/0.9157973174366617/0.9157973174366617\n",
      "Average Prec/Rec/F1 at 681 = 0.9161160058737151/0.9161160058737151/0.9161160058737151\n",
      "Average Prec/Rec/F1 at 691 = 0.915520984081042/0.915520984081042/0.915520984081042\n",
      "Average Prec/Rec/F1 at 701 = 0.9154778887303852/0.9154778887303852/0.9154778887303852\n",
      "Average Prec/Rec/F1 at 711 = 0.9156118143459916/0.9156118143459916/0.9156118143459916\n",
      "Average Prec/Rec/F1 at 721 = 0.9153952843273232/0.9153952843273232/0.9153952843273232\n",
      "Average Prec/Rec/F1 at 731 = 0.9153556771545828/0.9153556771545828/0.9153556771545828\n",
      "Average Prec/Rec/F1 at 741 = 0.9153171390013495/0.9153171390013495/0.9153171390013495\n",
      "Average Prec/Rec/F1 at 751 = 0.9151131824234354/0.9151131824234354/0.9151131824234354\n",
      "Average Prec/Rec/F1 at 761 = 0.9149145860709592/0.9149145860709592/0.9149145860709592\n",
      "Average Prec/Rec/F1 at 771 = 0.915207522697795/0.915207522697795/0.915207522697795\n",
      "Average Prec/Rec/F1 at 781 = 0.9151728553137004/0.9151728553137004/0.9151728553137004\n",
      "Average Prec/Rec/F1 at 791 = 0.9151390644753477/0.9151390644753477/0.9151390644753477\n",
      "Average Prec/Rec/F1 at 801 = 0.9152621722846442/0.9152621722846442/0.9152621722846442\n",
      "Average Prec/Rec/F1 at 811 = 0.9155363748458692/0.9155363748458692/0.9155363748458692\n",
      "Average Prec/Rec/F1 at 821 = 0.915347137637028/0.915347137637028/0.915347137637028\n",
      "Average Prec/Rec/F1 at 831 = 0.9153128760529483/0.9153128760529483/0.9153128760529483\n",
      "Average Prec/Rec/F1 at 841 = 0.9151307966706302/0.9151307966706302/0.9151307966706302\n",
      "Average Prec/Rec/F1 at 851 = 0.9152467685076381/0.9152467685076381/0.9152467685076381\n",
      "Average Prec/Rec/F1 at 861 = 0.9153600464576074/0.9153600464576074/0.9153600464576074\n",
      "Average Prec/Rec/F1 at 871 = 0.9151836969001148/0.9151836969001148/0.9151836969001148\n",
      "Average Prec/Rec/F1 at 881 = 0.9148694665153235/0.9148694665153235/0.9148694665153235\n",
      "Average Prec/Rec/F1 at 891 = 0.9144219977553311/0.9144219977553311/0.9144219977553311\n",
      "Average Prec/Rec/F1 at 901 = 0.9142619311875694/0.9142619311875694/0.9142619311875694\n",
      "Average Prec/Rec/F1 at 911 = 0.9139681668496158/0.9139681668496158/0.9139681668496158\n",
      "Average Prec/Rec/F1 at 921 = 0.9139522258414766/0.9139522258414766/0.9139522258414766\n",
      "Average Prec/Rec/F1 at 931 = 0.9140708915145005/0.9140708915145005/0.9140708915145005\n",
      "Average Prec/Rec/F1 at 941 = 0.9144527098831031/0.9144527098831031/0.9144527098831031\n",
      "Average Prec/Rec/F1 at 951 = 0.9143007360672976/0.9143007360672976/0.9143007360672976\n",
      "Average Prec/Rec/F1 at 961 = 0.9144120707596254/0.9144120707596254/0.9144120707596254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Prec/Rec/F1 at 971 = 0.9146498455200824/0.9146498455200824/0.9146498455200824\n",
      "Average Prec/Rec/F1 at 981 = 0.9147553516819572/0.9147553516819572/0.9147553516819572\n",
      "Average Prec/Rec/F1 at 991 = 0.9144803229061554/0.9144803229061554/0.9144803229061554\n"
     ]
    }
   ],
   "source": [
    "# # Evaluating with randomly generated documents\n",
    "\n",
    "NUM_DOCS = 1000\n",
    "\n",
    "def get_scores_for_rand_doc_mix(clf): # clf: a ClfHAC object.\n",
    "    type1, type2 = np.random.choice(TYPES, 2, replace=False)\n",
    "#     print(type1, type2)\n",
    "    _,_,dm,dl = get_mixture(type1, type2) # document a,b don't play into evaluation here.\n",
    "    _, prec, rec, f1 = clf.evaluate(dm, dl, plot=False)\n",
    "    return prec, rec, f1\n",
    "\n",
    "precs,recs,f1s = [],[],[]\n",
    "for i in range(NUM_DOCS):\n",
    "    prec,rec,f1 = get_scores_for_rand_doc_mix(clf_hac)\n",
    "    precs.append(prec)\n",
    "    recs.append(rec)\n",
    "    f1s.append(f1)\n",
    "    if i%10==0:\n",
    "        print('Average Prec/Rec/F1 at {} = {}/{}/{}'.format(i+1,np.mean(precs),np.mean(recs),np.mean(f1s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-based clustering evaluation:\n",
      "Precision = 0.75 | Recall = 0.75 | F1 = 0.75\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB9YAAANVCAYAAAAtFBVGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJmoSJdUmSJEmSJEmSJEmSJvG/YvS1XfF5Uq8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ae74b14b6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True | Pred | Sentence\n",
      "0    | 2    | ['pig', 'share2', 'cat', 'deer', 'dog']\n",
      "0    | 2    | ['dog', 'pig', 'pig', 'share1', 'cat', 'deer', 'share2']\n",
      "0    | 1    | ['pig', 'share1', 'share2', 'cat', 'share1', 'share1', 'deer']\n",
      "0    | 1    | ['pig', 'horse', 'pig', 'share1', 'share2', 'dog', 'horse']\n",
      "1    | 1    | ['car', 'car', 'share2', 'share1', 'bus', 'share2', 'train', 'share1', 'bus', 'bus', 'bus', 'bike', 'bike']\n",
      "1    | 1    | ['bike', 'share2', 'bus', 'train', 'motorcycle']\n",
      "1    | 2    | ['bike', 'motorcycle', 'bike', 'car', 'train', 'bike', 'car', 'car', 'train', 'bike', 'car', 'share2', 'share1', 'bike']\n",
      "1    | 1    | ['train', 'bus', 'train', 'train', 'bus', 'motorcycle', 'bike', 'share1', 'share2', 'bike', 'car', 'share2']\n",
      "0    | 2    | ['share2', 'dog', 'deer', 'deer', 'share1', 'share1', 'deer', 'dog', 'deer', 'horse', 'share2']\n",
      "1    | 2    | ['bike', 'bus', 'train', 'car', 'bus', 'bus', 'bus', 'train', 'car', 'motorcycle']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "da,db,dm,dl = get_mixture('ANIMAL', 'VEHICLE')\n",
    "clf_hac.evaluate(dm, dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _,_,dm2,dl2 = get_mixture('ANIMAL', 'VEHICLE')\n",
    "# clf_hac.evaluate(dm2, dl2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
